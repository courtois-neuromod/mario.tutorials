{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579b22aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Brain Encoding with RL Features\n",
    "\n",
    "## Predicting Brain Activity from Agent Representations\n",
    "\n",
    "**Overview:**\n",
    "This notebook uses the CNN activations from the RL agent (notebook 02) to predict brain activity during gameplay.\n",
    "\n",
    "**What we'll cover:**\n",
    "1. Understanding the encoding model framework\n",
    "2. Loading and preparing BOLD data\n",
    "3. Loading CNN activations from the agent\n",
    "4. Aligning timepoints between BOLD and activations\n",
    "5. Fitting ridge regression encoding models\n",
    "6. Comparing layer performance\n",
    "7. Visualizing brain maps\n",
    "\n",
    "**Key question:** Which layer of the agent best predicts brain activity, and where?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5aaccc2",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup - imports and configuration\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "src_dir = Path('..') / 'src'\n",
    "sys.path.insert(0, str(src_dir))\n",
    "\n",
    "# Import utilities\n",
    "from utils import (\n",
    "    get_sourcedata_path,\n",
    "    load_events,\n",
    "    get_session_runs,\n",
    "    load_bold\n",
    ")\n",
    "\n",
    "# Import RL utilities\n",
    "from rl_utils import (\n",
    "    create_simple_proxy_features,\n",
    "    convolve_with_hrf,\n",
    "    apply_pca\n",
    ")\n",
    "\n",
    "# Import RL visualizations\n",
    "from rl_viz_utils import (\n",
    "    plot_pca_variance_per_layer,\n",
    "    plot_layer_activations_sample\n",
    ")\n",
    "\n",
    "# Import encoding utilities\n",
    "from encoding_utils import (\n",
    "    load_and_prepare_bold,\n",
    "    fit_encoding_model_per_layer,\n",
    "    compare_layer_performance\n",
    ")\n",
    "\n",
    "# Import encoding visualizations\n",
    "from encoding_viz_utils import (\n",
    "    plot_layer_comparison_bars,\n",
    "    plot_r2_brainmap\n",
    ")\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Get sourcedata path\n",
    "sourcedata_path = get_sourcedata_path()\n",
    "\n",
    "print(\"‚úì Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0ce3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. The Encoding Model Framework\n",
    "\n",
    "**Goal:** Predict BOLD activity from RL agent features\n",
    "\n",
    "**Model:** Ridge Regression (linear regression with L2 regularization)\n",
    "\n",
    "```\n",
    "BOLD(voxel, time) = Œ£ Œ≤·µ¢ ¬∑ Feature_i(time) + Œµ\n",
    "```\n",
    "\n",
    "**Why ridge regression?**\n",
    "- Handles high-dimensional features (50 PCA components)\n",
    "- L2 penalty prevents overfitting: `||Œ≤||¬≤ ‚â§ Œ±`\n",
    "- Cross-validation selects optimal regularization strength Œ±\n",
    "- Fast to fit (~5 mins for whole brain)\n",
    "\n",
    "**Alternative approaches:**\n",
    "- Lasso (L1): Sparse feature selection\n",
    "- Elastic net: L1 + L2\n",
    "- Nonlinear: Kernel ridge, neural networks\n",
    "\n",
    "**For interpretability and speed, we use ridge regression.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e9bf216",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 runs: ['run-1', 'run-2', 'run-3', 'run-4']\n",
      "  run-1: 953 events\n",
      "  run-2: 986 events\n",
      "  run-3: 892 events\n",
      "  run-4: 1033 events\n",
      "\n",
      "Creating common brain mask...\n",
      "‚úì Common mask: 213,443 voxels\n",
      "\n",
      "‚úì All prerequisites loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load prerequisites\n",
    "\n",
    "from nilearn.masking import compute_multi_epi_mask\n",
    "\n",
    "# Define constants (assumed from the first tutorial)\n",
    "SUBJECT = 'sub-01'\n",
    "SESSION = 'ses-010'\n",
    "TR = 1.49\n",
    "\n",
    "\n",
    "# Get runs\n",
    "runs = get_session_runs(SUBJECT, SESSION, sourcedata_path)\n",
    "print(f\"Found {len(runs)} runs: {runs}\")\n",
    "\n",
    "# Load events\n",
    "all_events = []\n",
    "for run in runs:\n",
    "    events = load_events(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    all_events.append(events)\n",
    "    print(f\"  {run}: {len(events)} events\")\n",
    "\n",
    "# Create common mask (or load from main tutorial)\n",
    "print(\"\\nCreating common brain mask...\")\n",
    "bold_imgs= []\n",
    "for run in runs:\n",
    "    bold_img = load_bold(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    bold_imgs.append(bold_img)\n",
    "\n",
    "common_mask = compute_multi_epi_mask(bold_imgs, n_jobs=1)\n",
    "n_voxels = int((common_mask.get_fdata() > 0).sum())\n",
    "print(f\"‚úì Common mask: {n_voxels:,} voxels\")\n",
    "\n",
    "print(\"\\n‚úì All prerequisites loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e1530c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Loading Prerequisites\n",
    "\n",
    "We need:\n",
    "- Subject/session info (sub-01, ses-010)\n",
    "- Run IDs (4 runs)\n",
    "- BOLD images (preprocessed fMRI data)\n",
    "- Event files (for alignment)\n",
    "- Common brain mask (from GLM analysis)\n",
    "\n",
    "**Note:** If you haven't run notebook 01, this will create a fresh mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f2d50",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": "# Load and align activations from replays\n\n# First, check if we have a trained model\nfrom pathlib import Path\n\nMODEL_DIR = Path('../models')\nMODEL_PATH = MODEL_DIR / 'mario_ppo_agent.pth'\n\nif not MODEL_PATH.exists():\n    print(f\"‚úó No trained model found at: {MODEL_PATH}\")\n    print(\"\\nYou need a trained RL agent to extract activations.\")\n    print(\"Please train an agent first by running:\")\n    print(\"  python ../train_mario_agent.py --steps 5000000\")\n    print(\"\\n‚ö† Cannot proceed with encoding analysis without trained model\")\n    HAS_MODEL = False\nelse:\n    print(f\"‚úì Found trained model: {MODEL_PATH}\")\n    HAS_MODEL = True\n    \n    # Load the model\n    from rl_utils import load_pretrained_model, align_activations_to_bold\n    \n    print(\"\\nLoading model...\")\n    model = load_pretrained_model(MODEL_PATH, device='cpu')\n    print(\"‚úì Model loaded\")\n    \n    # Align activations to BOLD\n    # This will:\n    # 1. Load replay files for each game segment\n    # 2. Extract RL activations at 60Hz  \n    # 3. Downsample to TR (1.49s)\n    # 4. Apply HRF convolution\n    # 5. Create NaN mask for non-gameplay periods\n    \n    alignment_results = align_activations_to_bold(\n        model=model,\n        subject=SUBJECT,\n        session=SESSION,\n        runs=runs,\n        sourcedata_path=sourcedata_path,\n        tr=TR,\n        device='cpu',\n        apply_hrf=True  # Apply HRF convolution\n    )\n    \n    # Extract results\n    layer_activations = alignment_results['activations']\n    valid_mask = alignment_results['mask']\n    run_info = alignment_results['run_info']\n    \n    print(f\"\\n{'='*70}\")\n    print(\"Alignment summary:\")\n    for info in run_info:\n        print(f\"  {info['run']}: {info['n_valid_trs']}/{info['n_trs']} TRs \"\n              f\"({info['n_segments']} game segments)\")\n    print(f\"{'='*70}\\n\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "535d9cdd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": "## 3. Loading and Aligning RL Activations\n\n**NEW APPROACH:**\n\nInstead of using pre-extracted activations, we now:\n\n1. **Load replay files** from the human subject's actual gameplay\n   - Uses `.bk2` replay files from `sourcedata/mario/`\n   - Matches exact stimuli presented during fMRI scanning\n\n2. **Extract activations frame-by-frame** (60Hz)\n   - Pass replay frames through trained RL agent\n   - Collect CNN activations from all layers\n\n3. **Align to fMRI timing**\n   - Use `mario.annotations` files to get game segment timing\n   - Downsample from 60Hz to TR (1.49s)\n   - Apply HRF convolution\n\n4. **Handle multiple games per run**\n   - Concatenate gameplay segments\n   - Mask inter-game periods with NaN\n\n**This ensures perfect alignment between RL activations and BOLD data!**"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5490c7fd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning BOLD (detrending, standardizing)...\n",
      "\n",
      "‚úì BOLD prepared:\n",
      "  Shape: (1799, 213443)\n",
      "  Timepoints: 1799\n",
      "  Voxels: 213,443\n"
     ]
    }
   ],
   "source": [
    "# Clean and prepare BOLD data\n",
    "\n",
    "from encoding_utils import load_and_prepare_bold\n",
    "\n",
    "print(\"Cleaning BOLD (detrending, standardizing)...\\n\")\n",
    "\n",
    "bold_data = load_and_prepare_bold(\n",
    "    bold_imgs,\n",
    "    mask_img=common_mask,\n",
    "    confounds_list=None,  # Already cleaned in GLM\n",
    "    detrend=True,\n",
    "    standardize=True,\n",
    "    high_pass=1/128,\n",
    "    t_r=TR\n",
    ")\n",
    "\n",
    "print(f\"‚úì BOLD prepared:\")\n",
    "print(f\"  Shape: {bold_data.shape}\")\n",
    "print(f\"  Timepoints: {bold_data.shape[0]}\")\n",
    "print(f\"  Voxels: {bold_data.shape[1]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec498b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Cleaning and Preparing BOLD Data\n",
    "\n",
    "**Preprocessing steps:**\n",
    "1. **Detrending:** Remove linear drift within each run\n",
    "2. **High-pass filtering:** Remove slow fluctuations (<1/128 Hz)\n",
    "3. **Standardization:** Z-score each voxel (mean=0, std=1)\n",
    "\n",
    "**Why clean BOLD?**\n",
    "- Scanner drift confounds encoding\n",
    "- Slow fluctuations unrelated to task\n",
    "- Standardization ensures comparable scales\n",
    "\n",
    "**Output:** `(timepoints √ó voxels)` matrix ready for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485af949",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": "# Check alignment between BOLD and activations\n\nif HAS_MODEL:\n    n_bold = bold_data.shape[0]\n    n_acts = list(layer_activations.values())[0].shape[0]\n    \n    print(f\"BOLD timepoints: {n_bold}\")\n    print(f\"Activations timepoints: {n_acts}\")\n    print(f\"Valid (gameplay) timepoints: {valid_mask.sum()}\")\n    print(f\"Invalid (non-gameplay) timepoints: {(~valid_mask).sum()}\")\n    \n    # Ensure dimensions match\n    if n_bold != n_acts:\n        print(f\"\\n‚ö† Dimension mismatch!\")\n        print(f\"  Truncating to minimum length: {min(n_bold, n_acts)}\")\n        n_time = min(n_bold, n_acts)\n        bold_data = bold_data[:n_time]\n        valid_mask = valid_mask[:n_time]\n        for layer in layer_activations.keys():\n            layer_activations[layer] = layer_activations[layer][:n_time]\n    else:\n        print(\"\\n‚úì Dimensions match!\")\nelse:\n    print(\"‚ö† No model available, skipping alignment check\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "e68a2b66",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": "## 5. Alignment Status\n\n**Automatic alignment completed!**\n\nThe `align_activations_to_bold()` function has:\n\n1. ‚úÖ **Loaded replay files** for each game segment\n2. ‚úÖ **Extracted RL activations** at 60Hz from replay frames\n3. ‚úÖ **Downsampled to TR** using temporal averaging within each TR window\n4. ‚úÖ **Applied HRF convolution** to account for hemodynamic lag\n5. ‚úÖ **Created validity mask** to mark gameplay vs non-gameplay periods\n\n**Key differences from old approach:**\n- OLD: Arbitrary agent gameplay, misaligned\n- NEW: Exact subject gameplay from replays, perfectly aligned\n\n**Dimensions should now match:**\n- BOLD: Number of TRs across all runs\n- Activations: Same number of TRs (with NaN for non-gameplay)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f09db7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": "# Create train/test split (only on valid gameplay TRs)\n\nif HAS_MODEL:\n    # Get valid indices\n    valid_indices = np.where(valid_mask)[0]\n    n_valid = len(valid_indices)\n    \n    # Split valid indices 80/20\n    n_train_valid = int(n_valid * 0.8)\n    \n    train_valid_indices = valid_indices[:n_train_valid]\n    test_valid_indices = valid_indices[n_train_valid:]\n    \n    print(f\"Train/test split on valid (gameplay) TRs:\")\n    print(f\"  Train: {len(train_valid_indices)} TRs\")\n    print(f\"  Test: {len(test_valid_indices)} TRs\")\n    print(f\"\\nNote: Invalid (non-gameplay) TRs are excluded from both train and test\")\nelse:\n    print(\"‚ö† No model available, skipping train/test split\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "78103a6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": "## 6. Applying PCA to Layer Activations\n\n**Why PCA?**\n- Raw activations have thousands of features per layer\n- PCA reduces dimensionality while preserving variance\n- Makes encoding models more tractable and prevents overfitting\n\n**NaN handling:**\n- PCA is fit only on valid (gameplay) TRs\n- Invalid TRs remain NaN in the reduced activations\n- This ensures proper alignment with BOLD data\n\n**Target:** 50 PCA components per layer (or fewer if needed)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07912745",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": "# Apply PCA to layer activations\n\nif HAS_MODEL:\n    from rl_utils import apply_pca_with_nan_handling\n    \n    print(\"Applying PCA to reduce dimensionality...\")\n    print(\"(PCA is fit only on valid gameplay TRs)\\n\")\n    \n    pca_results = apply_pca_with_nan_handling(\n        layer_activations,\n        valid_mask,\n        n_components=50,\n        variance_threshold=0.9\n    )\n    \n    # Extract reduced activations\n    reduced_activations = pca_results['reduced_activations']\n    pca_models = pca_results['pca_models']\n    variance_explained = pca_results['variance_explained']\n    \n    print(f\"\\n{'='*70}\")\n    print(\"PCA summary:\")\n    for layer, acts in reduced_activations.items():\n        print(f\"  {layer}: {acts.shape[1]} components\")\n    print(f\"{'='*70}\\n\")\nelse:\n    print(\"‚ö† No model available, skipping PCA\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "394c681a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": "## 7. Fitting Ridge Regression Encoding Models\n\n**For each layer:**\n1. Use PCA-reduced activations (50 components)\n2. Cross-validate to find optimal Œ± (regularization strength)\n3. Fit ridge regression on training data (gameplay TRs only)\n4. Predict BOLD on test data\n5. Compute R¬≤ per voxel\n\n**Hyperparameter search:** Œ± ‚àà [0.1, 1, 10, 100, 1000, 10000, 100000]\n\n**NaN handling:**\n- Only valid (gameplay) TRs are used for training and testing\n- Invalid TRs are automatically excluded\n\n**Output per layer:**\n- Best Œ±\n- R¬≤ map: `(voxels,)` array\n- Trained model\n\n**Runtime:** ~5-10 minutes for all 5 layers √ó 213k voxels\n\n**Interpretation:**\n- R¬≤ > 0: Features explain variance in BOLD\n- R¬≤ ‚âà 0: No prediction (chance level)\n- Negative R¬≤: Worse than mean baseline"
  },
  {
   "cell_type": "code",
   "id": "a0kcwmp9zb",
   "source": "# Fit ridge regression encoding models\n\nif HAS_MODEL:\n    from encoding_utils import fit_encoding_model_per_layer\n    \n    alphas = [0.1, 1, 10, 100, 1000, 10000, 100000]\n    \n    print(\"Fitting ridge regression (5 layers √ó voxels)...\")\n    print(\"This takes ~5-10 minutes\\n\")\n    \n    encoding_results = fit_encoding_model_per_layer(\n        reduced_activations,\n        bold_data,\n        common_mask,\n        train_valid_indices,\n        test_valid_indices,\n        alphas=alphas,\n        valid_mask=valid_mask  # Pass valid mask for NaN handling\n    )\n    \n    print(\"\\n‚úì Encoding complete!\")\nelse:\n    print(\"‚ö† No model available, skipping encoding\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dbb1ff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": "# Compare layer performance\n\nif HAS_MODEL:\n    from encoding_utils import compare_layer_performance\n    from encoding_viz_utils import plot_layer_comparison_bars\n    \n    comparison_df = compare_layer_performance(encoding_results)\n    \n    print(\"Layer Performance:\\n\")\n    print(\"=\" * 80)\n    print(comparison_df.to_string(index=False))\n    print(\"=\" * 80)\n    \n    best_layer = comparison_df.iloc[0]['layer']\n    best_r2 = comparison_df.iloc[0]['mean_r2']\n    \n    print(f\"\\n‚≠ê Best: {best_layer.upper()} (R¬≤ = {best_r2:.4f})\")\n    \n    # Visualize\n    layer_order = ['conv1', 'conv2', 'conv3', 'conv4', 'linear']\n    fig = plot_layer_comparison_bars(encoding_results, layer_order)\n    plt.show()\nelse:\n    print(\"‚ö† No model available, skipping layer comparison\")"
  },
  {
   "cell_type": "markdown",
   "id": "89e8bac1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 8. Comparing Layer Performance\n",
    "\n",
    "**Question:** Which CNN layer best predicts brain activity?\n",
    "\n",
    "**Metrics:**\n",
    "- Mean R¬≤ (test set)\n",
    "- Median R¬≤ (robust to outliers)\n",
    "- % voxels with R¬≤ > 0.01 (significantly predicted)\n",
    "\n",
    "**Expected pattern (if hypothesis holds):**\n",
    "- Early layers (conv1/2) ‚Üí Visual cortex\n",
    "- Middle layers (conv3/4) ‚Üí Motor/parietal\n",
    "- Late layers (linear) ‚Üí Frontal/executive\n",
    "\n",
    "**See bar plot below for comparison.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b0c840",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": "# Visualize R¬≤ brain maps (best layer)\n\nif HAS_MODEL:\n    from encoding_viz_utils import plot_r2_brainmap\n    \n    best_layer = comparison_df.iloc[0]['layer']\n    best_r2_map = encoding_results[best_layer]['r2_map']\n    \n    print(f\"Best layer: {best_layer.upper()}\\n\")\n    \n    fig = plot_r2_brainmap(\n        best_r2_map, \n        best_layer,\n        threshold=0.01,\n        vmax=0.2\n    )\n    plt.show()\n    \n    print(\"\\nüìç Interpretation:\")\n    print(\"  Hot regions = Well predicted by this layer\")\n    print(\"  - Early layers ‚Üí Visual cortex\")\n    print(\"  - Middle layers ‚Üí Motor/parietal\")\n    print(\"  - Late layers ‚Üí Frontal/executive\")\nelse:\n    print(\"‚ö† No model available, skipping brain map visualization\")"
  },
  {
   "cell_type": "markdown",
   "id": "763ffbbf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": "## Summary: Brain Encoding with Proper Alignment\n\n**What we accomplished:**\n\n1. ‚úÖ **Loaded RL model:** Trained PPO agent\n2. ‚úÖ **Extracted activations from replays:** Used actual gameplay .bk2 files\n3. ‚úÖ **Proper temporal alignment:**\n   - Matched replay frames to fMRI TRs using annotations\n   - Downsampled from 60Hz to TR (1.49s)\n   - Applied HRF convolution\n   - Masked non-gameplay periods with NaN\n4. ‚úÖ **Applied PCA:** Reduced to 50 components per layer (on valid TRs only)\n5. ‚úÖ **Fit encoding models:** Ridge regression with NaN-aware training\n6. ‚úÖ **Compared layers:** Identified which layer best predicts brain activity\n7. ‚úÖ **Visualized brain maps:** Localized where each layer is encoded\n\n---\n\n### Key Improvements Over Original Approach\n\n**What was fixed:**\n\n1. **Proper replay alignment:**\n   - OLD: Agent played arbitrary Level1-1, misaligned with BOLD\n   - NEW: Extract activations from exact subject gameplay replays\n\n2. **Temporal alignment:**\n   - OLD: No alignment, simple truncation\n   - NEW: Use annotation files to map frames ‚Üí TRs with onset/duration\n\n3. **HRF convolution:**\n   - OLD: Missing HRF convolution\n   - NEW: Applied SPM HRF after downsampling\n\n4. **Multiple games per run:**\n   - OLD: Couldn't handle multiple game segments\n   - NEW: Concatenate segments, mask inter-game periods with NaN\n\n5. **NaN handling:**\n   - OLD: No way to exclude non-gameplay periods\n   - NEW: Valid mask ensures encoding models only train on gameplay TRs\n\n---\n\n### Expected Results\n\n**If brain uses RL-like representations:**\n\n- **Early layers (conv1/2):** Predict visual cortex\n  - Edge detection, textures, low-level visual features\n  - Occipital lobe activation\n\n- **Middle layers (conv3/4):** Predict parietal/motor cortex\n  - Spatial layout, object positions, movement planning\n  - Parietal lobe, premotor cortex\n\n- **Late layers (linear):** Predict frontal cortex\n  - Value estimates, policy selection, abstract strategy\n  - Prefrontal cortex, anterior cingulate\n\n**Hierarchical gradient:**\n- R¬≤ should increase from early ‚Üí late layers if brain uses RL features\n- Brain maps should show posterior ‚Üí anterior gradient\n\n---\n\n### Methodological Lessons\n\n**Critical requirements for encoding analysis:**\n\n1. **Proper stimulus alignment:**\n   - Use exact stimuli presented to subject\n   - Match timing precisely (frame-by-frame if needed)\n   - Account for hemodynamic lag (HRF)\n\n2. **Data quality:**\n   - Sufficient valid trials (gameplay periods)\n   - Good signal-to-noise ratio\n   - Proper preprocessing (motion correction, etc.)\n\n3. **Statistical power:**\n   - Multiple sessions/subjects for group analysis\n   - Cross-validation to avoid overfitting\n   - Appropriate regularization (ridge Œ±)\n\n4. **Interpretation:**\n   - Compare to baseline models (GLM with behavioral features)\n   - Test specific hypotheses about layer-to-region mapping\n   - Consider alternative explanations (motion, attention, etc.)\n\n---\n\n### Comparison to GLM (Notebook 01)\n\n**GLM:**\n- ‚úÖ Hypothesis-driven (LEFT_THUMB vs RIGHT_THUMB)\n- ‚úÖ Simple, interpretable\n- ‚úÖ Works with sparse events\n- ‚úÖ Found significant effects (contralateral motor control)\n\n**Encoding (This notebook):**\n- ‚úÖ Data-driven (learned RL features)\n- ‚úÖ Exploratory (discover representations)\n- ‚úÖ Tests computational theories\n- ‚è≥ Requires more data and careful alignment\n\n**Both approaches are complementary:**\n- GLM: Validate known effects\n- Encoding: Discover new representations\n\n---\n\n### Next Steps & Extensions\n\n**To improve these results:**\n\n1. **More data:**\n   - Aggregate across multiple sessions\n   - Multi-subject analysis\n   - Increase gameplay duration\n\n2. **Better features:**\n   - Try different layers simultaneously\n   - Non-linear encoding (kernel ridge, neural networks)\n   - Task-specific features (value, prediction error, etc.)\n\n3. **Control analyses:**\n   - Compare to pixel-based features\n   - Test against behavioral-only models\n   - Permutation testing for significance\n\n4. **Advanced methods:**\n   - Hyperalignment across subjects\n   - Representational similarity analysis (RSA)\n   - Decoding (BOLD ‚Üí predicted actions)\n\n---\n\n### Research Questions\n\n**This pipeline enables investigating:**\n\n1. **Computational neuroscience:**\n   - Do brain and RL agent use similar representations?\n   - Where are value and policy encoded in the brain?\n   - How do representations change with learning?\n\n2. **Cognitive neuroscience:**\n   - How does the brain represent game state?\n   - What role does prediction play in decision-making?\n   - How are visual and motor systems integrated?\n\n3. **AI alignment:**\n   - Can we build agents that think like humans?\n   - What makes representations interpretable?\n   - How to design human-aligned reward functions?\n\n**This tutorial provides a complete, working pipeline for these investigations!**"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}