{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579b22aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Brain Encoding: Predicting Brain Activity from RL Agent\n",
    "\n",
    "**Goal:** Use CNN activations to predict BOLD activity.\n",
    "\n",
    "**Pipeline:** Load data â†’ Align â†’ PCA â†’ Ridge regression â†’ Compare layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523306a1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "setup"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ–¥ï¸ Running locally\n",
      "âœ… Repository ready at: /home/hyruuk/GitHub/neuromod/mario_analysis/mario.tutorials\n",
      "ðŸ“¦ Installing dependencies from notebooks/03_requirements.txt...\n",
      "  âœ“ Dependencies installed\n",
      "\n",
      "âœ… Setup complete!\n",
      "ðŸ“¥ Downloading 4 files matching pattern '*.tsv'...\n",
      "action summary:\n",
      "  get (notneeded: 4)\n",
      "âœ“ Downloaded 4 files\n",
      "ðŸ“¥ Downloading 28 files matching pattern '*.json'...\n",
      "action summary:\n",
      "  get (notneeded: 28)\n",
      "âœ“ Downloaded 28 files\n",
      "ðŸ“¥ Downloading 28 files matching pattern '*MNI152NLin2009cAsym*'...\n",
      "action summary:\n",
      "  get (notneeded: 28)\n",
      "âœ“ Downloaded 28 files\n",
      "ðŸ“¥ Downloading 8 files matching pattern '*confounds*'...\n",
      "action summary:\n",
      "  get (notneeded: 8)\n",
      "âœ“ Downloaded 8 files\n",
      "ðŸ“¥ Downloading 36 files matching pattern '*confs.npy'...\n",
      "action summary:\n",
      "  get (notneeded: 36)\n",
      "âœ“ Downloaded 36 files\n",
      "ðŸ“¥ Downloading 50 files matching pattern '*'...\n",
      "action summary:\n",
      "  get (notneeded: 50)\n",
      "âœ“ Downloaded 50 files\n",
      "ðŸ“¥ Downloading 72 files matching pattern '*.json'...\n",
      "action summary:\n",
      "  get (notneeded: 72)\n",
      "âœ“ Downloaded 72 files\n",
      "\n",
      "âœ… Data download complete!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Set a subject and a session to explore\n",
    "SUBJECT = 'sub-01'\n",
    "SESSION = 'ses-010'\n",
    "TR = 1.49\n",
    "REQUIREMENTS_PATH = \"notebooks/03_requirements.txt\" \n",
    "\n",
    "# Detect Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"ðŸš€ Detected Google Colab\")\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    print(\"ðŸ–¥ï¸ Running locally\")\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Define project path\n",
    "PROJECT_PATH = Path.cwd().resolve()\n",
    "if PROJECT_PATH.name != \"mario.tutorials\":\n",
    "    PROJECT_PATH = PROJECT_PATH.parent\n",
    "SOURCEDATA_PATH = PROJECT_PATH / \"sourcedata\"\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Clone the repository if not already present\n",
    "    if not Path(\"mario.tutorials\").exists():\n",
    "        print(\"ðŸ“¥ Cloning mario.tutorials repository...\")\n",
    "        subprocess.run([\"git\", \"clone\", \"https://github.com/courtois-neuromod/mario.tutorials\"], check=True)\n",
    "\n",
    "    # Change to repo directory and add src to path\n",
    "    os.chdir(\"mario.tutorials\")\n",
    "    PROJECT_PATH = Path.cwd().resolve()\n",
    "    sys.path.insert(0, str(PROJECT_PATH / \"src\"))\n",
    "    os.chdir(PROJECT_PATH)\n",
    "\n",
    "    # Install some dependencies required in Colab\n",
    "    from utils import setup_colab_environment\n",
    "    setup_colab_environment()\n",
    "    print(f\"âœ… Repository ready at: {PROJECT_PATH}\")\n",
    "else:\n",
    "    sys.path.insert(0, str(PROJECT_PATH / \"src\"))\n",
    "    os.chdir(PROJECT_PATH)\n",
    "    print(f\"âœ… Repository ready at: {PROJECT_PATH}\")\n",
    "\n",
    "\n",
    "from utils import install_dependencies\n",
    "install_dependencies(REQUIREMENTS_PATH)\n",
    "\n",
    "print(\"\\nâœ… Setup complete!\")\n",
    "\n",
    "\n",
    "### Download the data\n",
    "\n",
    "from utils import download_cneuromod_data\n",
    "\n",
    "# Download event files\n",
    "download_cneuromod_data(\n",
    "    'mario',\n",
    "    subject=SUBJECT,\n",
    "    session=SESSION,\n",
    "    pattern='*.tsv'\n",
    ")\n",
    "download_cneuromod_data(\n",
    "    'mario.annotations',\n",
    "    subject=SUBJECT,\n",
    "    session=SESSION,\n",
    "    pattern='*.tsv'\n",
    ")\n",
    "\n",
    "# Install fMRIPrep dataset\n",
    "download_cneuromod_data('mario.fmriprep',\n",
    "                      subject=SUBJECT,\n",
    "                      session=SESSION,\n",
    "                      pattern='*.json')\n",
    "\n",
    "# Install fMRIPrep dataset\n",
    "download_cneuromod_data('mario.fmriprep',\n",
    "                      subject=SUBJECT,\n",
    "                      session=SESSION,\n",
    "                      pattern='*MNI152NLin2009cAsym*')\n",
    "\n",
    "# Install confounds\n",
    "download_cneuromod_data('mario.fmriprep',\n",
    "                      subject=SUBJECT,\n",
    "                      session=SESSION,\n",
    "                      pattern='*confounds*')\n",
    "download_cneuromod_data('mario.replays',\n",
    "                      subject=SUBJECT,\n",
    "                      session=SESSION,\n",
    "                      pattern='*confs.npy')\n",
    "\n",
    "\n",
    "\n",
    "download_cneuromod_data('anat.smriprep',\n",
    "                      subject=SUBJECT,\n",
    "                      pattern='*')\n",
    "\n",
    "# Download confound info files (low-level features)\n",
    "download_cneuromod_data(\n",
    "    'mario.replays',\n",
    "    subject=SUBJECT,\n",
    "    session=SESSION,\n",
    "    pattern='*.json'\n",
    ")\n",
    "print(\"\\nâœ… Data download complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5aaccc2",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Imports and configuration\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import utilities\n",
    "from utils import (\n",
    "    get_sourcedata_path,\n",
    "    load_events,\n",
    "    get_session_runs,\n",
    "    get_bold_path,\n",
    "    load_bold\n",
    ")\n",
    "\n",
    "# Import RL utilities\n",
    "from rl import apply_pca\n",
    "\n",
    "# Import parcellation utilities\n",
    "from parcellation import (\n",
    "    create_random_parcellation,\n",
    "    extract_parcel_bold_from_parcellation,\n",
    "    load_schaefer_atlas\n",
    ")\n",
    "\n",
    "# Import encoding utilities\n",
    "from encoding import (\n",
    "    fit_atlas_encoding_per_layer,\n",
    "    compare_atlas_layer_performance,\n",
    "    get_top_parcels\n",
    ")\n",
    "\n",
    "# Import caching utilities\n",
    "from parcellation import (\n",
    "    load_complete_results,\n",
    "    check_complete_results_exist,\n",
    "    save_complete_results\n",
    ")\n",
    "\n",
    "# Import visualization functions\n",
    "from visualization import (\n",
    "    plot_network_performance_grid,\n",
    "    plot_glass_brain_r2,\n",
    "    visualize_best_parcel_prediction\n",
    ")\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Get sourcedata path\n",
    "sourcedata_path = get_sourcedata_path()\n",
    "\n",
    "print(\"âœ“ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0ce3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoding Model\n",
    "\n",
    "**Approach:** Ridge regression with PCA-reduced CNN features.\n",
    "\n",
    "**Why ridge?** Efficient, prevents overfitting, interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e9bf216",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 runs: ['run-1', 'run-2', 'run-3', 'run-4']\n",
      "  run-1: 953 events\n",
      "  run-2: 986 events\n",
      "  run-3: 892 events\n",
      "  run-4: 1033 events\n",
      "\n",
      "Loading BOLD data...\n",
      "\n",
      "Creating common brain mask...\n",
      "âœ“ Common mask: 213,443 voxels\n",
      "\n",
      "âœ“ All prerequisites loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load prerequisites\n",
    "\n",
    "from nilearn.masking import compute_multi_epi_mask\n",
    "\n",
    "# Get runs\n",
    "runs = get_session_runs(SUBJECT, SESSION, sourcedata_path)\n",
    "print(f\"Found {len(runs)} runs: {runs}\")\n",
    "\n",
    "# Load events\n",
    "all_events = []\n",
    "for run in runs:\n",
    "    events = load_events(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    all_events.append(events)\n",
    "    print(f\"  {run}: {len(events)} events\")\n",
    "\n",
    "# Load BOLD images and paths\n",
    "print(\"\\nLoading BOLD data...\")\n",
    "bold_imgs = []\n",
    "bold_paths = []\n",
    "for run in runs:\n",
    "    bold_path = get_bold_path(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    bold_img = load_bold(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    bold_paths.append(str(bold_path))  # Convert Path to string for nilearn\n",
    "    bold_imgs.append(bold_img)\n",
    "\n",
    "# Create common mask\n",
    "print(\"\\nCreating common brain mask...\")\n",
    "common_mask = compute_multi_epi_mask(bold_imgs, n_jobs=1)\n",
    "n_voxels = int((common_mask.get_fdata() > 0).sum())\n",
    "print(f\"âœ“ Common mask: {n_voxels:,} voxels\")\n",
    "\n",
    "print(\"\\nâœ“ All prerequisites loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e1530c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading Data\n",
    "\n",
    "Loading: BOLD images (4 runs), events, brain mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "434f2d50",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Found trained model: models/ppo_mario_4_levels_1000k_20251209_224222.pth\n",
      "\n",
      "Loading model...\n",
      "âœ“ Model loaded\n",
      "\n",
      "======================================================================\n",
      "Aligning RL activations to BOLD for sub-01 ses-010\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Processing run-1:\n",
      "--------------------------------------------------\n",
      "  âš  Events file not found: /home/hyruuk/GitHub/neuromod/mario_analysis/mario.tutorials/sourcedata/mario/sub-01/ses-010/func/sub-01_ses-010_task-mario_run-01_events.tsv\n",
      "\n",
      "Processing run-2:\n",
      "--------------------------------------------------\n",
      "  âš  Events file not found: /home/hyruuk/GitHub/neuromod/mario_analysis/mario.tutorials/sourcedata/mario/sub-01/ses-010/func/sub-01_ses-010_task-mario_run-02_events.tsv\n",
      "\n",
      "Processing run-3:\n",
      "--------------------------------------------------\n",
      "  âš  Events file not found: /home/hyruuk/GitHub/neuromod/mario_analysis/mario.tutorials/sourcedata/mario/sub-01/ses-010/func/sub-01_ses-010_task-mario_run-03_events.tsv\n",
      "\n",
      "Processing run-4:\n",
      "--------------------------------------------------\n",
      "  âš  Events file not found: /home/hyruuk/GitHub/neuromod/mario_analysis/mario.tutorials/sourcedata/mario/sub-01/ses-010/func/sub-01_ses-010_task-mario_run-04_events.tsv\n",
      "\n",
      "======================================================================\n",
      "âŒ ERROR: No runs were successfully processed!\n",
      "======================================================================\n",
      "\n",
      "Possible issues:\n",
      "  1. Annotation files not found\n",
      "  2. No gym-retro_game segments in annotation files\n",
      "  3. Replay files (.bk2) not found\n",
      "  4. Error during activation extraction\n",
      "\n",
      "Please check the error messages above.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No valid runs to process. Check annotation and replay files.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ“ Model loaded\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Align activations to BOLD\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# This will:\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# 1. Load replay files for each game segment\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# 4. Apply HRF convolution\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# 5. Create NaN mask for non-gameplay periods\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m alignment_results = \u001b[43malign_activations_to_bold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSUBJECT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSESSION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mruns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mruns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43msourcedata_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43msourcedata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapply_hrf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Apply HRF convolution\u001b[39;49;00m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbold_imgs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbold_imgs\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass BOLD images for exact TR count\u001b[39;49;00m\n\u001b[32m     56\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Extract results\u001b[39;00m\n\u001b[32m     59\u001b[39m layer_activations = alignment_results[\u001b[33m'\u001b[39m\u001b[33mactivations\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/neuromod/mario_analysis/mario.tutorials/src/rl.py:1246\u001b[39m, in \u001b[36malign_activations_to_bold\u001b[39m\u001b[34m(model, subject, session, runs, sourcedata_path, tr, device, apply_hrf, bold_imgs)\u001b[39m\n\u001b[32m   1244\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  4. Error during activation extraction\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1245\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPlease check the error messages above.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1246\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo valid runs to process. Check annotation and replay files.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1248\u001b[39m \u001b[38;5;66;03m# Concatenate all runs\u001b[39;00m\n\u001b[32m   1249\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m70\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: No valid runs to process. Check annotation and replay files."
     ]
    }
   ],
   "source": [
    "# Load and align activations from replays\n",
    "\n",
    "# First, check if we have a trained model\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_DIR = Path('models/')\n",
    "\n",
    "# Find all .pth files\n",
    "model_files = list(MODEL_DIR.glob('*.pth'))\n",
    "\n",
    "if not model_files:\n",
    "    print(f\"âœ— No trained model found in: {MODEL_DIR}\")\n",
    "    print(\"\\nYou need a trained RL agent to extract activations.\")\n",
    "    print(\"Please train an agent first by running:\")\n",
    "    print(\"  python ../train_mario_agent.py --steps 5000000\")\n",
    "    print(\"\\nâš  Cannot proceed with encoding analysis without trained model\")\n",
    "    HAS_MODEL = False\n",
    "else:\n",
    "    # Sort by modification time (newest first)\n",
    "    model_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    MODEL_PATH = model_files[0]\n",
    "    \n",
    "    if len(model_files) > 1:\n",
    "        print(f\"Found {len(model_files)} models. Using the newest one:\")\n",
    "        for mp in model_files:\n",
    "            print(f\"  - {mp.name} ({'selected' if mp == MODEL_PATH else 'skipped'})\")\n",
    "            \n",
    "    print(f\"âœ“ Found trained model: {MODEL_PATH}\")\n",
    "    HAS_MODEL = True\n",
    "    \n",
    "    # Load the model\n",
    "    from rl import load_pretrained_model, align_activations_to_bold\n",
    "    \n",
    "    print(\"\\nLoading model...\")\n",
    "    model = load_pretrained_model(MODEL_PATH, device='cpu')\n",
    "    print(\"âœ“ Model loaded\")\n",
    "    \n",
    "    # Align activations to BOLD\n",
    "    # This will:\n",
    "    # 1. Load replay files for each game segment\n",
    "    # 2. Extract RL activations at 60Hz  \n",
    "    # 3. Downsample to TR (1.49s)\n",
    "    # 4. Apply HRF convolution\n",
    "    # 5. Create NaN mask for non-gameplay periods\n",
    "    \n",
    "    alignment_results = align_activations_to_bold(\n",
    "        model=model,\n",
    "        subject=SUBJECT,\n",
    "        session=SESSION,\n",
    "        runs=runs,\n",
    "        sourcedata_path=sourcedata_path,\n",
    "        tr=TR,\n",
    "        device='cpu',\n",
    "        apply_hrf=True,  # Apply HRF convolution\n",
    "        bold_imgs=bold_imgs  # Pass BOLD images for exact TR count\n",
    "    )\n",
    "    \n",
    "    # Extract results\n",
    "    layer_activations = alignment_results['activations']\n",
    "    valid_mask = alignment_results['mask']\n",
    "    run_info = alignment_results['run_info']\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Alignment summary:\")\n",
    "    for info in run_info:\n",
    "        print(f\"  {info['run']}: {info['n_valid_trs']}/{info['n_trs']} TRs \"\n",
    "              f\"({info['n_segments']} game segments)\")\n",
    "    print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535d9cdd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading RL Activations\n",
    "\n",
    "Extracting CNN activations from replay files â†’ downsampling to TR â†’ applying HRF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c46966",
   "metadata": {},
   "source": [
    "## Load Cached Results (if available)\n",
    "\n",
    "Check if we have already computed results. If yes, load them and skip all processing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab462b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if cached results exist and load if available\n",
    "\n",
    "from parcellation import load_complete_results, check_complete_results_exist\n",
    "\n",
    "# Define cache file path\n",
    "CACHE_DIR = Path('derivatives/encoding_results')\n",
    "CACHE_FILE = CACHE_DIR / f'{SUBJECT}_{SESSION}_complete_results.pkl'\n",
    "\n",
    "# Try to load cached results\n",
    "cached_results = load_complete_results(CACHE_FILE)\n",
    "\n",
    "if cached_results is not None:\n",
    "    # Extract all variables from cache\n",
    "    parcel_bold = cached_results['parcel_bold']\n",
    "    all_pca_results = cached_results['all_pca_results']\n",
    "    all_encoding_results = cached_results['all_encoding_results']\n",
    "    train_indices = cached_results['train_indices']\n",
    "    test_indices = cached_results['test_indices']\n",
    "    valid_mask_cached = cached_results['valid_mask']\n",
    "\n",
    "    # Reconstruct atlas dictionary (we only saved labels)\n",
    "    atlas = {\n",
    "        'labels': cached_results['atlas_labels']\n",
    "    }\n",
    "\n",
    "    # Set PCA dims from metadata\n",
    "    pca_dims = cached_results['metadata']['pca_dims']\n",
    "\n",
    "    # Set flag to skip processing\n",
    "    USE_CACHED_RESULTS = True\n",
    "\n",
    "    print(\"\\nâš¡ USING CACHED RESULTS - ALL PROCESSING SKIPPED! âš¡\")\n",
    "    print(f\"\\nTo force recomputation, delete: {CACHE_FILE}\\n\")\n",
    "else:\n",
    "    print(\"\\nðŸ“Š No cache found - will process from scratch and save results\")\n",
    "    print(f\"Results will be saved to: {CACHE_FILE}\\n\")\n",
    "    USE_CACHED_RESULTS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5490c7fd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create random parcellation and extract parcel-averaged BOLD\n",
    "\n",
    "if not USE_CACHED_RESULTS:\n",
    "    from parcellation import create_random_parcellation, extract_parcel_bold_from_parcellation, get_parcel_labels\n",
    "    from nilearn.interfaces.fmriprep import load_confounds\n",
    "\n",
    "    print(\"Creating random parcellation...\")\n",
    "    n_parcels = 400\n",
    "    parcellation_img = create_random_parcellation(common_mask, n_parcels=n_parcels)\n",
    "    print(f\"âœ“ Created {n_parcels} random parcels\\n\")\n",
    "\n",
    "    # Load confounds for each run\n",
    "    print(\"Loading confounds...\")\n",
    "    confounds_list = []\n",
    "    for bold_path in bold_paths:\n",
    "        confounds, _ = load_confounds(\n",
    "            bold_path,\n",
    "            strategy=[\"motion\", \"high_pass\", \"wm_csf\"],\n",
    "            motion=\"basic\",\n",
    "            wm_csf=\"basic\"\n",
    "        )\n",
    "        confounds_list.append(confounds)\n",
    "\n",
    "    # Extract parcel-averaged BOLD\n",
    "    print(\"Extracting parcel-averaged BOLD...\")\n",
    "    print(\"  (Averaging ~534 voxels per parcel)\")\n",
    "    print(\"  (Confound regression + detrending + standardization)\")\n",
    "    parcel_bold = extract_parcel_bold_from_parcellation(\n",
    "        bold_imgs,\n",
    "        parcellation_img,\n",
    "        confounds_list=confounds_list,\n",
    "        detrend=True,\n",
    "        standardize=True,\n",
    "        t_r=TR\n",
    "    )\n",
    "\n",
    "    # Create simple parcel labels\n",
    "    parcel_labels = get_parcel_labels(parcellation_img)\n",
    "    \n",
    "    # Create atlas dictionary for compatibility with existing code\n",
    "    atlas = {\n",
    "        'maps': parcellation_img,\n",
    "        'labels': parcel_labels\n",
    "    }\n",
    "\n",
    "    print(f\"\\nâœ“ Parcel BOLD prepared:\")\n",
    "    print(f\"  Shape: {parcel_bold.shape}\")\n",
    "    print(f\"  Timepoints: {parcel_bold.shape[0]}\")\n",
    "    print(f\"  Parcels: {parcel_bold.shape[1]}\")\n",
    "    print(f\"  Mean: {parcel_bold.mean():.6f}, Std: {parcel_bold.std():.6f}\")\n",
    "    print(f\"\\n  Speedup: ~{n_voxels // parcel_bold.shape[1]:.0f}x faster than voxel-wise!\")\n",
    "else:\n",
    "    print(\"â­ï¸  Skipping BOLD extraction (using cached data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec498b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cleaning BOLD with Random Parcellation\n",
    "\n",
    "**Random parcellation approach:**\n",
    "1. Create 400 random parcels ensuring uniform whole-brain coverage\n",
    "2. Average BOLD signal within each parcel (~534 voxels â†’ 1 value)\n",
    "3. Apply confound regression, detrending, standardization\n",
    "\n",
    "**Benefits:**\n",
    "- **~534x faster** than voxel-wise (400 parcels vs 213k voxels)\n",
    "- **Better RÂ²** due to noise reduction from averaging\n",
    "- **Uniform coverage** (no missing brain regions like functional atlases)\n",
    "- **Simpler** (no atlas alignment issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485af949",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOLD timepoints: 1794\n",
      "Activations timepoints: 1794\n",
      "Valid (gameplay) timepoints: 1692\n",
      "Invalid (non-gameplay) timepoints: 102\n",
      "\n",
      "âœ“ Dimensions match!\n"
     ]
    }
   ],
   "source": [
    "# Check alignment between BOLD and activations\n",
    "\n",
    "if HAS_MODEL:\n",
    "    n_bold = parcel_bold.shape[0]\n",
    "    n_acts = list(layer_activations.values())[0].shape[0]\n",
    "\n",
    "    print(f\"BOLD timepoints: {n_bold}\")\n",
    "    print(f\"Activations timepoints: {n_acts}\")\n",
    "    print(f\"Valid (gameplay) timepoints: {valid_mask.sum()}\")\n",
    "    print(f\"Invalid (non-gameplay) timepoints: {(~valid_mask).sum()}\")\n",
    "\n",
    "    # Ensure dimensions match\n",
    "    if n_bold != n_acts:\n",
    "        print(f\"\\nâš  Dimension mismatch!\")\n",
    "        print(f\"  Truncating to minimum length: {min(n_bold, n_acts)}\")\n",
    "        n_time = min(n_bold, n_acts)\n",
    "        parcel_bold = parcel_bold[:n_time]\n",
    "        valid_mask = valid_mask[:n_time]\n",
    "        for layer in layer_activations.keys():\n",
    "            layer_activations[layer] = layer_activations[layer][:n_time]\n",
    "    else:\n",
    "        print(\"\\nâœ“ Dimensions match!\")\n",
    "else:\n",
    "    print(\"âš  No model available, skipping alignment check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f9d9ba",
   "metadata": {},
   "source": [
    "## Load Cached Results (if available)\n",
    "\n",
    "Check if we have already computed results. If yes, load them and skip all processing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e5269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if cached results exist and load if available\n",
    "\n",
    "from parcellation import load_complete_results, check_complete_results_exist\n",
    "\n",
    "# Define cache file path\n",
    "CACHE_DIR = Path('derivatives/encoding_results')\n",
    "CACHE_FILE = CACHE_DIR / f'{SUBJECT}_{SESSION}_complete_results.pkl'\n",
    "\n",
    "# Try to load cached results\n",
    "cached_results = load_complete_results(CACHE_FILE)\n",
    "\n",
    "if cached_results is not None:\n",
    "    # Extract all variables from cache\n",
    "    parcel_bold = cached_results['parcel_bold']\n",
    "    all_pca_results = cached_results['all_pca_results']\n",
    "    all_encoding_results = cached_results['all_encoding_results']\n",
    "    train_indices = cached_results['train_indices']\n",
    "    test_indices = cached_results['test_indices']\n",
    "    valid_mask_cached = cached_results['valid_mask']\n",
    "\n",
    "    # Reconstruct atlas dictionary (we only saved labels)\n",
    "    atlas = {\n",
    "        'labels': cached_results['atlas_labels']\n",
    "    }\n",
    "\n",
    "    # Set PCA dims from metadata\n",
    "    pca_dims = cached_results['metadata']['pca_dims']\n",
    "\n",
    "    # Set flag to skip processing\n",
    "    USE_CACHED_RESULTS = True\n",
    "\n",
    "    print(\"\\nâš¡ USING CACHED RESULTS - ALL PROCESSING SKIPPED! âš¡\")\n",
    "    print(f\"\\nTo force recomputation, delete: {CACHE_FILE}\\n\")\n",
    "else:\n",
    "    print(\"\\nðŸ“Š No cache found - will process from scratch and save results\")\n",
    "    print(f\"Results will be saved to: {CACHE_FILE}\\n\")\n",
    "    USE_CACHED_RESULTS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a2b66",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Alignment\n",
    "\n",
    "Activations aligned to BOLD using HRF convolution and temporal downsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f09db7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â­ï¸  Skipping train/test split (using cached splits)\n",
      "  Train: 1184 TRs, Test: 508 TRs\n"
     ]
    }
   ],
   "source": [
    "# Create random train/test split\n",
    "\n",
    "if not USE_CACHED_RESULTS:\n",
    "    print(\"Setting up random train/test split (70/30)...\")\n",
    "    print(\"\\nNote: We use random split instead of leave-run-out cross-validation.\")\n",
    "    print(\"This is because CNN features show strong temporal drift across runs,\")\n",
    "    print(\"making cross-run generalization impossible.\\n\")\n",
    "\n",
    "    # Get all valid (gameplay) indices\n",
    "    all_valid_indices = np.where(valid_mask)[0]\n",
    "\n",
    "    # Random split (70/30)\n",
    "    np.random.seed(42)\n",
    "    n_train = int(len(all_valid_indices) * 0.7)\n",
    "    shuffled = np.random.permutation(all_valid_indices)\n",
    "    train_indices = shuffled[:n_train]\n",
    "    test_indices = shuffled[n_train:]\n",
    "\n",
    "    print(f\"Train/test split:\")\n",
    "    print(f\"  Train: {len(train_indices)} TRs (70%)\")\n",
    "    print(f\"  Test:  {len(test_indices)} TRs (30%)\")\n",
    "    print(f\"\\nâœ“ Split complete!\")\n",
    "else:\n",
    "    print(\"â­ï¸  Skipping train/test split (using cached splits)\")\n",
    "    print(f\"  Train: {len(train_indices)} TRs, Test: {len(test_indices)} TRs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78103a6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train/Test Split\n",
    "\n",
    "**Strategy:** Random 70/30 split (not leave-run-out)\n",
    "\n",
    "**Why?** CNN features show temporal drift across runs, making cross-run prediction fail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d068630",
   "metadata": {},
   "source": [
    "## PCA Dimensionality Reduction\n",
    "\n",
    "**Why?** CNN layers have 500-50k features, too many for limited training data.\n",
    "\n",
    "**Testing:** 10, 50, 100 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05711c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â­ï¸  Skipping PCA (using cached PCA results)\n",
      "  PCA dimensions: [10, 50, 100]\n"
     ]
    }
   ],
   "source": [
    "# Apply PCA dimensionality reduction\n",
    "\n",
    "if not USE_CACHED_RESULTS:\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # Test multiple PCA dimensions\n",
    "    pca_dims = [10, 50, 100]\n",
    "\n",
    "    print(\"Applying PCA with multiple dimensions...\")\n",
    "    print(\"(PCA is fit only on training data)\\n\")\n",
    "\n",
    "    # Store results for each dimension\n",
    "    all_pca_results = {}\n",
    "\n",
    "    for n_comp in pca_dims:\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"PCA with {n_comp} components\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        pca_results = {}\n",
    "\n",
    "        for layer_name, layer_acts in layer_activations.items():\n",
    "            # Get train/test data\n",
    "            X_train = layer_acts[train_indices]\n",
    "            X_test = layer_acts[test_indices]\n",
    "\n",
    "            # Step 1: Standardize (fit on train only)\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "            # Step 2: PCA (fit on train only)\n",
    "            n_components_actual = min(n_comp, X_train_scaled.shape[1])\n",
    "            pca = PCA(n_components=n_components_actual, random_state=42)\n",
    "            X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "            X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "            # Reconstruct full arrays with NaN for non-gameplay TRs\n",
    "            X_train_full = np.full((len(layer_acts), n_components_actual), np.nan)\n",
    "            X_test_full = np.full((len(layer_acts), n_components_actual), np.nan)\n",
    "            X_train_full[train_indices] = X_train_pca\n",
    "            X_test_full[test_indices] = X_test_pca\n",
    "\n",
    "            # Combine train and test (one will have NaN where the other has data)\n",
    "            X_combined = np.where(np.isnan(X_train_full), X_test_full, X_train_full)\n",
    "\n",
    "            pca_results[layer_name] = {\n",
    "                'data': X_combined,\n",
    "                'pca': pca,\n",
    "                'scaler': scaler,\n",
    "                'variance_explained': pca.explained_variance_ratio_.sum()\n",
    "            }\n",
    "\n",
    "            print(f\"  {layer_name}:\")\n",
    "            print(f\"    Original: {layer_acts.shape[1]} features\")\n",
    "            print(f\"    Reduced:  {n_components_actual} components\")\n",
    "            print(f\"    Variance explained: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "        all_pca_results[n_comp] = pca_results\n",
    "        print()\n",
    "\n",
    "    print(f\"{'='*70}\")\n",
    "    print(\"âœ“ PCA complete for all dimensions!\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "else:\n",
    "    print(\"â­ï¸  Skipping PCA (using cached PCA results)\")\n",
    "    pca_dims = cached_results['metadata']['pca_dims']\n",
    "    print(f\"  PCA dimensions: {pca_dims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299f14cd",
   "metadata": {},
   "source": [
    "## Fitting Models (Atlas-Based) with Caching\n",
    "\n",
    "Fitting ridge regression for 400 parcels per layer. Each parcel gets its own optimal alpha.\n",
    "\n",
    "**Caching System:**\n",
    "- **First run:** Computes models and saves results to disk (~6-10 minutes)\n",
    "- **Subsequent runs:** Loads from cache (~1 second!)\n",
    "- Cache location: `derivatives/encoding_results/{subject}_{session}_atlas_encoding_results.pkl`\n",
    "\n",
    "**To force recomputation:** Delete the cache file and re-run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e165da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit ridge regression encoding models (atlas-based)\n",
    "\n",
    "if not USE_CACHED_RESULTS:\n",
    "    from encoding import fit_atlas_encoding_per_layer\n",
    "\n",
    "    alphas = [0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "\n",
    "    # Store results for each dimension\n",
    "    all_encoding_results = {}\n",
    "\n",
    "    for n_comp in pca_dims:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Fitting encoding models for {n_comp} PCA components\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "\n",
    "        # Extract PCA-reduced activations\n",
    "        pca_activations = {}\n",
    "        for layer_name, result in all_pca_results[n_comp].items():\n",
    "            pca_activations[layer_name] = result['data']\n",
    "\n",
    "        print(f\"Fitting ridge regression ({len(layer_activations)} layers Ã— {n_comp} components Ã— {parcel_bold.shape[1]} parcels)...\")\n",
    "        print(\"This will take ~2-3 minutes...\\n\")\n",
    "\n",
    "        encoding_results = fit_atlas_encoding_per_layer(\n",
    "            pca_activations,\n",
    "            parcel_bold,\n",
    "            atlas,\n",
    "            train_indices,\n",
    "            test_indices,\n",
    "            alphas=alphas,\n",
    "            valid_mask=valid_mask\n",
    "        )\n",
    "\n",
    "        all_encoding_results[n_comp] = encoding_results\n",
    "        print(f\"\\nâœ“ Encoding complete for {n_comp} components!\")\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"All encoding models fitted!\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "else:\n",
    "    print(\"â­ï¸  Skipping encoding (using cached encoding results)\")\n",
    "    print(f\"  Encoding results for {len(all_encoding_results)} PCA dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df219937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete results to disk\n",
    "\n",
    "if not USE_CACHED_RESULTS:\n",
    "    from parcellation import save_complete_results\n",
    "\n",
    "    save_complete_results(\n",
    "        parcel_bold=parcel_bold,\n",
    "        all_pca_results=all_pca_results,\n",
    "        all_encoding_results=all_encoding_results,\n",
    "        train_indices=train_indices,\n",
    "        test_indices=test_indices,\n",
    "        valid_mask=valid_mask,\n",
    "        atlas=atlas,\n",
    "        filepath=CACHE_FILE,\n",
    "        pca_dims=pca_dims,\n",
    "        subject=SUBJECT,\n",
    "        session=SESSION\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nâ­ï¸  Results already loaded from cache - no need to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8789f7f",
   "metadata": {},
   "source": [
    "## Comparing Performance\n",
    "\n",
    "**Questions:** Which layer predicts best? Which PCA dimension is optimal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c1a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare layer performance across PCA dimensions\n",
    "\n",
    "from encoding import compare_atlas_layer_performance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LAYER PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_comparisons = {}\n",
    "\n",
    "for n_comp in pca_dims:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PCA: {n_comp} components\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    comparison_df = compare_atlas_layer_performance(all_encoding_results[n_comp])\n",
    "    all_comparisons[n_comp] = comparison_df\n",
    "\n",
    "    print(comparison_df.to_string(index=False))\n",
    "\n",
    "    best_layer = comparison_df.iloc[0]['layer']\n",
    "    best_r2 = comparison_df.iloc[0]['mean_r2']\n",
    "    best_median = comparison_df.iloc[0]['median_r2']\n",
    "    print(f\"\\nâ­ Best layer: {best_layer.upper()}\")\n",
    "    print(f\"   Mean RÂ² = {best_r2:.4f}, Median RÂ² = {best_median:.4f}\")\n",
    "\n",
    "# Create comparison plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "layer_order = ['conv1', 'conv2', 'conv3', 'conv4', 'linear']\n",
    "\n",
    "for idx, n_comp in enumerate(pca_dims):\n",
    "    ax = axes[idx]\n",
    "    comparison_df = all_comparisons[n_comp]\n",
    "\n",
    "    # Reorder by layer_order\n",
    "    comparison_df['layer'] = pd.Categorical(comparison_df['layer'], categories=layer_order, ordered=True)\n",
    "    comparison_df = comparison_df.sort_values('layer')\n",
    "\n",
    "    # Bar plot\n",
    "    bars = ax.bar(comparison_df['layer'], comparison_df['mean_r2'],\n",
    "                  color='steelblue', alpha=0.8, edgecolor='black')\n",
    "\n",
    "    # Highlight best layer\n",
    "    best_idx = comparison_df['mean_r2'].argmax()\n",
    "    bars[best_idx].set_color('darkorange')\n",
    "\n",
    "    ax.set_xlabel('Layer', fontsize=12)\n",
    "    ax.set_ylabel('Mean RÂ²', fontsize=12)\n",
    "    ax.set_title(f'{n_comp} PCA Components', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_ylim([0, max(0.05, comparison_df['mean_r2'].max() * 1.2)])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY: Best performance for each PCA dimension\")\n",
    "print(f\"{'='*80}\")\n",
    "summary_data = []\n",
    "for n_comp in pca_dims:\n",
    "    comparison_df = all_comparisons[n_comp]\n",
    "    best_row = comparison_df.iloc[0]\n",
    "    summary_data.append({\n",
    "        'pca_dims': n_comp,\n",
    "        'best_layer': best_row['layer'],\n",
    "        'mean_r2': best_row['mean_r2'],\n",
    "        'median_r2': best_row['median_r2'],\n",
    "        'pct_positive': best_row['pct_positive']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b12310",
   "metadata": {},
   "source": [
    "## Brain Maps\n",
    "\n",
    "Visualizing where in the brain RL features are encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3675cb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top parcels for best performing layer\n",
    "\n",
    "from encoding import get_top_parcels\n",
    "\n",
    "# Find best overall performance\n",
    "best_n_comp = None\n",
    "best_layer = None\n",
    "best_r2 = -np.inf\n",
    "\n",
    "for n_comp in pca_dims:\n",
    "    comparison_df = all_comparisons[n_comp]\n",
    "    top_r2 = comparison_df.iloc[0]['mean_r2']\n",
    "    if top_r2 > best_r2:\n",
    "        best_r2 = top_r2\n",
    "        best_n_comp = n_comp\n",
    "        best_layer = comparison_df.iloc[0]['layer']\n",
    "\n",
    "print(f\"Best overall performance:\")\n",
    "print(f\"  Layer: {best_layer}\")\n",
    "print(f\"  PCA components: {best_n_comp}\")\n",
    "print(f\"  Mean RÂ²: {best_r2:.4f}\\n\")\n",
    "\n",
    "# Get top 20 parcels\n",
    "top_parcels = get_top_parcels(all_encoding_results[best_n_comp], best_layer, n_top=20)\n",
    "\n",
    "print(f\"Top 20 parcels for {best_layer} ({best_n_comp} PCA components):\\n\")\n",
    "print(top_parcels.to_string(index=False))\n",
    "\n",
    "# Plot distribution\n",
    "result = all_encoding_results[best_n_comp][best_layer]\n",
    "r2_test = result['r2_test']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(r2_test[r2_test > 0], bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(np.mean(r2_test), color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Mean = {np.mean(r2_test):.4f}')\n",
    "axes[0].axvline(np.median(r2_test), color='orange', linestyle='--', linewidth=2,\n",
    "                label=f'Median = {np.median(r2_test):.4f}')\n",
    "axes[0].set_xlabel('RÂ²')\n",
    "axes[0].set_ylabel('Number of parcels')\n",
    "axes[0].set_title(f'{best_layer} - RÂ² Distribution', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Top parcels bar plot\n",
    "top_10_for_plot = top_parcels.head(10)\n",
    "\n",
    "axes[1].barh(range(len(top_10_for_plot)), top_10_for_plot['r2'][::-1],\n",
    "             color='steelblue', alpha=0.8, edgecolor='black')\n",
    "axes[1].set_yticks(range(len(top_10_for_plot)))\n",
    "axes[1].set_yticklabels(top_10_for_plot['label'].tolist()[::-1], fontsize=9)\n",
    "axes[1].set_xlabel('RÂ²')\n",
    "axes[1].set_title('Top 10 Parcels', fontweight='bold')\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b33242",
   "metadata": {},
   "source": [
    "## Brain Surface Visualization\n",
    "\n",
    "Plotting RÂ² values on a glass brain with parcels.\n",
    "\n",
    "Shows which brain regions are best predicted by RL agent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "network_grid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Performance Grid: All CNN Layers\n",
    "\n",
    "from visualization import plot_network_performance_grid\n",
    "from parcellation import load_schaefer_atlas\n",
    "\n",
    "# Load Schaefer atlas for network labels\n",
    "atlas_schaefer = load_schaefer_atlas(n_rois=400, yeo_networks=7)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"NETWORK PERFORMANCE ACROSS CNN LAYERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig_grid = plot_network_performance_grid(\n",
    "    all_encoding_results,\n",
    "    best_n_comp,\n",
    "    atlas_schaefer,\n",
    "    figsize=(20, 12)\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Network performance grid complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a8e2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glass Brain View\n",
    "\n",
    "from visualization import plot_glass_brain_r2\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GLASS BRAIN VIEW\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig_glass = plot_glass_brain_r2(\n",
    "    all_encoding_results[best_n_comp],\n",
    "    best_layer,\n",
    "    atlas_schaefer,\n",
    "    r2_threshold=0.05\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Glass brain plot complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d79ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Trace for Best Parcel\n",
    "\n",
    "from visualization import visualize_best_parcel_prediction\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ACTUAL VS PREDICTED SIGNAL (BEST PARCEL)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Reconstruct PCA activations\n",
    "pca_activations = {}\n",
    "for layer, res in all_pca_results[best_n_comp].items():\n",
    "    pca_activations[layer] = res['data']\n",
    "\n",
    "fig_trace = visualize_best_parcel_prediction(\n",
    "    pca_activations,\n",
    "    parcel_bold,\n",
    "    atlas_schaefer,\n",
    "    train_indices,\n",
    "    test_indices,\n",
    "    best_layer,\n",
    "    all_encoding_results[best_n_comp]\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Prediction trace complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5564052a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Key Finding:** CNN features predict brain activity with RÂ² ~ 0.01-0.02 (typical for untrained networks).\n",
    "\n",
    "**Method:**\n",
    "- Random train/test split (avoids cross-run distribution shift)\n",
    "- PCA dimensionality reduction\n",
    "- Voxel-wise ridge regression (each voxel gets optimal alpha)\n",
    "\n",
    "**Limitations:**\n",
    "- Untrained CNN (task-trained network would improve results)\n",
    "- Linear model (nonlinear methods may capture more variance)\n",
    "- Limited data (~1200 training TRs)\n",
    "\n",
    "**Next steps:** Try pretrained CNN, nonlinear models, ROI analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
