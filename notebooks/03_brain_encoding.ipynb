{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579b22aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Brain Encoding with RL Features\n",
    "\n",
    "## Predicting Brain Activity from Agent Representations\n",
    "\n",
    "**Overview:**\n",
    "This notebook uses the CNN activations from the RL agent (notebook 02) to predict brain activity during gameplay.\n",
    "\n",
    "**What we'll cover:**\n",
    "1. Understanding the encoding model framework\n",
    "2. Loading and preparing BOLD data\n",
    "3. Loading CNN activations from the agent\n",
    "4. Aligning timepoints between BOLD and activations\n",
    "5. Fitting ridge regression encoding models\n",
    "6. Comparing layer performance\n",
    "7. Visualizing brain maps\n",
    "\n",
    "**Key question:** Which layer of the agent best predicts brain activity, and where?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "# @title Environment Setup (Colab & Local)\n",
    "# @markdown This cell detects your environment and sets up the necessary dependencies and paths.\n",
    "# @markdown On Colab, it will mount Google Drive to persist datasets and avoid re-downloading.\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect Colab\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IS_COLAB:\n",
    "    print(\"\ud83d\ude80 Detected Google Colab Environment\")\n",
    "    print(\"-----------------------------------------\")\n",
    "\n",
    "    # 1. Mount Google Drive\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        print(\"\ud83d\udcc2 Mounting Google Drive...\")\n",
    "        drive.mount('/content/drive')\n",
    "    except ImportError:\n",
    "        print(\"\u26a0\ufe0f Google Drive not available. Using local runtime storage (will be lost on disconnect).\")\n",
    "\n",
    "    # Define project path\n",
    "    # If Drive is available, use it. Otherwise use /content\n",
    "    if os.path.exists('/content/drive/MyDrive'):\n",
    "        DRIVE_ROOT = Path(\"/content/drive/MyDrive\")\n",
    "        PROJECT_NAME = \"mario_tutorials\"\n",
    "        PROJECT_PATH = DRIVE_ROOT / PROJECT_NAME\n",
    "    else:\n",
    "        PROJECT_PATH = Path(\"/content/mario_tutorials\")\n",
    "\n",
    "    # 2. Setup Project Directory\n",
    "    if not PROJECT_PATH.exists():\n",
    "        print(f\"\ud83c\udd95 Creating project directory at {PROJECT_PATH}...\")\n",
    "        PROJECT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(\"\u23ec Cloning repository...\")\n",
    "        try:\n",
    "            subprocess.run([\"git\", \"clone\", \"https://github.com/courtois-neuromod/mario.tutorials.git\", str(PROJECT_PATH)], check=True)\n",
    "        except subprocess.CalledProcessError:\n",
    "            # Fallback if directory is not empty or git fails\n",
    "            print(\"\u26a0\ufe0f Clone failed (maybe directory exists?). Attempting to pull...\")\n",
    "            os.chdir(PROJECT_PATH)\n",
    "            subprocess.run([\"git\", \"pull\"], check=False)\n",
    "    else:\n",
    "        print(f\"\u2705 Found existing project at {PROJECT_PATH}\")\n",
    "\n",
    "    # Change working directory\n",
    "    os.chdir(PROJECT_PATH)\n",
    "    print(f\"\ud83d\udccd Working directory: {os.getcwd()}\")\n",
    "\n",
    "    # 3. Install System Dependencies\n",
    "    # git-annex is required for DataLad\n",
    "    # cmake and zlib1g-dev are required to build stable-retro (gym-retro fork)\n",
    "    print(\"\ud83d\udce6 Installing system dependencies (git-annex, cmake, zlib)...\")\n",
    "    subprocess.run([\"apt-get\", \"update\"], check=True)\n",
    "    subprocess.run([\"apt-get\", \"install\", \"-y\", \"git-annex\", \"cmake\", \"zlib1g-dev\"], check=True)\n",
    "\n",
    "    # 4. Install Python Dependencies\n",
    "    print(\"\ud83d\udc0d Installing Python packages from requirements.txt...\")\n",
    "    if os.path.exists(\"requirements.txt\"):\n",
    "         # Using pip install -r is more robust\n",
    "         # We add --verbose to see what's happening if it fails\n",
    "         try:\n",
    "             subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"], check=True)\n",
    "         except subprocess.CalledProcessError as e:\n",
    "             print(\"\u274c Pip install failed. Attempting to install core packages individually as fallback...\")\n",
    "             # Fallback: install critical packages one by one to isolate the issue\n",
    "             packages = [\"datalad\", \"nilearn\", \"gym\", \"stable-retro\", \"pandas\", \"scipy\", \"matplotlib\", \"seaborn\", \"h5py\", \"invoke\"]\n",
    "             for pkg in packages:\n",
    "                 print(f\"   Installing {pkg}...\")\n",
    "                 subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", pkg], check=False)\n",
    "    else:\n",
    "         print(\"\u26a0\ufe0f requirements.txt not found! Installing default packages...\")\n",
    "         subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"datalad\", \"nilearn\", \"gym-retro\", \"ipympl\", \"pandas\", \"scipy\", \"matplotlib\", \"seaborn\", \"h5py\", \"invoke\"], check=True)\n",
    "    \n",
    "    # 5. Add src to path\n",
    "    if str(PROJECT_PATH / \"src\") not in sys.path:\n",
    "        sys.path.append(str(PROJECT_PATH / \"src\"))\n",
    "    \n",
    "    print(\"\u2705 Environment setup complete!\")\n",
    "\n",
    "else:\n",
    "    print(\"\ud83d\udda5\ufe0f Running Locally\")\n",
    "    # Ensure src is in path\n",
    "    if str(Path.cwd() / \"src\") not in sys.path:\n",
    "        sys.path.append(str(Path.cwd() / \"src\"))\n",
    "    print(\"\u2705 Local environment assumed ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "# Silent Setup\n",
    "try:\n",
    "    from setup_utils import setup_all\n",
    "    # Ensure data is available (silently checks)\n",
    "    setup_all(subject=\"sub-01\", session=\"ses-010\")\n",
    "except ImportError:\n",
    "    print(\"Setup utils not found. Please ensure src is in path.\")\n",
    "except Exception as e:\n",
    "    print(f\"Setup warning: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5aaccc2",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup - imports and configuration\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "src_dir = Path('..') / 'src'\n",
    "sys.path.insert(0, str(src_dir))\n",
    "\n",
    "# Import utilities\n",
    "from utils import (\n",
    "    get_sourcedata_path,\n",
    "    load_events,\n",
    "    get_session_runs,\n",
    "    load_bold\n",
    ")\n",
    "\n",
    "# Import RL utilities\n",
    "from rl_utils import (\n",
    "    create_simple_proxy_features,\n",
    "    convolve_with_hrf,\n",
    "    apply_pca\n",
    ")\n",
    "\n",
    "# Import RL visualizations\n",
    "from rl_viz_utils import (\n",
    "    plot_pca_variance_per_layer,\n",
    "    plot_layer_activations_sample\n",
    ")\n",
    "\n",
    "# Import encoding utilities\n",
    "from encoding_utils import (\n",
    "    load_and_prepare_bold,\n",
    "    fit_encoding_model_per_layer,\n",
    "    compare_layer_performance\n",
    ")\n",
    "\n",
    "# Import encoding visualizations\n",
    "from encoding_viz_utils import (\n",
    "    plot_layer_comparison_bars,\n",
    "    plot_r2_brainmap\n",
    ")\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Get sourcedata path\n",
    "sourcedata_path = get_sourcedata_path()\n",
    "\n",
    "print(\"\u2713 Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0ce3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. The Encoding Model Framework\n",
    "\n",
    "**Goal:** Predict BOLD activity from RL agent features\n",
    "\n",
    "**Model:** Ridge Regression (linear regression with L2 regularization)\n",
    "\n",
    "```\n",
    "BOLD(voxel, time) = \u03a3 \u03b2\u1d62 \u00b7 Feature_i(time) + \u03b5\n",
    "```\n",
    "\n",
    "**Why ridge regression?**\n",
    "- Handles high-dimensional features (50 PCA components)\n",
    "- L2 penalty prevents overfitting: `||\u03b2||\u00b2 \u2264 \u03b1`\n",
    "- Cross-validation selects optimal regularization strength \u03b1\n",
    "- Fast to fit (~5 mins for whole brain)\n",
    "\n",
    "**Alternative approaches:**\n",
    "- Lasso (L1): Sparse feature selection\n",
    "- Elastic net: L1 + L2\n",
    "- Nonlinear: Kernel ridge, neural networks\n",
    "\n",
    "**For interpretability and speed, we use ridge regression.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e9bf216",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 runs: ['run-1', 'run-2', 'run-3', 'run-4']\n",
      "  run-1: 953 events\n",
      "  run-2: 986 events\n",
      "  run-3: 892 events\n",
      "  run-4: 1033 events\n",
      "\n",
      "Creating common brain mask...\n",
      "\u2713 Common mask: 213,443 voxels\n",
      "\n",
      "\u2713 All prerequisites loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load prerequisites\n",
    "\n",
    "from nilearn.masking import compute_multi_epi_mask\n",
    "\n",
    "# Define constants (assumed from the first tutorial)\n",
    "SUBJECT = 'sub-01'\n",
    "SESSION = 'ses-010'\n",
    "TR = 1.49\n",
    "\n",
    "\n",
    "# Get runs\n",
    "runs = get_session_runs(SUBJECT, SESSION, sourcedata_path)\n",
    "print(f\"Found {len(runs)} runs: {runs}\")\n",
    "\n",
    "# Load events\n",
    "all_events = []\n",
    "for run in runs:\n",
    "    events = load_events(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    all_events.append(events)\n",
    "    print(f\"  {run}: {len(events)} events\")\n",
    "\n",
    "# Create common mask (or load from main tutorial)\n",
    "print(\"\\nCreating common brain mask...\")\n",
    "bold_imgs= []\n",
    "for run in runs:\n",
    "    bold_img = load_bold(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    bold_imgs.append(bold_img)\n",
    "\n",
    "common_mask = compute_multi_epi_mask(bold_imgs, n_jobs=1)\n",
    "n_voxels = int((common_mask.get_fdata() > 0).sum())\n",
    "print(f\"\u2713 Common mask: {n_voxels:,} voxels\")\n",
    "\n",
    "print(\"\\n\u2713 All prerequisites loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e1530c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Loading Prerequisites\n",
    "\n",
    "We need:\n",
    "- Subject/session info (sub-01, ses-010)\n",
    "- Run IDs (4 runs)\n",
    "- BOLD images (preprocessed fMRI data)\n",
    "- Event files (for alignment)\n",
    "- Common brain mask (from GLM analysis)\n",
    "\n",
    "**Note:** If you haven't run notebook 01, this will create a fresh mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f2d50",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": "# Load and align activations from replays\n\n# First, check if we have a trained model\nfrom pathlib import Path\n\nMODEL_DIR = Path('../models')\nMODEL_PATH = MODEL_DIR / 'mario_ppo_agent.pth'\n\nif not MODEL_PATH.exists():\n    print(f\"\u2717 No trained model found at: {MODEL_PATH}\")\n    print(\"\\nYou need a trained RL agent to extract activations.\")\n    print(\"Please train an agent first by running:\")\n    print(\"  python ../train_mario_agent.py --steps 5000000\")\n    print(\"\\n\u26a0 Cannot proceed with encoding analysis without trained model\")\n    HAS_MODEL = False\nelse:\n    print(f\"\u2713 Found trained model: {MODEL_PATH}\")\n    HAS_MODEL = True\n    \n    # Load the model\n    from rl_utils import load_pretrained_model, align_activations_to_bold\n    \n    print(\"\\nLoading model...\")\n    model = load_pretrained_model(MODEL_PATH, device='cpu')\n    print(\"\u2713 Model loaded\")\n    \n    # Align activations to BOLD\n    # This will:\n    # 1. Load replay files for each game segment\n    # 2. Extract RL activations at 60Hz  \n    # 3. Downsample to TR (1.49s)\n    # 4. Apply HRF convolution\n    # 5. Create NaN mask for non-gameplay periods\n    \n    alignment_results = align_activations_to_bold(\n        model=model,\n        subject=SUBJECT,\n        session=SESSION,\n        runs=runs,\n        sourcedata_path=sourcedata_path,\n        tr=TR,\n        device='cpu',\n        apply_hrf=True  # Apply HRF convolution\n    )\n    \n    # Extract results\n    layer_activations = alignment_results['activations']\n    valid_mask = alignment_results['mask']\n    run_info = alignment_results['run_info']\n    \n    print(f\"\\n{'='*70}\")\n    print(\"Alignment summary:\")\n    for info in run_info:\n        print(f\"  {info['run']}: {info['n_valid_trs']}/{info['n_trs']} TRs \"\n              f\"({info['n_segments']} game segments)\")\n    print(f\"{'='*70}\\n\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "535d9cdd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": "## 3. Loading and Aligning RL Activations\n\n**NEW APPROACH:**\n\nInstead of using pre-extracted activations, we now:\n\n1. **Load replay files** from the human subject's actual gameplay\n   - Uses `.bk2` replay files from `sourcedata/mario/`\n   - Matches exact stimuli presented during fMRI scanning\n\n2. **Extract activations frame-by-frame** (60Hz)\n   - Pass replay frames through trained RL agent\n   - Collect CNN activations from all layers\n\n3. **Align to fMRI timing**\n   - Use `mario.annotations` files to get game segment timing\n   - Downsample from 60Hz to TR (1.49s)\n   - Apply HRF convolution\n\n4. **Handle multiple games per run**\n   - Concatenate gameplay segments\n   - Mask inter-game periods with NaN\n\n**This ensures perfect alignment between RL activations and BOLD data!**"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5490c7fd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning BOLD (detrending, standardizing)...\n",
      "\n",
      "\u2713 BOLD prepared:\n",
      "  Shape: (1799, 213443)\n",
      "  Timepoints: 1799\n",
      "  Voxels: 213,443\n"
     ]
    }
   ],
   "source": [
    "# Clean and prepare BOLD data\n",
    "\n",
    "from encoding_utils import load_and_prepare_bold\n",
    "\n",
    "print(\"Cleaning BOLD (detrending, standardizing)...\\n\")\n",
    "\n",
    "bold_data = load_and_prepare_bold(\n",
    "    bold_imgs,\n",
    "    mask_img=common_mask,\n",
    "    confounds_list=None,  # Already cleaned in GLM\n",
    "    detrend=True,\n",
    "    standardize=True,\n",
    "    high_pass=1/128,\n",
    "    t_r=TR\n",
    ")\n",
    "\n",
    "print(f\"\u2713 BOLD prepared:\")\n",
    "print(f\"  Shape: {bold_data.shape}\")\n",
    "print(f\"  Timepoints: {bold_data.shape[0]}\")\n",
    "print(f\"  Voxels: {bold_data.shape[1]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec498b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Cleaning and Preparing BOLD Data\n",
    "\n",
    "**Preprocessing steps:**\n",
    "1. **Detrending:** Remove linear drift within each run\n",
    "2. **High-pass filtering:** Remove slow fluctuations (<1/128 Hz)\n",
    "3. **Standardization:** Z-score each voxel (mean=0, std=1)\n",
    "\n",
    "**Why clean BOLD?**\n",
    "- Scanner drift confounds encoding\n",
    "- Slow fluctuations unrelated to task\n",
    "- Standardization ensures comparable scales\n",
    "\n",
    "**Output:** `(timepoints \u00d7 voxels)` matrix ready for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485af949",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": "# Check alignment between BOLD and activations\n\nif HAS_MODEL:\n    n_bold = bold_data.shape[0]\n    n_acts = list(layer_activations.values())[0].shape[0]\n    \n    print(f\"BOLD timepoints: {n_bold}\")\n    print(f\"Activations timepoints: {n_acts}\")\n    print(f\"Valid (gameplay) timepoints: {valid_mask.sum()}\")\n    print(f\"Invalid (non-gameplay) timepoints: {(~valid_mask).sum()}\")\n    \n    # Ensure dimensions match\n    if n_bold != n_acts:\n        print(f\"\\n\u26a0 Dimension mismatch!\")\n        print(f\"  Truncating to minimum length: {min(n_bold, n_acts)}\")\n        n_time = min(n_bold, n_acts)\n        bold_data = bold_data[:n_time]\n        valid_mask = valid_mask[:n_time]\n        for layer in layer_activations.keys():\n            layer_activations[layer] = layer_activations[layer][:n_time]\n    else:\n        print(\"\\n\u2713 Dimensions match!\")\nelse:\n    print(\"\u26a0 No model available, skipping alignment check\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "e68a2b66",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": "## 5. Alignment Status\n\n**Automatic alignment completed!**\n\nThe `align_activations_to_bold()` function has:\n\n1. \u2705 **Loaded replay files** for each game segment\n2. \u2705 **Extracted RL activations** at 60Hz from replay frames\n3. \u2705 **Downsampled to TR** using temporal averaging within each TR window\n4. \u2705 **Applied HRF convolution** to account for hemodynamic lag\n5. \u2705 **Created validity mask** to mark gameplay vs non-gameplay periods\n\n**Key differences from old approach:**\n- OLD: Arbitrary agent gameplay, misaligned\n- NEW: Exact subject gameplay from replays, perfectly aligned\n\n**Dimensions should now match:**\n- BOLD: Number of TRs across all runs\n- Activations: Same number of TRs (with NaN for non-gameplay)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f09db7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": "# Create train/test split (only on valid gameplay TRs)\n\nif HAS_MODEL:\n    # Get valid indices\n    valid_indices = np.where(valid_mask)[0]\n    n_valid = len(valid_indices)\n    \n    # Split valid indices 80/20\n    n_train_valid = int(n_valid * 0.8)\n    \n    train_valid_indices = valid_indices[:n_train_valid]\n    test_valid_indices = valid_indices[n_train_valid:]\n    \n    print(f\"Train/test split on valid (gameplay) TRs:\")\n    print(f\"  Train: {len(train_valid_indices)} TRs\")\n    print(f\"  Test: {len(test_valid_indices)} TRs\")\n    print(f\"\\nNote: Invalid (non-gameplay) TRs are excluded from both train and test\")\nelse:\n    print(\"\u26a0 No model available, skipping train/test split\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "78103a6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": "## 6. Applying PCA to Layer Activations\n\n**Why PCA?**\n- Raw activations have thousands of features per layer\n- PCA reduces dimensionality while preserving variance\n- Makes encoding models more tractable and prevents overfitting\n\n**NaN handling:**\n- PCA is fit only on valid (gameplay) TRs\n- Invalid TRs remain NaN in the reduced activations\n- This ensures proper alignment with BOLD data\n\n**Target:** 50 PCA components per layer (or fewer if needed)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07912745",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": "# Apply PCA to layer activations\n\nif HAS_MODEL:\n    from rl_utils import apply_pca_with_nan_handling\n    \n    print(\"Applying PCA to reduce dimensionality...\")\n    print(\"(PCA is fit only on valid gameplay TRs)\\n\")\n    \n    pca_results = apply_pca_with_nan_handling(\n        layer_activations,\n        valid_mask,\n        n_components=50,\n        variance_threshold=0.9\n    )\n    \n    # Extract reduced activations\n    reduced_activations = pca_results['reduced_activations']\n    pca_models = pca_results['pca_models']\n    variance_explained = pca_results['variance_explained']\n    \n    print(f\"\\n{'='*70}\")\n    print(\"PCA summary:\")\n    for layer, acts in reduced_activations.items():\n        print(f\"  {layer}: {acts.shape[1]} components\")\n    print(f\"{'='*70}\\n\")\nelse:\n    print(\"\u26a0 No model available, skipping PCA\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "394c681a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": "## 7. Fitting Ridge Regression Encoding Models\n\n**For each layer:**\n1. Use PCA-reduced activations (50 components)\n2. Cross-validate to find optimal \u03b1 (regularization strength)\n3. Fit ridge regression on training data (gameplay TRs only)\n4. Predict BOLD on test data\n5. Compute R\u00b2 per voxel\n\n**Hyperparameter search:** \u03b1 \u2208 [0.1, 1, 10, 100, 1000, 10000, 100000]\n\n**NaN handling:**\n- Only valid (gameplay) TRs are used for training and testing\n- Invalid TRs are automatically excluded\n\n**Output per layer:**\n- Best \u03b1\n- R\u00b2 map: `(voxels,)` array\n- Trained model\n\n**Runtime:** ~5-10 minutes for all 5 layers \u00d7 213k voxels\n\n**Interpretation:**\n- R\u00b2 > 0: Features explain variance in BOLD\n- R\u00b2 \u2248 0: No prediction (chance level)\n- Negative R\u00b2: Worse than mean baseline"
  },
  {
   "cell_type": "code",
   "id": "a0kcwmp9zb",
   "source": "# Fit ridge regression encoding models\n\nif HAS_MODEL:\n    from encoding_utils import fit_encoding_model_per_layer\n    \n    alphas = [0.1, 1, 10, 100, 1000, 10000, 100000]\n    \n    print(\"Fitting ridge regression (5 layers \u00d7 voxels)...\")\n    print(\"This takes ~5-10 minutes\\n\")\n    \n    encoding_results = fit_encoding_model_per_layer(\n        reduced_activations,\n        bold_data,\n        common_mask,\n        train_valid_indices,\n        test_valid_indices,\n        alphas=alphas,\n        valid_mask=valid_mask  # Pass valid mask for NaN handling\n    )\n    \n    print(\"\\n\u2713 Encoding complete!\")\nelse:\n    print(\"\u26a0 No model available, skipping encoding\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dbb1ff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": "# Compare layer performance\n\nif HAS_MODEL:\n    from encoding_utils import compare_layer_performance\n    from encoding_viz_utils import plot_layer_comparison_bars\n    \n    comparison_df = compare_layer_performance(encoding_results)\n    \n    print(\"Layer Performance:\\n\")\n    print(\"=\" * 80)\n    print(comparison_df.to_string(index=False))\n    print(\"=\" * 80)\n    \n    best_layer = comparison_df.iloc[0]['layer']\n    best_r2 = comparison_df.iloc[0]['mean_r2']\n    \n    print(f\"\\n\u2b50 Best: {best_layer.upper()} (R\u00b2 = {best_r2:.4f})\")\n    \n    # Visualize\n    layer_order = ['conv1', 'conv2', 'conv3', 'conv4', 'linear']\n    fig = plot_layer_comparison_bars(encoding_results, layer_order)\n    plt.show()\nelse:\n    print(\"\u26a0 No model available, skipping layer comparison\")"
  },
  {
   "cell_type": "markdown",
   "id": "89e8bac1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 8. Comparing Layer Performance\n",
    "\n",
    "**Question:** Which CNN layer best predicts brain activity?\n",
    "\n",
    "**Metrics:**\n",
    "- Mean R\u00b2 (test set)\n",
    "- Median R\u00b2 (robust to outliers)\n",
    "- % voxels with R\u00b2 > 0.01 (significantly predicted)\n",
    "\n",
    "**Expected pattern (if hypothesis holds):**\n",
    "- Early layers (conv1/2) \u2192 Visual cortex\n",
    "- Middle layers (conv3/4) \u2192 Motor/parietal\n",
    "- Late layers (linear) \u2192 Frontal/executive\n",
    "\n",
    "**See bar plot below for comparison.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b0c840",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": "# Visualize R\u00b2 brain maps (best layer)\n\nif HAS_MODEL:\n    from encoding_viz_utils import plot_r2_brainmap\n    \n    best_layer = comparison_df.iloc[0]['layer']\n    best_r2_map = encoding_results[best_layer]['r2_map']\n    \n    print(f\"Best layer: {best_layer.upper()}\\n\")\n    \n    fig = plot_r2_brainmap(\n        best_r2_map, \n        best_layer,\n        threshold=0.01,\n        vmax=0.2\n    )\n    plt.show()\n    \n    print(\"\\n\ud83d\udccd Interpretation:\")\n    print(\"  Hot regions = Well predicted by this layer\")\n    print(\"  - Early layers \u2192 Visual cortex\")\n    print(\"  - Middle layers \u2192 Motor/parietal\")\n    print(\"  - Late layers \u2192 Frontal/executive\")\nelse:\n    print(\"\u26a0 No model available, skipping brain map visualization\")"
  },
  {
   "cell_type": "markdown",
   "id": "763ffbbf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": "## Summary: Brain Encoding with Proper Alignment\n\n**What we accomplished:**\n\n1. \u2705 **Loaded RL model:** Trained PPO agent\n2. \u2705 **Extracted activations from replays:** Used actual gameplay .bk2 files\n3. \u2705 **Proper temporal alignment:**\n   - Matched replay frames to fMRI TRs using annotations\n   - Downsampled from 60Hz to TR (1.49s)\n   - Applied HRF convolution\n   - Masked non-gameplay periods with NaN\n4. \u2705 **Applied PCA:** Reduced to 50 components per layer (on valid TRs only)\n5. \u2705 **Fit encoding models:** Ridge regression with NaN-aware training\n6. \u2705 **Compared layers:** Identified which layer best predicts brain activity\n7. \u2705 **Visualized brain maps:** Localized where each layer is encoded\n\n---\n\n### Key Improvements Over Original Approach\n\n**What was fixed:**\n\n1. **Proper replay alignment:**\n   - OLD: Agent played arbitrary Level1-1, misaligned with BOLD\n   - NEW: Extract activations from exact subject gameplay replays\n\n2. **Temporal alignment:**\n   - OLD: No alignment, simple truncation\n   - NEW: Use annotation files to map frames \u2192 TRs with onset/duration\n\n3. **HRF convolution:**\n   - OLD: Missing HRF convolution\n   - NEW: Applied SPM HRF after downsampling\n\n4. **Multiple games per run:**\n   - OLD: Couldn't handle multiple game segments\n   - NEW: Concatenate segments, mask inter-game periods with NaN\n\n5. **NaN handling:**\n   - OLD: No way to exclude non-gameplay periods\n   - NEW: Valid mask ensures encoding models only train on gameplay TRs\n\n---\n\n### Expected Results\n\n**If brain uses RL-like representations:**\n\n- **Early layers (conv1/2):** Predict visual cortex\n  - Edge detection, textures, low-level visual features\n  - Occipital lobe activation\n\n- **Middle layers (conv3/4):** Predict parietal/motor cortex\n  - Spatial layout, object positions, movement planning\n  - Parietal lobe, premotor cortex\n\n- **Late layers (linear):** Predict frontal cortex\n  - Value estimates, policy selection, abstract strategy\n  - Prefrontal cortex, anterior cingulate\n\n**Hierarchical gradient:**\n- R\u00b2 should increase from early \u2192 late layers if brain uses RL features\n- Brain maps should show posterior \u2192 anterior gradient\n\n---\n\n### Methodological Lessons\n\n**Critical requirements for encoding analysis:**\n\n1. **Proper stimulus alignment:**\n   - Use exact stimuli presented to subject\n   - Match timing precisely (frame-by-frame if needed)\n   - Account for hemodynamic lag (HRF)\n\n2. **Data quality:**\n   - Sufficient valid trials (gameplay periods)\n   - Good signal-to-noise ratio\n   - Proper preprocessing (motion correction, etc.)\n\n3. **Statistical power:**\n   - Multiple sessions/subjects for group analysis\n   - Cross-validation to avoid overfitting\n   - Appropriate regularization (ridge \u03b1)\n\n4. **Interpretation:**\n   - Compare to baseline models (GLM with behavioral features)\n   - Test specific hypotheses about layer-to-region mapping\n   - Consider alternative explanations (motion, attention, etc.)\n\n---\n\n### Comparison to GLM (Notebook 01)\n\n**GLM:**\n- \u2705 Hypothesis-driven (LEFT_THUMB vs RIGHT_THUMB)\n- \u2705 Simple, interpretable\n- \u2705 Works with sparse events\n- \u2705 Found significant effects (contralateral motor control)\n\n**Encoding (This notebook):**\n- \u2705 Data-driven (learned RL features)\n- \u2705 Exploratory (discover representations)\n- \u2705 Tests computational theories\n- \u23f3 Requires more data and careful alignment\n\n**Both approaches are complementary:**\n- GLM: Validate known effects\n- Encoding: Discover new representations\n\n---\n\n### Next Steps & Extensions\n\n**To improve these results:**\n\n1. **More data:**\n   - Aggregate across multiple sessions\n   - Multi-subject analysis\n   - Increase gameplay duration\n\n2. **Better features:**\n   - Try different layers simultaneously\n   - Non-linear encoding (kernel ridge, neural networks)\n   - Task-specific features (value, prediction error, etc.)\n\n3. **Control analyses:**\n   - Compare to pixel-based features\n   - Test against behavioral-only models\n   - Permutation testing for significance\n\n4. **Advanced methods:**\n   - Hyperalignment across subjects\n   - Representational similarity analysis (RSA)\n   - Decoding (BOLD \u2192 predicted actions)\n\n---\n\n### Research Questions\n\n**This pipeline enables investigating:**\n\n1. **Computational neuroscience:**\n   - Do brain and RL agent use similar representations?\n   - Where are value and policy encoded in the brain?\n   - How do representations change with learning?\n\n2. **Cognitive neuroscience:**\n   - How does the brain represent game state?\n   - What role does prediction play in decision-making?\n   - How are visual and motor systems integrated?\n\n3. **AI alignment:**\n   - Can we build agents that think like humans?\n   - What makes representations interpretable?\n   - How to design human-aligned reward functions?\n\n**This tutorial provides a complete, working pipeline for these investigations!**"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}