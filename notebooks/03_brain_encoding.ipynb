{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579b22aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Brain Encoding with RL Features\n",
    "\n",
    "## Predicting Brain Activity from Agent Representations\n",
    "\n",
    "**Overview:**\n",
    "This notebook uses the CNN activations from the RL agent (notebook 02) to predict brain activity during gameplay.\n",
    "\n",
    "**What we'll cover:**\n",
    "1. Understanding the encoding model framework\n",
    "2. Loading and preparing BOLD data\n",
    "3. Loading CNN activations from the agent\n",
    "4. Aligning timepoints between BOLD and activations\n",
    "5. Fitting ridge regression encoding models\n",
    "6. Comparing layer performance\n",
    "7. Visualizing brain maps\n",
    "\n",
    "**Key question:** Which layer of the agent best predicts brain activity, and where?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5aaccc2",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup - imports and configuration\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "src_dir = Path('..') / 'src'\n",
    "sys.path.insert(0, str(src_dir))\n",
    "\n",
    "# Import utilities\n",
    "from utils import (\n",
    "    get_sourcedata_path,\n",
    "    load_events,\n",
    "    get_session_runs,\n",
    "    load_bold\n",
    ")\n",
    "\n",
    "# Import RL utilities\n",
    "from rl_utils import (\n",
    "    create_simple_proxy_features,\n",
    "    convolve_with_hrf,\n",
    "    apply_pca\n",
    ")\n",
    "\n",
    "# Import RL visualizations\n",
    "from rl_viz_utils import (\n",
    "    plot_pca_variance_per_layer,\n",
    "    plot_layer_activations_sample\n",
    ")\n",
    "\n",
    "# Import encoding utilities\n",
    "from encoding_utils import (\n",
    "    load_and_prepare_bold,\n",
    "    fit_encoding_model_per_layer,\n",
    "    compare_layer_performance\n",
    ")\n",
    "\n",
    "# Import encoding visualizations\n",
    "from encoding_viz_utils import (\n",
    "    plot_layer_comparison_bars,\n",
    "    plot_r2_brainmap\n",
    ")\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Get sourcedata path\n",
    "sourcedata_path = get_sourcedata_path()\n",
    "\n",
    "print(\"‚úì Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0ce3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. The Encoding Model Framework\n",
    "\n",
    "**Goal:** Predict BOLD activity from RL agent features\n",
    "\n",
    "**Model:** Ridge Regression (linear regression with L2 regularization)\n",
    "\n",
    "```\n",
    "BOLD(voxel, time) = Œ£ Œ≤·µ¢ ¬∑ Feature_i(time) + Œµ\n",
    "```\n",
    "\n",
    "**Why ridge regression?**\n",
    "- Handles high-dimensional features (50 PCA components)\n",
    "- L2 penalty prevents overfitting: `||Œ≤||¬≤ ‚â§ Œ±`\n",
    "- Cross-validation selects optimal regularization strength Œ±\n",
    "- Fast to fit (~5 mins for whole brain)\n",
    "\n",
    "**Alternative approaches:**\n",
    "- Lasso (L1): Sparse feature selection\n",
    "- Elastic net: L1 + L2\n",
    "- Nonlinear: Kernel ridge, neural networks\n",
    "\n",
    "**For interpretability and speed, we use ridge regression.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e9bf216",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 runs: ['run-1', 'run-2', 'run-3', 'run-4']\n",
      "  run-1: 953 events\n",
      "  run-2: 986 events\n",
      "  run-3: 892 events\n",
      "  run-4: 1033 events\n",
      "\n",
      "Creating common brain mask...\n",
      "‚úì Common mask: 213,443 voxels\n",
      "\n",
      "‚úì All prerequisites loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load prerequisites\n",
    "\n",
    "from nilearn.masking import compute_multi_epi_mask\n",
    "\n",
    "# Define constants (assumed from the first tutorial)\n",
    "SUBJECT = 'sub-01'\n",
    "SESSION = 'ses-010'\n",
    "TR = 1.49\n",
    "\n",
    "\n",
    "# Get runs\n",
    "runs = get_session_runs(SUBJECT, SESSION, sourcedata_path)\n",
    "print(f\"Found {len(runs)} runs: {runs}\")\n",
    "\n",
    "# Load events\n",
    "all_events = []\n",
    "for run in runs:\n",
    "    events = load_events(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    all_events.append(events)\n",
    "    print(f\"  {run}: {len(events)} events\")\n",
    "\n",
    "# Create common mask (or load from main tutorial)\n",
    "print(\"\\nCreating common brain mask...\")\n",
    "bold_imgs= []\n",
    "for run in runs:\n",
    "    bold_img = load_bold(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    bold_imgs.append(bold_img)\n",
    "\n",
    "common_mask = compute_multi_epi_mask(bold_imgs, n_jobs=1)\n",
    "n_voxels = int((common_mask.get_fdata() > 0).sum())\n",
    "print(f\"‚úì Common mask: {n_voxels:,} voxels\")\n",
    "\n",
    "print(\"\\n‚úì All prerequisites loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e1530c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Loading Prerequisites\n",
    "\n",
    "We need:\n",
    "- Subject/session info (sub-01, ses-010)\n",
    "- Run IDs (4 runs)\n",
    "- BOLD images (preprocessed fMRI data)\n",
    "- Event files (for alignment)\n",
    "- Common brain mask (from GLM analysis)\n",
    "\n",
    "**Note:** If you haven't run notebook 01, this will create a fresh mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "434f2d50",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded pre-computed activations\n",
      "  Layers: ['conv1', 'conv2', 'conv3', 'conv4', 'linear']\n",
      "  conv1: (1000, 50)\n",
      "  conv2: (1000, 50)\n",
      "  conv3: (1000, 50)\n",
      "  conv4: (1000, 50)\n",
      "  linear: (1000, 50)\n"
     ]
    }
   ],
   "source": [
    "# Load activations from 02_reinforcement_learning.ipynb\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "activations_file = Path('../derivatives/activations/reduced_activations.pkl')\n",
    "\n",
    "if activations_file.exists():\n",
    "    with open(activations_file, 'rb') as f:\n",
    "        reduced_activations = pickle.load(f)\n",
    "    \n",
    "    print(\"‚úì Loaded pre-computed activations\")\n",
    "    print(f\"  Layers: {list(reduced_activations.keys())}\")\n",
    "    for layer, acts in reduced_activations.items():\n",
    "        print(f\"  {layer}: {acts.shape}\")\n",
    "else:\n",
    "    print(f\"‚úó Activations file not found: {activations_file}\")\n",
    "    print(\"\\nPlease run 02_reinforcement_learning.ipynb first to extract CNN activations.\")\n",
    "    print(\"Alternatively, you can use proxy features for testing:\")\n",
    "    print(\"  (but results won't be meaningful)\")\n",
    "    \n",
    "    # Create minimal proxy for testing\n",
    "    import numpy as np\n",
    "    np.random.seed(42)\n",
    "    total_trs = sum(len(bold_img.get_fdata()[0,0,0,:]) for bold_img in bold_imgs)\n",
    "    \n",
    "    reduced_activations = {}\n",
    "    for layer in ['conv1', 'conv2', 'conv3', 'conv4', 'linear']:\n",
    "        reduced_activations[layer] = np.random.randn(total_trs, 50)\n",
    "    \n",
    "    print(\"\\n‚ö† Using random proxy features (not meaningful!)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535d9cdd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Loading CNN Activations\n",
    "\n",
    "**Source:** Saved from notebook 02 (`reduced_activations.pkl`)\n",
    "\n",
    "**Format:** Dictionary with 5 layers\n",
    "- Each layer: `(timesteps, 50)` array of PCA components\n",
    "\n",
    "**If file is missing:**\n",
    "- You must run notebook 02 first to extract activations\n",
    "- Alternatively, random proxy features can be used for testing (not meaningful)\n",
    "\n",
    "**Expected output:** 5 layers √ó (1000 timesteps, 50 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5490c7fd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning BOLD (detrending, standardizing)...\n",
      "\n",
      "‚úì BOLD prepared:\n",
      "  Shape: (1799, 213443)\n",
      "  Timepoints: 1799\n",
      "  Voxels: 213,443\n"
     ]
    }
   ],
   "source": [
    "# Clean and prepare BOLD data\n",
    "\n",
    "from encoding_utils import load_and_prepare_bold\n",
    "\n",
    "print(\"Cleaning BOLD (detrending, standardizing)...\\n\")\n",
    "\n",
    "bold_data = load_and_prepare_bold(\n",
    "    bold_imgs,\n",
    "    mask_img=common_mask,\n",
    "    confounds_list=None,  # Already cleaned in GLM\n",
    "    detrend=True,\n",
    "    standardize=True,\n",
    "    high_pass=1/128,\n",
    "    t_r=TR\n",
    ")\n",
    "\n",
    "print(f\"‚úì BOLD prepared:\")\n",
    "print(f\"  Shape: {bold_data.shape}\")\n",
    "print(f\"  Timepoints: {bold_data.shape[0]}\")\n",
    "print(f\"  Voxels: {bold_data.shape[1]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec498b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Cleaning and Preparing BOLD Data\n",
    "\n",
    "**Preprocessing steps:**\n",
    "1. **Detrending:** Remove linear drift within each run\n",
    "2. **High-pass filtering:** Remove slow fluctuations (<1/128 Hz)\n",
    "3. **Standardization:** Z-score each voxel (mean=0, std=1)\n",
    "\n",
    "**Why clean BOLD?**\n",
    "- Scanner drift confounds encoding\n",
    "- Slow fluctuations unrelated to task\n",
    "- Standardization ensures comparable scales\n",
    "\n",
    "**Output:** `(timepoints √ó voxels)` matrix ready for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "485af949",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOLD timepoints: 1799\n",
      "Activations timepoints: 1000\n",
      "\n",
      "‚úì Aligned to 1000 timepoints\n"
     ]
    }
   ],
   "source": [
    "# Align timepoints between BOLD and activations\n",
    "\n",
    "n_bold = bold_data.shape[0]\n",
    "n_acts = list(reduced_activations.values())[0].shape[0]\n",
    "\n",
    "print(f\"BOLD timepoints: {n_bold}\")\n",
    "print(f\"Activations timepoints: {n_acts}\")\n",
    "\n",
    "# Take minimum (align)\n",
    "n_time = min(n_bold, n_acts)\n",
    "\n",
    "bold_data = bold_data[:n_time]\n",
    "for layer in reduced_activations.keys():\n",
    "    reduced_activations[layer] = reduced_activations[layer][:n_time]\n",
    "\n",
    "print(f\"\\n‚úì Aligned to {n_time} timepoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a2b66",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Aligning BOLD and Activation Timepoints\n",
    "\n",
    "**Problem:** BOLD and activations may have different lengths\n",
    "- BOLD: 1799 TRs (all 4 runs concatenated)\n",
    "- Activations: 1000 steps (agent gameplay)\n",
    "\n",
    "**Solution:** Trim both to minimum length\n",
    "\n",
    "**Important:** In a full analysis, you'd align by:\n",
    "1. Matching gameplay frames to fMRI TRs\n",
    "2. Using event timing (onset/duration)\n",
    "3. Applying HRF convolution\n",
    "\n",
    "For this tutorial, we use simple truncation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02f09db7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test split:\n",
      "  Train: 800 timepoints\n",
      "  Test: 200 timepoints\n"
     ]
    }
   ],
   "source": [
    "# Create train/test split (80/20)\n",
    "\n",
    "n_train = int(n_time * 0.8)\n",
    "train_idx = np.arange(n_train)\n",
    "test_idx = np.arange(n_train, n_time)\n",
    "\n",
    "print(f\"Train/test split:\")\n",
    "print(f\"  Train: {len(train_idx)} timepoints\")\n",
    "print(f\"  Test: {len(test_idx)} timepoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78103a6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6. Creating Train/Test Split\n",
    "\n",
    "**Strategy:** 80/20 train/test split\n",
    "\n",
    "**Why split?**\n",
    "- Training set: Fit regression weights Œ≤\n",
    "- Test set: Evaluate generalization (R¬≤)\n",
    "- Prevents overfitting to noise\n",
    "\n",
    "**Cross-validation:** We also use CV to select regularization Œ±\n",
    "\n",
    "**Output:**\n",
    "- Train: 800 timepoints\n",
    "- Test: 200 timepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07912745",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ridge regression (5 layers √ó voxels)...\n",
      "This takes ~3-5 minutes\n",
      "\n",
      "Fitting encoding model for layer: conv1\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fit ridge regression encoding models\n",
    "\n",
    "from encoding_utils import fit_encoding_model_per_layer\n",
    "\n",
    "alphas = [0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "\n",
    "print(\"Fitting ridge regression (5 layers √ó voxels)...\")\n",
    "print(\"This takes ~3-5 minutes\\n\")\n",
    "\n",
    "encoding_results = fit_encoding_model_per_layer(\n",
    "    reduced_activations, \n",
    "    bold_data, \n",
    "    common_mask,\n",
    "    train_idx, \n",
    "    test_idx, \n",
    "    alphas=alphas\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Encoding complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394c681a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 7. Fitting Ridge Regression Encoding Models\n",
    "\n",
    "**For each layer:**\n",
    "1. Cross-validate to find optimal Œ± (regularization strength)\n",
    "2. Fit ridge regression on training data\n",
    "3. Predict BOLD on test data\n",
    "4. Compute R¬≤ per voxel\n",
    "\n",
    "**Hyperparameter search:** Œ± ‚àà [0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "\n",
    "**Output per layer:**\n",
    "- Best Œ±\n",
    "- R¬≤ map: `(voxels,)` array\n",
    "- Trained model\n",
    "\n",
    "**Runtime:** ~5 minutes for all 5 layers √ó 213k voxels\n",
    "\n",
    "**Interpretation:**\n",
    "- R¬≤ > 0: Features explain variance in BOLD\n",
    "- R¬≤ ‚âà 0: No prediction (chance level)\n",
    "- Negative R¬≤: Worse than mean baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dbb1ff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Compare layer performance\n",
    "\n",
    "from encoding_utils import compare_layer_performance\n",
    "from encoding_viz_utils import plot_layer_comparison_bars\n",
    "\n",
    "comparison_df = compare_layer_performance(encoding_results)\n",
    "\n",
    "print(\"Layer Performance:\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_layer = comparison_df.iloc[0]['layer']\n",
    "best_r2 = comparison_df.iloc[0]['mean_r2']\n",
    "\n",
    "print(f\"\\n‚≠ê Best: {best_layer.upper()} (R¬≤ = {best_r2:.4f})\")\n",
    "\n",
    "# Visualize\n",
    "layer_order = ['conv1', 'conv2', 'conv3', 'conv4', 'linear']\n",
    "fig = plot_layer_comparison_bars(encoding_results, layer_order)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e8bac1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 8. Comparing Layer Performance\n",
    "\n",
    "**Question:** Which CNN layer best predicts brain activity?\n",
    "\n",
    "**Metrics:**\n",
    "- Mean R¬≤ (test set)\n",
    "- Median R¬≤ (robust to outliers)\n",
    "- % voxels with R¬≤ > 0.01 (significantly predicted)\n",
    "\n",
    "**Expected pattern (if hypothesis holds):**\n",
    "- Early layers (conv1/2) ‚Üí Visual cortex\n",
    "- Middle layers (conv3/4) ‚Üí Motor/parietal\n",
    "- Late layers (linear) ‚Üí Frontal/executive\n",
    "\n",
    "**See bar plot below for comparison.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b0c840",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize R¬≤ brain maps (best layer)\n",
    "\n",
    "from encoding_viz_utils import plot_r2_brainmap\n",
    "\n",
    "best_layer = comparison_df.iloc[0]['layer']\n",
    "best_r2_map = encoding_results[best_layer]['r2_map']\n",
    "\n",
    "print(f\"Best layer: {best_layer.upper()}\\n\")\n",
    "\n",
    "fig = plot_r2_brainmap(\n",
    "    best_r2_map, \n",
    "    best_layer,\n",
    "    threshold=0.01,\n",
    "    vmax=0.2\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìç Interpretation:\")\n",
    "print(\"  Hot regions = Well predicted by this layer\")\n",
    "print(\"  - Early layers ‚Üí Visual cortex\")\n",
    "print(\"  - Middle layers ‚Üí Motor/parietal\")\n",
    "print(\"  - Late layers ‚Üí Frontal/executive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763ffbbf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary: Brain Encoding Results\n",
    "\n",
    "**What we accomplished:**\n",
    "\n",
    "1. ‚úÖ **Loaded agent activations:** 5 CNN layers with 50 PCA components each\n",
    "2. ‚úÖ **Prepared BOLD data:** Cleaned, standardized, aligned\n",
    "3. ‚úÖ **Fit encoding models:** Ridge regression for each layer\n",
    "4. ‚úÖ **Compared layers:** Identified which layer best predicts brain activity\n",
    "5. ‚úÖ **Visualized brain maps:** Localized where each layer is encoded\n",
    "\n",
    "---\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "**Layer performance:**\n",
    "- All layers show low R¬≤ (‚âà-0.003 to 0.002)\n",
    "- This suggests either:\n",
    "  1. Misalignment between activations and BOLD (no HRF convolution)\n",
    "  2. Different runs (agent played Level1-1, BOLD from mixed levels)\n",
    "  3. Need more training data\n",
    "  4. RL features don't capture brain representations (null result)\n",
    "\n",
    "**What went wrong?**\n",
    "- Agent activations extracted from gameplay on Level1-1\n",
    "- BOLD data from all 4 runs (mixed levels, different gameplay)\n",
    "- No temporal alignment via events\n",
    "- No HRF convolution of features\n",
    "\n",
    "---\n",
    "\n",
    "### Methodological Lessons\n",
    "\n",
    "**For real encoding analysis, you need:**\n",
    "\n",
    "1. **Proper alignment:**\n",
    "   - Match agent gameplay frames to exact fMRI TRs\n",
    "   - Use event timing (onset/duration) from .tsv files\n",
    "   - Ensure same levels/runs for agent and BOLD\n",
    "\n",
    "2. **HRF convolution:**\n",
    "   - Convolve features with canonical HRF\n",
    "   - BOLD lags neural activity by ~6 seconds\n",
    "   - Without HRF: features won't align with BOLD peaks\n",
    "\n",
    "3. **More data:**\n",
    "   - Multiple runs of the same level\n",
    "   - Multiple subjects (group-level analysis)\n",
    "   - More timepoints for stable estimates\n",
    "\n",
    "4. **Better features:**\n",
    "   - Try different layers\n",
    "   - Try concatenating layers\n",
    "   - Try nonlinear encoding (kernel ridge, DNN)\n",
    "\n",
    "---\n",
    "\n",
    "### What This Tutorial Demonstrates\n",
    "\n",
    "**Despite low R¬≤, this tutorial shows:**\n",
    "\n",
    "‚úÖ **Complete pipeline:** RL agent ‚Üí Feature extraction ‚Üí Brain encoding\n",
    "‚úÖ **Scalable methods:** Works for any agent, any task\n",
    "‚úÖ **Hypothesis testing:** Can test if brain uses RL-like representations\n",
    "‚úÖ **Negative results matter:** Shows importance of proper alignment\n",
    "\n",
    "**For better results:** Run agent on same gameplay as fMRI, align properly, apply HRF.\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison to GLM\n",
    "\n",
    "**GLM (Notebook 01):**\n",
    "- Hypothesis-driven (LEFT_THUMB vs RIGHT_THUMB)\n",
    "- Interpretable (contralateral motor control)\n",
    "- Works with sparse events\n",
    "- ‚úÖ **Found significant effects** (FWE-corrected)\n",
    "\n",
    "**Encoding (Notebook 03):**\n",
    "- Data-driven (learned RL features)\n",
    "- Exploratory (discover what brain encodes)\n",
    "- Requires dense features + alignment\n",
    "- ‚ùå **No significant effects** (alignment issues)\n",
    "\n",
    "**Complementary approaches:**\n",
    "- GLM: Test specific hypotheses\n",
    "- Encoding: Discover representations\n",
    "\n",
    "**Both are valuable!**\n",
    "\n",
    "---\n",
    "\n",
    "### Future Directions\n",
    "\n",
    "**To improve these results:**\n",
    "\n",
    "1. **Replay alignment:** Extract activations from exact replays (.bk2 files)\n",
    "2. **Event-based sampling:** Use button press onsets to align\n",
    "3. **HRF modeling:** Convolve features with canonical HRF\n",
    "4. **Multi-session:** Aggregate across multiple sessions\n",
    "5. **Hyperalignment:** Align subjects' functional spaces\n",
    "6. **Deep encoding:** Use nonlinear models (DNNs)\n",
    "\n",
    "**Research questions:**\n",
    "- Do hierarchical RL features match cortical hierarchy?\n",
    "- Which brain regions encode value vs policy?\n",
    "- Do representations change with learning?\n",
    "- Can we decode intended actions from brain activity?\n",
    "\n",
    "**This tutorial provides the foundation for these investigations!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
