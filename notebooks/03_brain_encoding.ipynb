{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579b22aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Brain Encoding with RL Features\n",
    "\n",
    "## Predicting Brain Activity from Agent Representations\n",
    "\n",
    "**Overview:**\n",
    "This notebook uses the CNN activations from the RL agent (notebook 02) to predict brain activity during gameplay.\n",
    "\n",
    "**What we'll cover:**\n",
    "1. Understanding the encoding model framework\n",
    "2. Loading and preparing BOLD data\n",
    "3. Loading CNN activations from the agent\n",
    "4. Aligning timepoints between BOLD and activations\n",
    "5. Fitting ridge regression encoding models\n",
    "6. Comparing layer performance\n",
    "7. Visualizing brain maps\n",
    "\n",
    "**Key question:** Which layer of the agent best predicts brain activity, and where?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "setup"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª Detected Local Environment.\n",
      "‚úÖ Ready. Working directory: /home/hyruuk/GitHub/neuromod/mario_analysis/mario.tutorials\n"
     ]
    }
   ],
   "source": [
    "# @title Environment Setup\n",
    "# @markdown Run this cell to set up the environment and download the necessary data.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "REPO_URL = \"https://github.com/courtois-neuromod/mario.tutorials.git\"\n",
    "PROJECT_PATH = Path(\"/content/mario.tutorials\")\n",
    "REQUIREMENTS_FILE = \"notebooks/03_requirements.txt\"\n",
    "SUBJECT = \"sub-01\"\n",
    "SESSION = \"ses-001\"\n",
    "TR = 1.49\n",
    "DOWNLOAD_STIMULI = True\n",
    "\n",
    "def run_shell(cmd):\n",
    "    print(f\"Running: {cmd}\")\n",
    "    subprocess.check_call(cmd, shell=True)\n",
    "\n",
    "# Detect Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üöÄ Detected Google Colab. Setting up ephemeral environment...\")\n",
    "    \n",
    "    # 1. Clone Repository\n",
    "    if not PROJECT_PATH.exists():\n",
    "        run_shell(f\"git clone {REPO_URL} {PROJECT_PATH}\")\n",
    "    else:\n",
    "        run_shell(f\"cd {PROJECT_PATH} && git pull\")\n",
    "    \n",
    "    os.chdir(PROJECT_PATH)\n",
    "    sys.path.insert(0, str(PROJECT_PATH / \"src\"))\n",
    "    \n",
    "    # 2. Run Setup\n",
    "    from setup_utils import setup_project\n",
    "    setup_project(REQUIREMENTS_FILE, SUBJECT, SESSION, download_stimuli_flag=DOWNLOAD_STIMULI)\n",
    "\n",
    "else:\n",
    "    print(\"üíª Detected Local Environment.\")\n",
    "    if Path.cwd().name == 'notebooks':\n",
    "        os.chdir(Path.cwd().parent)\n",
    "    sys.path.insert(0, str(Path.cwd() / \"src\"))\n",
    "    print(f\"‚úÖ Ready. Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "setup"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup utils not found. Please ensure src is in path.\n"
     ]
    }
   ],
   "source": [
    "# Silent Setup\n",
    "try:\n",
    "    from setup_utils import setup_all\n",
    "    # Ensure data is available (silently checks)\n",
    "    setup_all(subject=\"sub-01\", session=\"ses-010\")\n",
    "except ImportError:\n",
    "    print(\"Setup utils not found. Please ensure src is in path.\")\n",
    "except Exception as e:\n",
    "    print(f\"Setup warning: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aaccc2",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": "# Setup - imports and configuration\n\nimport sys\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Add src to path\nsrc_dir = Path('..') / 'src'\nsys.path.insert(0, str(src_dir))\n\n# Import utilities\nfrom utils import (\n    get_sourcedata_path,\n    load_events,\n    get_session_runs,\n    get_bold_path,\n    load_bold\n)\n\n# Import RL utilities\nfrom rl_utils import (\n    create_simple_proxy_features,\n    convolve_with_hrf,\n    apply_pca\n)\n\n# Import RL visualizations\nfrom rl_viz_utils import (\n    plot_pca_variance_per_layer,\n    plot_layer_activations_sample\n)\n\n# Import encoding utilities\nfrom encoding_utils import (\n    load_and_prepare_bold,\n    fit_encoding_model_per_layer,\n    compare_layer_performance\n)\n\n# Import encoding visualizations\nfrom encoding_viz_utils import (\n    plot_layer_comparison_bars,\n    plot_r2_brainmap\n)\n\n# Plotting style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (14, 6)\nplt.rcParams['font.size'] = 11\n\n# Get sourcedata path\nsourcedata_path = get_sourcedata_path()\n\nprint(\"‚úì Setup complete!\")"
  },
  {
   "cell_type": "markdown",
   "id": "c8d0ce3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. The Encoding Model Framework\n",
    "\n",
    "**Goal:** Predict BOLD activity from RL agent features\n",
    "\n",
    "**Model:** Ridge Regression (linear regression with L2 regularization)\n",
    "\n",
    "```\n",
    "BOLD(voxel, time) = Œ£ Œ≤·µ¢ ¬∑ Feature_i(time) + Œµ\n",
    "```\n",
    "\n",
    "**Why ridge regression?**\n",
    "- Handles high-dimensional features (50 PCA components)\n",
    "- L2 penalty prevents overfitting: `||Œ≤||¬≤ ‚â§ Œ±`\n",
    "- Cross-validation selects optimal regularization strength Œ±\n",
    "- Fast to fit (~5 mins for whole brain)\n",
    "\n",
    "**Alternative approaches:**\n",
    "- Lasso (L1): Sparse feature selection\n",
    "- Elastic net: L1 + L2\n",
    "- Nonlinear: Kernel ridge, neural networks\n",
    "\n",
    "**For interpretability and speed, we use ridge regression.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9bf216",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": "# Load prerequisites\n\nfrom nilearn.masking import compute_multi_epi_mask\n\n# Get runs\nruns = get_session_runs(SUBJECT, SESSION, sourcedata_path)\nprint(f\"Found {len(runs)} runs: {runs}\")\n\n# Load events\nall_events = []\nfor run in runs:\n    events = load_events(SUBJECT, SESSION, run, sourcedata_path)\n    all_events.append(events)\n    print(f\"  {run}: {len(events)} events\")\n\n# Load BOLD images and paths\nprint(\"\\nLoading BOLD data...\")\nbold_imgs = []\nbold_paths = []\nfor run in runs:\n    bold_path = get_bold_path(SUBJECT, SESSION, run, sourcedata_path)\n    bold_img = load_bold(SUBJECT, SESSION, run, sourcedata_path)\n    bold_paths.append(str(bold_path))  # Convert Path to string for nilearn\n    bold_imgs.append(bold_img)\n\n# Create common mask\nprint(\"\\nCreating common brain mask...\")\ncommon_mask = compute_multi_epi_mask(bold_imgs, n_jobs=1)\nn_voxels = int((common_mask.get_fdata() > 0).sum())\nprint(f\"‚úì Common mask: {n_voxels:,} voxels\")\n\nprint(\"\\n‚úì All prerequisites loaded!\")"
  },
  {
   "cell_type": "markdown",
   "id": "65e1530c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Loading Prerequisites\n",
    "\n",
    "We need:\n",
    "- Subject/session info (sub-01, ses-010)\n",
    "- Run IDs (4 runs)\n",
    "- BOLD images (preprocessed fMRI data)\n",
    "- Event files (for alignment)\n",
    "- Common brain mask (from GLM analysis)\n",
    "\n",
    "**Note:** If you haven't run notebook 01, this will create a fresh mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f2d50",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": "# Load and align activations from replays\n\n# First, check if we have a trained model\nfrom pathlib import Path\n\nMODEL_DIR = Path('models/')\nMODEL_PATH = MODEL_DIR / 'mario_ppo_agent.pth'\n\nif not MODEL_PATH.exists():\n    print(f\"‚úó No trained model found at: {MODEL_PATH}\")\n    print(\"\\nYou need a trained RL agent to extract activations.\")\n    print(\"Please train an agent first by running:\")\n    print(\"  python ../train_mario_agent.py --steps 5000000\")\n    print(\"\\n‚ö† Cannot proceed with encoding analysis without trained model\")\n    HAS_MODEL = False\nelse:\n    print(f\"‚úì Found trained model: {MODEL_PATH}\")\n    HAS_MODEL = True\n    \n    # Load the model\n    from rl_utils import load_pretrained_model, align_activations_to_bold\n    \n    print(\"\\nLoading model...\")\n    model = load_pretrained_model(MODEL_PATH, device='cpu')\n    print(\"‚úì Model loaded\")\n    \n    # Align activations to BOLD\n    # This will:\n    # 1. Load replay files for each game segment\n    # 2. Extract RL activations at 60Hz  \n    # 3. Downsample to TR (1.49s)\n    # 4. Apply HRF convolution\n    # 5. Create NaN mask for non-gameplay periods\n    \n    alignment_results = align_activations_to_bold(\n        model=model,\n        subject=SUBJECT,\n        session=SESSION,\n        runs=runs,\n        sourcedata_path=sourcedata_path,\n        tr=TR,\n        device='cpu',\n        apply_hrf=True,  # Apply HRF convolution\n        bold_imgs=bold_imgs  # Pass BOLD images for exact TR count\n    )\n    \n    # Extract results\n    layer_activations = alignment_results['activations']\n    valid_mask = alignment_results['mask']\n    run_info = alignment_results['run_info']\n    \n    print(f\"\\n{'='*70}\")\n    print(\"Alignment summary:\")\n    for info in run_info:\n        print(f\"  {info['run']}: {info['n_valid_trs']}/{info['n_trs']} TRs \"\n              f\"({info['n_segments']} game segments)\")\n    print(f\"{'='*70}\\n\")"
  },
  {
   "cell_type": "markdown",
   "id": "535d9cdd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Loading and Aligning RL Activations\n",
    "\n",
    "**NEW APPROACH:**\n",
    "\n",
    "Instead of using pre-extracted activations, we now:\n",
    "\n",
    "1. **Load replay files** from the human subject's actual gameplay\n",
    "   - Uses `.bk2` replay files from `sourcedata/mario/`\n",
    "   - Matches exact stimuli presented during fMRI scanning\n",
    "\n",
    "2. **Extract activations frame-by-frame** (60Hz)\n",
    "   - Pass replay frames through trained RL agent\n",
    "   - Collect CNN activations from all layers\n",
    "\n",
    "3. **Align to fMRI timing**\n",
    "   - Use `mario.annotations` files to get game segment timing\n",
    "   - Downsample from 60Hz to TR (1.49s)\n",
    "   - Apply HRF convolution\n",
    "\n",
    "4. **Handle multiple games per run**\n",
    "   - Concatenate gameplay segments\n",
    "   - Mask inter-game periods with NaN\n",
    "\n",
    "**This ensures perfect alignment between RL activations and BOLD data!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5490c7fd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": "# Clean and prepare BOLD data\n\nfrom encoding_utils import load_and_prepare_bold\n\nprint(\"Cleaning BOLD data...\")\nprint(\"This performs:\")\nprint(\"  1. Confound regression (motion, WM, CSF, global signal)\")\nprint(\"  2. Detrending (remove linear drift)\")\nprint(\"  3. Standardization (z-score each voxel)\")\nprint(\"\\nNote: High-pass filtering is handled by fMRIPrep confounds\\n\")\n\nbold_data = load_and_prepare_bold(\n    bold_paths,  # Use paths instead of images for confound loading\n    mask_img=common_mask,\n    detrend=True,\n    standardize=True,\n    t_r=TR,\n    load_confounds_from_fmriprep=True  # Automatically load confounds from fMRIPrep\n)\n\nprint(f\"‚úì BOLD prepared:\")\nprint(f\"  Shape: {bold_data.shape}\")\nprint(f\"  Timepoints: {bold_data.shape[0]}\")\nprint(f\"  Voxels: {bold_data.shape[1]:,}\")"
  },
  {
   "cell_type": "markdown",
   "id": "01ec498b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": "## 4. Cleaning and Preparing BOLD Data\n\n**Preprocessing steps:**\n\n1. **Confound regression:** Remove nuisance signals from each voxel's timeseries\n   - Motion parameters (6 DOF: translation + rotation)\n   - White matter signal (non-neural tissue)\n   - CSF signal (physiological pulsations)\n   - Global signal (whole-brain average)\n   - High-pass filter components (from fMRIPrep, removes slow drifts <1/128 Hz)\n\n2. **Detrending:** Remove linear drift within each run\n\n3. **Standardization:** Z-score each voxel (mean=0, std=1)\n\n**What is confound regression?**\n\nThink of it as \"noise cancellation\" for fMRI:\n- BOLD signal = neural activity + artifacts (motion, heartbeat, breathing, scanner drift)\n- For each voxel, we fit a linear model: `BOLD = Œ≤‚ÇÅ¬∑motion + Œ≤‚ÇÇ¬∑WM + Œ≤‚ÇÉ¬∑CSF + ... + Œµ`\n- We keep only the residual (Œµ) = signal unexplained by confounds\n- This \"cleaned\" signal better reflects neural activity\n\n**Why is this important?**\n- Head motion creates spurious correlations between brain regions\n- Without cleaning, you might \"predict\" brain activity that's actually just head movement\n- Confound regression removes these artifacts while preserving neural signals\n\n**Output:** `(timepoints √ó voxels)` matrix ready for regression, with artifacts removed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485af949",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Check alignment between BOLD and activations\n",
    "\n",
    "if HAS_MODEL:\n",
    "    n_bold = bold_data.shape[0]\n",
    "    n_acts = list(layer_activations.values())[0].shape[0]\n",
    "    \n",
    "    print(f\"BOLD timepoints: {n_bold}\")\n",
    "    print(f\"Activations timepoints: {n_acts}\")\n",
    "    print(f\"Valid (gameplay) timepoints: {valid_mask.sum()}\")\n",
    "    print(f\"Invalid (non-gameplay) timepoints: {(~valid_mask).sum()}\")\n",
    "    \n",
    "    # Ensure dimensions match\n",
    "    if n_bold != n_acts:\n",
    "        print(f\"\\n‚ö† Dimension mismatch!\")\n",
    "        print(f\"  Truncating to minimum length: {min(n_bold, n_acts)}\")\n",
    "        n_time = min(n_bold, n_acts)\n",
    "        bold_data = bold_data[:n_time]\n",
    "        valid_mask = valid_mask[:n_time]\n",
    "        for layer in layer_activations.keys():\n",
    "            layer_activations[layer] = layer_activations[layer][:n_time]\n",
    "    else:\n",
    "        print(\"\\n‚úì Dimensions match!\")\n",
    "else:\n",
    "    print(\"‚ö† No model available, skipping alignment check\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a2b66",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Alignment Status\n",
    "\n",
    "**Automatic alignment completed!**\n",
    "\n",
    "The `align_activations_to_bold()` function has:\n",
    "\n",
    "1. ‚úÖ **Loaded replay files** for each game segment\n",
    "2. ‚úÖ **Extracted RL activations** at 60Hz from replay frames\n",
    "3. ‚úÖ **Downsampled to TR** using temporal averaging within each TR window\n",
    "4. ‚úÖ **Applied HRF convolution** to account for hemodynamic lag\n",
    "5. ‚úÖ **Created validity mask** to mark gameplay vs non-gameplay periods\n",
    "\n",
    "**Key differences from old approach:**\n",
    "- OLD: Arbitrary agent gameplay, misaligned\n",
    "- NEW: Exact subject gameplay from replays, perfectly aligned\n",
    "\n",
    "**Dimensions should now match:**\n",
    "- BOLD: Number of TRs across all runs\n",
    "- Activations: Same number of TRs (with NaN for non-gameplay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f09db7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": "# Create run-based train/test splits\n\nif HAS_MODEL:\n    print(\"Setting up run-based cross-validation...\")\n    print(\"\\nIMPORTANT: For proper generalization, we should use leave-one-run-out CV.\")\n    print(\"This ensures the model is tested on completely unseen runs.\\n\")\n    \n    # Calculate run boundaries in concatenated data\n    run_boundaries = [0]\n    for info in run_info:\n        run_boundaries.append(run_boundaries[-1] + info['n_trs'])\n    \n    print(\"Run boundaries (in concatenated array):\")\n    for i, (run, info) in enumerate(zip(runs, run_info)):\n        start_idx = run_boundaries[i]\n        end_idx = run_boundaries[i+1]\n        print(f\"  {run}: TRs {start_idx}-{end_idx} ({info['n_trs']} TRs, {info['n_valid_trs']} valid)\")\n    \n    # For simplicity in this tutorial, we'll use first 3 runs for training, last run for testing\n    # In a real analysis, you should do full leave-one-run-out cross-validation\n    test_run_idx = 3  # Use last run as test set\n    \n    # Get train indices (first 3 runs) and test indices (last run)\n    train_start = run_boundaries[0]\n    train_end = run_boundaries[test_run_idx]\n    test_start = run_boundaries[test_run_idx]\n    test_end = run_boundaries[test_run_idx + 1]\n    \n    # Get valid (gameplay) indices within train and test sets\n    all_indices = np.arange(len(valid_mask))\n    train_all_indices = all_indices[train_start:train_end]\n    test_all_indices = all_indices[test_start:test_end]\n    \n    # Filter to only valid (gameplay) TRs\n    train_valid_indices = train_all_indices[valid_mask[train_start:train_end]]\n    test_valid_indices = test_all_indices[valid_mask[test_start:test_end]]\n    \n    print(f\"\\nRun-based split:\")\n    print(f\"  Train runs: {runs[:test_run_idx]}\")\n    print(f\"  Test run: {runs[test_run_idx]}\")\n    print(f\"  Train TRs (gameplay only): {len(train_valid_indices)}\")\n    print(f\"  Test TRs (gameplay only): {len(test_valid_indices)}\")\n    \n    print(\"\\n‚ö† Note: For a full analysis, implement leave-one-run-out CV and average results!\")\nelse:\n    print(\"‚ö† No model available, skipping train/test split\")"
  },
  {
   "cell_type": "markdown",
   "id": "78103a6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": "## 6. Run-Based Train/Test Split\n\n**Critical methodological point:** We must use **run-based cross-validation**, not random splitting!\n\n**Why run-based?**\n- **Temporal autocorrelation**: Adjacent TRs are correlated (hemodynamic response spans ~15-20 seconds)\n- **Random split**: Train and test would contain adjacent TRs from the same run ‚Üí inflated performance\n- **Run-based split**: Test set is from completely unseen runs ‚Üí true generalization\n\n**Leave-One-Run-Out (LORO) Cross-Validation:**\n- Train on N-1 runs, test on 1 held-out run\n- Repeat for each run as test set\n- Average results across folds\n- This is the gold standard for fMRI encoding models\n\n**Simplified approach (this notebook):**\n- Train: Runs 1-3\n- Test: Run 4\n- For a real analysis, implement full LORO and average across all folds\n\n**Only use gameplay TRs:**\n- Both train and test only include TRs where the subject was actually playing\n- Non-gameplay periods (between games) are excluded using the valid_mask"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07912745",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Apply PCA to layer activations\n",
    "\n",
    "if HAS_MODEL:\n",
    "    from rl_utils import apply_pca_with_nan_handling\n",
    "    \n",
    "    print(\"Applying PCA to reduce dimensionality...\")\n",
    "    print(\"(PCA is fit only on valid gameplay TRs)\\n\")\n",
    "    \n",
    "    pca_results = apply_pca_with_nan_handling(\n",
    "        layer_activations,\n",
    "        valid_mask,\n",
    "        n_components=50,\n",
    "        variance_threshold=0.9\n",
    "    )\n",
    "    \n",
    "    # Extract reduced activations\n",
    "    reduced_activations = pca_results['reduced_activations']\n",
    "    pca_models = pca_results['pca_models']\n",
    "    variance_explained = pca_results['variance_explained']\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"PCA summary:\")\n",
    "    for layer, acts in reduced_activations.items():\n",
    "        print(f\"  {layer}: {acts.shape[1]} components\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "else:\n",
    "    print(\"‚ö† No model available, skipping PCA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394c681a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 7. Fitting Ridge Regression Encoding Models\n",
    "\n",
    "**For each layer:**\n",
    "1. Use PCA-reduced activations (50 components)\n",
    "2. Cross-validate to find optimal Œ± (regularization strength)\n",
    "3. Fit ridge regression on training data (gameplay TRs only)\n",
    "4. Predict BOLD on test data\n",
    "5. Compute R¬≤ per voxel\n",
    "\n",
    "**Hyperparameter search:** Œ± ‚àà [0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "\n",
    "**NaN handling:**\n",
    "- Only valid (gameplay) TRs are used for training and testing\n",
    "- Invalid TRs are automatically excluded\n",
    "\n",
    "**Output per layer:**\n",
    "- Best Œ±\n",
    "- R¬≤ map: `(voxels,)` array\n",
    "- Trained model\n",
    "\n",
    "**Runtime:** ~5-10 minutes for all 5 layers √ó 213k voxels\n",
    "\n",
    "**Interpretation:**\n",
    "- R¬≤ > 0: Features explain variance in BOLD\n",
    "- R¬≤ ‚âà 0: No prediction (chance level)\n",
    "- Negative R¬≤: Worse than mean baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0kcwmp9zb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit ridge regression encoding models\n",
    "\n",
    "if HAS_MODEL:\n",
    "    from encoding_utils import fit_encoding_model_per_layer\n",
    "    \n",
    "    alphas = [0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "    \n",
    "    print(\"Fitting ridge regression (5 layers √ó voxels)...\")\n",
    "    print(\"This takes ~5-10 minutes\\n\")\n",
    "    \n",
    "    encoding_results = fit_encoding_model_per_layer(\n",
    "        reduced_activations,\n",
    "        bold_data,\n",
    "        common_mask,\n",
    "        train_valid_indices,\n",
    "        test_valid_indices,\n",
    "        alphas=alphas,\n",
    "        valid_mask=valid_mask  # Pass valid mask for NaN handling\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úì Encoding complete!\")\n",
    "else:\n",
    "    print(\"‚ö† No model available, skipping encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dbb1ff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Compare layer performance\n",
    "\n",
    "if HAS_MODEL:\n",
    "    from encoding_utils import compare_layer_performance\n",
    "    from encoding_viz_utils import plot_layer_comparison_bars\n",
    "    \n",
    "    comparison_df = compare_layer_performance(encoding_results)\n",
    "    \n",
    "    print(\"Layer Performance:\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    best_layer = comparison_df.iloc[0]['layer']\n",
    "    best_r2 = comparison_df.iloc[0]['mean_r2']\n",
    "    \n",
    "    print(f\"\\n‚≠ê Best: {best_layer.upper()} (R¬≤ = {best_r2:.4f})\")\n",
    "    \n",
    "    # Visualize\n",
    "    layer_order = ['conv1', 'conv2', 'conv3', 'conv4', 'linear']\n",
    "    fig = plot_layer_comparison_bars(encoding_results, layer_order)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö† No model available, skipping layer comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e8bac1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 8. Comparing Layer Performance\n",
    "\n",
    "**Question:** Which CNN layer best predicts brain activity?\n",
    "\n",
    "**Metrics:**\n",
    "- Mean R¬≤ (test set)\n",
    "- Median R¬≤ (robust to outliers)\n",
    "- % voxels with R¬≤ > 0.01 (significantly predicted)\n",
    "\n",
    "**Expected pattern (if hypothesis holds):**\n",
    "- Early layers (conv1/2) ‚Üí Visual cortex\n",
    "- Middle layers (conv3/4) ‚Üí Motor/parietal\n",
    "- Late layers (linear) ‚Üí Frontal/executive\n",
    "\n",
    "**See bar plot below for comparison.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b0c840",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize R¬≤ brain maps (best layer)\n",
    "\n",
    "if HAS_MODEL:\n",
    "    from encoding_viz_utils import plot_r2_brainmap\n",
    "    \n",
    "    best_layer = comparison_df.iloc[0]['layer']\n",
    "    best_r2_map = encoding_results[best_layer]['r2_map']\n",
    "    \n",
    "    print(f\"Best layer: {best_layer.upper()}\\n\")\n",
    "    \n",
    "    fig = plot_r2_brainmap(\n",
    "        best_r2_map, \n",
    "        best_layer,\n",
    "        threshold=0.01,\n",
    "        vmax=0.2\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìç Interpretation:\")\n",
    "    print(\"  Hot regions = Well predicted by this layer\")\n",
    "    print(\"  - Early layers ‚Üí Visual cortex\")\n",
    "    print(\"  - Middle layers ‚Üí Motor/parietal\")\n",
    "    print(\"  - Late layers ‚Üí Frontal/executive\")\n",
    "else:\n",
    "    print(\"‚ö† No model available, skipping brain map visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763ffbbf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary: Brain Encoding with Proper Alignment\n",
    "\n",
    "**What we accomplished:**\n",
    "\n",
    "1. ‚úÖ **Loaded RL model:** Trained PPO agent\n",
    "2. ‚úÖ **Extracted activations from replays:** Used actual gameplay .bk2 files\n",
    "3. ‚úÖ **Proper temporal alignment:**\n",
    "   - Matched replay frames to fMRI TRs using annotations\n",
    "   - Downsampled from 60Hz to TR (1.49s)\n",
    "   - Applied HRF convolution\n",
    "   - Masked non-gameplay periods with NaN\n",
    "4. ‚úÖ **Applied PCA:** Reduced to 50 components per layer (on valid TRs only)\n",
    "5. ‚úÖ **Fit encoding models:** Ridge regression with NaN-aware training\n",
    "6. ‚úÖ **Compared layers:** Identified which layer best predicts brain activity\n",
    "7. ‚úÖ **Visualized brain maps:** Localized where each layer is encoded\n",
    "\n",
    "---\n",
    "\n",
    "### Key Improvements Over Original Approach\n",
    "\n",
    "**What was fixed:**\n",
    "\n",
    "1. **Proper replay alignment:**\n",
    "   - OLD: Agent played arbitrary Level1-1, misaligned with BOLD\n",
    "   - NEW: Extract activations from exact subject gameplay replays\n",
    "\n",
    "2. **Temporal alignment:**\n",
    "   - OLD: No alignment, simple truncation\n",
    "   - NEW: Use annotation files to map frames ‚Üí TRs with onset/duration\n",
    "\n",
    "3. **HRF convolution:**\n",
    "   - OLD: Missing HRF convolution\n",
    "   - NEW: Applied SPM HRF after downsampling\n",
    "\n",
    "4. **Multiple games per run:**\n",
    "   - OLD: Couldn't handle multiple game segments\n",
    "   - NEW: Concatenate segments, mask inter-game periods with NaN\n",
    "\n",
    "5. **NaN handling:**\n",
    "   - OLD: No way to exclude non-gameplay periods\n",
    "   - NEW: Valid mask ensures encoding models only train on gameplay TRs\n",
    "\n",
    "---\n",
    "\n",
    "### Expected Results\n",
    "\n",
    "**If brain uses RL-like representations:**\n",
    "\n",
    "- **Early layers (conv1/2):** Predict visual cortex\n",
    "  - Edge detection, textures, low-level visual features\n",
    "  - Occipital lobe activation\n",
    "\n",
    "- **Middle layers (conv3/4):** Predict parietal/motor cortex\n",
    "  - Spatial layout, object positions, movement planning\n",
    "  - Parietal lobe, premotor cortex\n",
    "\n",
    "- **Late layers (linear):** Predict frontal cortex\n",
    "  - Value estimates, policy selection, abstract strategy\n",
    "  - Prefrontal cortex, anterior cingulate\n",
    "\n",
    "**Hierarchical gradient:**\n",
    "- R¬≤ should increase from early ‚Üí late layers if brain uses RL features\n",
    "- Brain maps should show posterior ‚Üí anterior gradient\n",
    "\n",
    "---\n",
    "\n",
    "### Methodological Lessons\n",
    "\n",
    "**Critical requirements for encoding analysis:**\n",
    "\n",
    "1. **Proper stimulus alignment:**\n",
    "   - Use exact stimuli presented to subject\n",
    "   - Match timing precisely (frame-by-frame if needed)\n",
    "   - Account for hemodynamic lag (HRF)\n",
    "\n",
    "2. **Data quality:**\n",
    "   - Sufficient valid trials (gameplay periods)\n",
    "   - Good signal-to-noise ratio\n",
    "   - Proper preprocessing (motion correction, etc.)\n",
    "\n",
    "3. **Statistical power:**\n",
    "   - Multiple sessions/subjects for group analysis\n",
    "   - Cross-validation to avoid overfitting\n",
    "   - Appropriate regularization (ridge Œ±)\n",
    "\n",
    "4. **Interpretation:**\n",
    "   - Compare to baseline models (GLM with behavioral features)\n",
    "   - Test specific hypotheses about layer-to-region mapping\n",
    "   - Consider alternative explanations (motion, attention, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison to GLM (Notebook 01)\n",
    "\n",
    "**GLM:**\n",
    "- ‚úÖ Hypothesis-driven (LEFT_THUMB vs RIGHT_THUMB)\n",
    "- ‚úÖ Simple, interpretable\n",
    "- ‚úÖ Works with sparse events\n",
    "- ‚úÖ Found significant effects (contralateral motor control)\n",
    "\n",
    "**Encoding (This notebook):**\n",
    "- ‚úÖ Data-driven (learned RL features)\n",
    "- ‚úÖ Exploratory (discover representations)\n",
    "- ‚úÖ Tests computational theories\n",
    "- ‚è≥ Requires more data and careful alignment\n",
    "\n",
    "**Both approaches are complementary:**\n",
    "- GLM: Validate known effects\n",
    "- Encoding: Discover new representations\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps & Extensions\n",
    "\n",
    "**To improve these results:**\n",
    "\n",
    "1. **More data:**\n",
    "   - Aggregate across multiple sessions\n",
    "   - Multi-subject analysis\n",
    "   - Increase gameplay duration\n",
    "\n",
    "2. **Better features:**\n",
    "   - Try different layers simultaneously\n",
    "   - Non-linear encoding (kernel ridge, neural networks)\n",
    "   - Task-specific features (value, prediction error, etc.)\n",
    "\n",
    "3. **Control analyses:**\n",
    "   - Compare to pixel-based features\n",
    "   - Test against behavioral-only models\n",
    "   - Permutation testing for significance\n",
    "\n",
    "4. **Advanced methods:**\n",
    "   - Hyperalignment across subjects\n",
    "   - Representational similarity analysis (RSA)\n",
    "   - Decoding (BOLD ‚Üí predicted actions)\n",
    "\n",
    "---\n",
    "\n",
    "### Research Questions\n",
    "\n",
    "**This pipeline enables investigating:**\n",
    "\n",
    "1. **Computational neuroscience:**\n",
    "   - Do brain and RL agent use similar representations?\n",
    "   - Where are value and policy encoded in the brain?\n",
    "   - How do representations change with learning?\n",
    "\n",
    "2. **Cognitive neuroscience:**\n",
    "   - How does the brain represent game state?\n",
    "   - What role does prediction play in decision-making?\n",
    "   - How are visual and motor systems integrated?\n",
    "\n",
    "3. **AI alignment:**\n",
    "   - Can we build agents that think like humans?\n",
    "   - What makes representations interpretable?\n",
    "   - How to design human-aligned reward functions?\n",
    "\n",
    "**This tutorial provides a complete, working pipeline for these investigations!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}