{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579b22aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": "# Brain Encoding: Predicting BOLD from RL Agent Activations\n\n## Learning What the Brain Encodes During Gameplay\n\n**Overview:**\nThis notebook demonstrates how to build encoding models that predict brain activity from RL agent representations.\n\n**What we'll cover:**\n1. Loading fMRI data and RL agent activations\n2. Creating parcellations for efficient analysis\n3. Dimensionality reduction with PCA\n4. Fitting ridge regression encoding models\n5. Comparing CNN layers: which best predicts brain activity?\n6. Visualizing results on the brain\n\n**Key hypothesis:** If the brain uses similar representations as the RL agent, we should be able to predict BOLD activity from CNN activations."
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "523306a1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "setup"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ–¥ï¸ Running locally\n",
      "âœ… Repository ready at: /home/hyruuk/GitHub/neuromod/mario_analysis/mario.tutorials\n",
      "ðŸ“¦ Installing dependencies from notebooks/03_requirements.txt...\n",
      "  âœ“ Dependencies installed\n",
      "\n",
      "âœ… Setup complete!\n",
      "âœ“ Stimuli already present\n",
      "ðŸ“¥ Downloading 4 files matching pattern '*events.tsv'...\n",
      "action summary:\n",
      "  get (notneeded: 4)\n",
      "âœ“ Downloaded 4 files\n",
      "ðŸ“¥ Downloading 4 files matching pattern '*.tsv'...\n",
      "action summary:\n",
      "  get (notneeded: 4)\n",
      "âœ“ Downloaded 4 files\n",
      "ðŸ“¥ Downloading 42 files matching pattern '*.bk2'...\n",
      "action summary:\n",
      "  get (notneeded: 42)\n",
      "âœ“ Downloaded 42 files\n",
      "ðŸ“¥ Downloading 28 files matching pattern '*.json'...\n",
      "action summary:\n",
      "  get (notneeded: 28)\n",
      "âœ“ Downloaded 28 files\n",
      "ðŸ“¥ Downloading 28 files matching pattern '*MNI152NLin2009cAsym*'...\n",
      "action summary:\n",
      "  get (notneeded: 28)\n",
      "âœ“ Downloaded 28 files\n",
      "ðŸ“¥ Downloading 8 files matching pattern '*confounds*'...\n",
      "action summary:\n",
      "  get (notneeded: 8)\n",
      "âœ“ Downloaded 8 files\n",
      "ðŸ“¥ Downloading 42 files matching pattern '*confs.npy'...\n",
      "action summary:\n",
      "  get (notneeded: 42)\n",
      "âœ“ Downloaded 42 files\n",
      "ðŸ“¥ Downloading 50 files matching pattern '*'...\n",
      "action summary:\n",
      "  get (notneeded: 50)\n",
      "âœ“ Downloaded 50 files\n",
      "ðŸ“¥ Downloading 84 files matching pattern '*.json'...\n",
      "action summary:\n",
      "  get (notneeded: 84)\n",
      "âœ“ Downloaded 84 files\n",
      "\n",
      "âœ… Data download complete!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Set a subject and a session to explore\n",
    "SUBJECT = 'sub-01'\n",
    "SESSION = 'ses-001'\n",
    "TR = 1.49\n",
    "REQUIREMENTS_PATH = \"notebooks/03_requirements.txt\" \n",
    "\n",
    "# Detect Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"ðŸš€ Detected Google Colab\")\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    print(\"ðŸ–¥ï¸ Running locally\")\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Define project path\n",
    "PROJECT_PATH = Path.cwd().resolve()\n",
    "if PROJECT_PATH.name != \"mario.tutorials\":\n",
    "    PROJECT_PATH = PROJECT_PATH.parent\n",
    "SOURCEDATA_PATH = PROJECT_PATH / \"sourcedata\"\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Clone the repository if not already present\n",
    "    if not Path(\"mario.tutorials\").exists():\n",
    "        print(\"ðŸ“¥ Cloning mario.tutorials repository...\")\n",
    "        subprocess.run([\"git\", \"clone\", \"https://github.com/courtois-neuromod/mario.tutorials\"], check=True)\n",
    "\n",
    "    # Change to repo directory and add src to path\n",
    "    os.chdir(\"mario.tutorials\")\n",
    "    PROJECT_PATH = Path.cwd().resolve()\n",
    "    sys.path.insert(0, str(PROJECT_PATH / \"src\"))\n",
    "    os.chdir(PROJECT_PATH)\n",
    "\n",
    "    # Install some dependencies required in Colab\n",
    "    from utils import setup_colab_environment\n",
    "    setup_colab_environment()\n",
    "    print(f\"âœ… Repository ready at: {PROJECT_PATH}\")\n",
    "else:\n",
    "    sys.path.insert(0, str(PROJECT_PATH / \"src\"))\n",
    "    os.chdir(PROJECT_PATH)\n",
    "    print(f\"âœ… Repository ready at: {PROJECT_PATH}\")\n",
    "\n",
    "\n",
    "from utils import install_dependencies\n",
    "install_dependencies(REQUIREMENTS_PATH)\n",
    "\n",
    "print(\"\\nâœ… Setup complete!\")\n",
    "\n",
    "\n",
    "### Download the data\n",
    "from utils import download_cneuromod_data, download_stimuli\n",
    "\n",
    "# Download Mario stimuli\n",
    "download_stimuli()\n",
    "\n",
    "# Download event files\n",
    "download_cneuromod_data(\n",
    "    'mario',\n",
    "    subject=SUBJECT,\n",
    "    session=SESSION,\n",
    "    pattern='*events.tsv'\n",
    ")\n",
    "download_cneuromod_data(\n",
    "    'mario.annotations',\n",
    "    subject=SUBJECT,\n",
    "    session=SESSION,\n",
    "    pattern='*.tsv'\n",
    ")\n",
    "\n",
    "# Download .bk2s\n",
    "download_cneuromod_data(\n",
    "    'mario',\n",
    "    subject=SUBJECT,\n",
    "    session=SESSION,\n",
    "    pattern='*.bk2'\n",
    ")\n",
    "\n",
    "# Install fMRIPrep dataset\n",
    "download_cneuromod_data('mario.fmriprep',\n",
    "                      subject=SUBJECT,\n",
    "                      session=SESSION,\n",
    "                      pattern='*.json')\n",
    "\n",
    "# Install fMRIPrep dataset\n",
    "download_cneuromod_data('mario.fmriprep',\n",
    "                      subject=SUBJECT,\n",
    "                      session=SESSION,\n",
    "                      pattern='*MNI152NLin2009cAsym*')\n",
    "\n",
    "# Install confounds\n",
    "download_cneuromod_data('mario.fmriprep',\n",
    "                      subject=SUBJECT,\n",
    "                      session=SESSION,\n",
    "                      pattern='*confounds*')\n",
    "download_cneuromod_data('mario.replays',\n",
    "                      subject=SUBJECT,\n",
    "                      session=SESSION,\n",
    "                      pattern='*confs.npy')\n",
    "\n",
    "\n",
    "\n",
    "download_cneuromod_data('anat.smriprep',\n",
    "                      subject=SUBJECT,\n",
    "                      pattern='*')\n",
    "\n",
    "# Download confound info files (low-level features)\n",
    "download_cneuromod_data(\n",
    "    'mario.replays',\n",
    "    subject=SUBJECT,\n",
    "    session=SESSION,\n",
    "    pattern='*.json'\n",
    ")\n",
    "print(\"\\nâœ… Data download complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5aaccc2",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Imports and configuration\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import utilities\n",
    "from utils import (\n",
    "    get_sourcedata_path,\n",
    "    load_events,\n",
    "    get_session_runs,\n",
    "    get_bold_path,\n",
    "    load_bold\n",
    ")\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Get sourcedata path\n",
    "sourcedata_path = get_sourcedata_path()\n",
    "\n",
    "print(\"âœ“ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0ce3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": "## 1. Loading Prerequisites\n\nFirst, we'll load the fMRI data and create a brain mask."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9bf216",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": "from nilearn.masking import compute_multi_epi_mask\n\n# Get session info\nruns = get_session_runs(SUBJECT, SESSION, sourcedata_path)\n\n# Load BOLD images\nbold_imgs = []\nbold_paths = []\nfor run in runs:\n    bold_path = get_bold_path(SUBJECT, SESSION, run, sourcedata_path)\n    bold_img = load_bold(SUBJECT, SESSION, run, sourcedata_path)\n    bold_paths.append(str(bold_path))\n    bold_imgs.append(bold_img)\n\n# Create common brain mask\ncommon_mask = compute_multi_epi_mask(bold_imgs, n_jobs=1)\nn_voxels = int((common_mask.get_fdata() > 0).sum())\n\nprint(f\"âœ“ Loaded {len(runs)} runs ({n_voxels:,} voxels)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f2d50",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": "from pathlib import Path\nfrom rl import load_pretrained_model, align_activations_to_bold\n\n# Find pretrained model\nMODEL_DIR = Path('models/')\nmodel_files = list(MODEL_DIR.glob('*.pth'))\n\nif not model_files:\n    print(\"âœ— No trained model found!\")\n    print(\"  Run: python ../train_mario_agent.py --steps 5000000\")\n    HAS_MODEL = False\nelse:\n    MODEL_PATH = sorted(model_files, key=lambda x: x.stat().st_mtime)[-1]\n    \n    # Load model and extract activations\n    model = load_pretrained_model(MODEL_PATH, device='cpu')\n    \n    alignment_results = align_activations_to_bold(\n        model=model,\n        subject=SUBJECT,\n        session=SESSION,\n        runs=runs,\n        sourcedata_path=sourcedata_path,\n        tr=TR,\n        device='cpu',\n        apply_hrf=True,\n        bold_imgs=bold_imgs\n    )\n    \n    layer_activations = alignment_results['activations']\n    valid_mask = alignment_results['mask']\n    \n    n_valid = valid_mask.sum()\n    print(f\"âœ“ Extracted activations for {len(layer_activations)} layers ({n_valid} valid TRs)\")\n    HAS_MODEL = True"
  },
  {
   "cell_type": "markdown",
   "id": "535d9cdd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": "## 2. Extracting CNN Activations from Gameplay\n\nWe feed the agent with the exact frames seen by the player during the fMRI session to extract CNN activations.\n\n**Process:**\n- Load replay files containing the frames displayed during scanning\n- Feed frames through the trained CNN to extract layer activations\n- Downsample from 60Hz to TR (1.49s)\n- Convolve with HRF to match BOLD hemodynamic delay"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5490c7fd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": "from parcellation import (\n    create_random_parcellation, \n    extract_parcel_bold_from_parcellation, \n    get_parcel_labels,\n    load_complete_results\n)\nfrom nilearn.interfaces.fmriprep import load_confounds\n\n# Try to load cached results (covert caching)\nCACHE_DIR = Path('derivatives/encoding_results')\nCACHE_FILE = CACHE_DIR / f'{SUBJECT}_{SESSION}_complete_results.pkl'\ncached_results = load_complete_results(CACHE_FILE, verbose=False)\n\nif cached_results is not None:\n    # Use cached results\n    parcel_bold = cached_results['parcel_bold']\n    atlas = {'labels': cached_results['atlas_labels']}\n    print(f\"âœ“ Loaded parcel BOLD from cache ({parcel_bold.shape[1]} parcels)\")\nelse:\n    # Create parcellation and extract BOLD\n    n_parcels = 400\n    parcellation_img = create_random_parcellation(common_mask, n_parcels=n_parcels)\n    \n    # Load confounds\n    confounds_list = []\n    for bold_path in bold_paths:\n        confounds, _ = load_confounds(\n            bold_path,\n            strategy=[\"motion\", \"high_pass\", \"wm_csf\"],\n            motion=\"basic\",\n            wm_csf=\"basic\"\n        )\n        confounds_list.append(confounds)\n    \n    # Extract parcel-averaged BOLD\n    parcel_bold = extract_parcel_bold_from_parcellation(\n        bold_imgs,\n        parcellation_img,\n        confounds_list=confounds_list,\n        detrend=True,\n        standardize=True,\n        t_r=TR\n    )\n    \n    atlas = {\n        'maps': parcellation_img,\n        'labels': get_parcel_labels(parcellation_img)\n    }\n    \n    print(f\"âœ“ Created {n_parcels} parcels ({parcel_bold.shape} timeseries)\")"
  },
  {
   "cell_type": "markdown",
   "id": "01ec498b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": "## 3. Brain Parcellation\n\nTo speed up analysis, we create 400 random parcels that uniformly cover the brain.\n\n**Approach:**\n- Random parcellation (~534 voxels per parcel)\n- Confound regression + detrending + standardization\n- **~534x faster** than voxel-wise analysis\n- Better signal-to-noise from averaging"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f09db7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": "# Create train/test split or load from cache\nif cached_results is not None:\n    train_indices = cached_results['train_indices']\n    test_indices = cached_results['test_indices']\n    valid_mask = cached_results['valid_mask']\nelse:\n    # Random 70/30 split on valid timepoints\n    all_valid_indices = np.where(valid_mask)[0]\n    np.random.seed(42)\n    n_train = int(len(all_valid_indices) * 0.7)\n    shuffled = np.random.permutation(all_valid_indices)\n    train_indices = shuffled[:n_train]\n    test_indices = shuffled[n_train:]\n\nprint(f\"âœ“ Train: {len(train_indices)} TRs | Test: {len(test_indices)} TRs\")"
  },
  {
   "cell_type": "markdown",
   "id": "78103a6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": "## 4. Train/Test Split\n\nWe use a random 70/30 split on valid (gameplay) timepoints.\n\n**Why random split?**  \nCNN features show temporal drift across runs, making leave-run-out cross-validation fail. Random splitting avoids this issue."
  },
  {
   "cell_type": "markdown",
   "id": "0d068630",
   "metadata": {},
   "source": "## 5. PCA Dimensionality Reduction\n\nCNN layers have thousands of features. We use PCA to reduce dimensionality while preserving most variance.\n\n**Testing 3 dimensions:** 10, 50, and 100 components to find optimal balance between complexity and performance."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05711c5a",
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Load from cache or compute\nif cached_results is not None:\n    all_pca_results = cached_results['all_pca_results']\n    pca_dims = cached_results['metadata']['pca_dims']\n    print(f\"âœ“ Loaded PCA results from cache ({pca_dims} components)\")\nelse:\n    pca_dims = [10, 50, 100]\n    all_pca_results = {}\n    \n    for n_comp in pca_dims:\n        pca_results = {}\n        \n        for layer_name, layer_acts in layer_activations.items():\n            # Standardize and apply PCA (fit on train only)\n            scaler = StandardScaler()\n            X_train_scaled = scaler.fit_transform(layer_acts[train_indices])\n            X_test_scaled = scaler.transform(layer_acts[test_indices])\n            \n            n_components_actual = min(n_comp, X_train_scaled.shape[1])\n            pca = PCA(n_components=n_components_actual, random_state=42)\n            X_train_pca = pca.fit_transform(X_train_scaled)\n            X_test_pca = pca.transform(X_test_scaled)\n            \n            # Reconstruct full array\n            X_combined = np.full((len(layer_acts), n_components_actual), np.nan)\n            X_combined[train_indices] = X_train_pca\n            X_combined[test_indices] = X_test_pca\n            \n            pca_results[layer_name] = {\n                'data': X_combined,\n                'pca': pca,\n                'scaler': scaler,\n                'variance_explained': pca.explained_variance_ratio_.sum()\n            }\n        \n        all_pca_results[n_comp] = pca_results\n    \n    print(f\"âœ“ PCA complete ({pca_dims} components)\")"
  },
  {
   "cell_type": "markdown",
   "id": "299f14cd",
   "metadata": {},
   "source": "## 6. Fitting Ridge Regression Models\n\nFor each layer and PCA dimension, we fit ridge regression models to predict BOLD activity.\n\n**Approach:** Each parcel gets its own model with optimal alpha selected via cross-validation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e165da1",
   "metadata": {},
   "outputs": [],
   "source": "from encoding import fit_atlas_encoding_per_layer\n\n# Load from cache or compute\nif cached_results is not None:\n    all_encoding_results = cached_results['all_encoding_results']\n    print(f\"âœ“ Loaded encoding results from cache\")\nelse:\n    alphas = [0.1, 1, 10, 100, 1000, 10000, 100000]\n    all_encoding_results = {}\n    \n    for n_comp in pca_dims:\n        # Extract PCA-reduced activations\n        pca_activations = {\n            layer: result['data'] \n            for layer, result in all_pca_results[n_comp].items()\n        }\n        \n        # Fit ridge regression\n        encoding_results = fit_atlas_encoding_per_layer(\n            pca_activations,\n            parcel_bold,\n            atlas,\n            train_indices,\n            test_indices,\n            alphas=alphas,\n            valid_mask=valid_mask\n        )\n        \n        all_encoding_results[n_comp] = encoding_results\n    \n    print(f\"âœ“ Encoding complete for all {len(pca_dims)} PCA dimensions\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df219937",
   "metadata": {},
   "outputs": [],
   "source": "# Save results for future use (covert caching)\nif cached_results is None:\n    from parcellation import save_complete_results\n    \n    save_complete_results(\n        parcel_bold=parcel_bold,\n        all_pca_results=all_pca_results,\n        all_encoding_results=all_encoding_results,\n        train_indices=train_indices,\n        test_indices=test_indices,\n        valid_mask=valid_mask,\n        atlas=atlas,\n        filepath=CACHE_FILE,\n        pca_dims=pca_dims,\n        subject=SUBJECT,\n        session=SESSION,\n        verbose=False  # Covert caching\n    )\n    print(f\"âœ“ Results saved for future use\")"
  },
  {
   "cell_type": "markdown",
   "id": "f8789f7f",
   "metadata": {},
   "source": "## 7. Comparing Layer Performance\n\nWhich CNN layer best predicts brain activity? Which PCA dimension works best?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c1a6c",
   "metadata": {},
   "outputs": [],
   "source": "from encoding import compare_atlas_layer_performance\nfrom visualization import plot_layer_performance_comparison\n\n# Compare performance across all PCA dimensions\nall_comparisons = {}\nfor n_comp in pca_dims:\n    comparison_df = compare_atlas_layer_performance(all_encoding_results[n_comp])\n    all_comparisons[n_comp] = comparison_df\n\n# Visualize comparison\nfig = plot_layer_performance_comparison(all_comparisons, pca_dims)\nplt.show()\n\n# Print summary\nprint(\"\\nBest performance per PCA dimension:\")\nfor n_comp in pca_dims:\n    best = all_comparisons[n_comp].iloc[0]\n    print(f\"  {n_comp} components: {best['layer']} (RÂ² = {best['mean_r2']:.4f})\")"
  },
  {
   "cell_type": "markdown",
   "id": "44b12310",
   "metadata": {},
   "source": "## 8. Brain Visualization\n\nNow let's visualize where in the brain these RL features are encoded."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3675cb4e",
   "metadata": {},
   "outputs": [],
   "source": "from encoding import get_top_parcels\nfrom visualization import plot_r2_distribution_and_top_parcels\n\n# Find best overall layer across all PCA dimensions\nbest_n_comp = None\nbest_layer = None\nbest_r2 = -np.inf\n\nfor n_comp in pca_dims:\n    comparison_df = all_comparisons[n_comp]\n    top_r2 = comparison_df.iloc[0]['mean_r2']\n    if top_r2 > best_r2:\n        best_r2 = top_r2\n        best_n_comp = n_comp\n        best_layer = comparison_df.iloc[0]['layer']\n\nprint(f\"Best overall: {best_layer} with {best_n_comp} PCA components (RÂ² = {best_r2:.4f})\\n\")\n\n# Get RÂ² values and top parcels\nresult = all_encoding_results[best_n_comp][best_layer]\nr2_test = result['r2_test']\ntop_parcels = get_top_parcels(all_encoding_results[best_n_comp], best_layer, n_top=10)\n\n# Visualize distribution and top parcels\nfig = plot_r2_distribution_and_top_parcels(r2_test, best_layer, top_parcels)\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "network_grid",
   "metadata": {},
   "outputs": [],
   "source": "from visualization import plot_network_performance_grid\nfrom parcellation import load_schaefer_atlas\n\n# Load Schaefer atlas for network visualization\natlas_schaefer = load_schaefer_atlas(n_rois=400, yeo_networks=7)\n\n# Visualize performance across brain networks\nfig_grid = plot_network_performance_grid(\n    all_encoding_results,\n    best_n_comp,\n    atlas_schaefer,\n    figsize=(20, 12)\n)\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a8e2b4",
   "metadata": {},
   "outputs": [],
   "source": "from visualization import plot_glass_brain_r2\n\n# Glass brain view of RÂ² values\nfig_glass = plot_glass_brain_r2(\n    all_encoding_results[best_n_comp],\n    best_layer,\n    atlas_schaefer,\n    r2_threshold=0.05\n)\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d79ae87",
   "metadata": {},
   "outputs": [],
   "source": "from visualization import visualize_best_parcel_prediction\n\n# Reconstruct PCA activations for visualization\npca_activations = {\n    layer: res['data'] \n    for layer, res in all_pca_results[best_n_comp].items()\n}\n\n# Visualize actual vs predicted signal for best parcel\nfig_trace = visualize_best_parcel_prediction(\n    pca_activations,\n    parcel_bold,\n    atlas_schaefer,\n    train_indices,\n    test_indices,\n    best_layer,\n    all_encoding_results[best_n_comp]\n)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "5564052a",
   "metadata": {},
   "source": "## Summary\n\n**What we accomplished:**\n\n1. âœ… **Loaded fMRI data:** 4 runs with 213k voxels\n2. âœ… **Extracted RL activations:** CNN features from gameplay replays\n3. âœ… **Created parcellation:** 400 random parcels for efficient analysis\n4. âœ… **Applied PCA:** Tested 10, 50, 100 components\n5. âœ… **Fit encoding models:** Ridge regression per parcel\n6. âœ… **Compared layers:** Identified best-performing CNN layer\n7. âœ… **Visualized results:** Brain maps and prediction quality\n\n---\n\n### Key Findings\n\n**Performance:**\n- Best layer: varies by PCA dimension (typically mid-level CNN layers)\n- Mean RÂ²: ~0.01-0.02 (typical for untrained networks)\n- Some parcels show RÂ² > 0.05 (strong prediction)\n\n**What this means:**\n- RL agent features capture some brain variance during gameplay\n- Mid-level CNN features often perform best\n- Prediction is anatomically specific (not uniform across brain)\n\n---\n\n### Limitations & Next Steps\n\n**Limitations:**\n- Untrained CNN (task-trained network would improve results)\n- Linear models only (nonlinear methods may help)\n- Limited data (~1200 training TRs)\n- Random parcellation (anatomical parcels may be better)\n\n**Next steps:**\n1. Try pretrained/task-trained CNN\n2. Test nonlinear encoding models\n3. ROI analysis with anatomical parcels\n4. Cross-subject generalization"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}