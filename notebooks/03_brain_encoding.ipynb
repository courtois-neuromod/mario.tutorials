{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5aaccc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup - imports and configuration\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "src_dir = Path('..') / 'src'\n",
    "sys.path.insert(0, str(src_dir))\n",
    "\n",
    "# Import utilities\n",
    "from utils import (\n",
    "    get_sourcedata_path,\n",
    "    load_events,\n",
    "    get_session_runs,\n",
    "    load_bold\n",
    ")\n",
    "\n",
    "# Import RL utilities\n",
    "from rl_utils import (\n",
    "    create_simple_proxy_features,\n",
    "    convolve_with_hrf,\n",
    "    apply_pca\n",
    ")\n",
    "\n",
    "# Import RL visualizations\n",
    "from rl_viz_utils import (\n",
    "    plot_pca_variance_per_layer,\n",
    "    plot_layer_activations_sample\n",
    ")\n",
    "\n",
    "# Import encoding utilities\n",
    "from encoding_utils import (\n",
    "    load_and_prepare_bold,\n",
    "    fit_encoding_model_per_layer,\n",
    "    compare_layer_performance\n",
    ")\n",
    "\n",
    "# Import encoding visualizations\n",
    "from encoding_viz_utils import (\n",
    "    plot_layer_comparison_bars,\n",
    "    plot_r2_brainmap\n",
    ")\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Get sourcedata path\n",
    "sourcedata_path = get_sourcedata_path()\n",
    "\n",
    "print(\"\u2713 Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Model Framework\n\n### The Brain Encoding Problem\n\n**Goal:** Use RL features to predict brain activity\n\n**Model:** Ridge Regression\n```\nBOLD(voxel, time) = \u03a3 \u03b2\u1d62 \u00b7 Feature_i(time) + \u03b5\n```\n\n**Ridge regression:** Linear regression with L2 regularization\n- Handles high-dimensional features (50 components)\n- Prevents overfitting\n- Cross-validation to select regularization strength (\u03b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9bf216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prerequisites\n",
    "\n",
    "from nilearn.masking import compute_multi_epi_mask\n",
    "\n",
    "# Define constants (assumed from the first tutorial)\n",
    "SUBJECT = 'sub-01'\n",
    "SESSION = 'ses-010'\n",
    "TR = 1.49\n",
    "\n",
    "\n",
    "# Get runs\n",
    "runs = get_session_runs(SUBJECT, SESSION, sourcedata_path)\n",
    "print(f\"Found {len(runs)} runs: {runs}\")\n",
    "\n",
    "# Load events\n",
    "all_events = []\n",
    "for run in runs:\n",
    "    events = load_events(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    all_events.append(events)\n",
    "    print(f\"  {run}: {len(events)} events\")\n",
    "\n",
    "# Create common mask (or load from main tutorial)\n",
    "print(\"\\nCreating common brain mask...\")\n",
    "bold_imgs= []\n",
    "for run in runs:\n",
    "    bold_img = load_bold(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    bold_imgs.append(bold_img)\n",
    "\n",
    "common_mask = compute_multi_epi_mask(bold_imgs, n_jobs=1)\n",
    "n_voxels = int((common_mask.get_fdata() > 0).sum())\n",
    "print(f\"\u2713 Common mask: {n_voxels:,} voxels\")\n",
    "\n",
    "print(\"\\n\u2713 All prerequisites loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the CNN activations extracted from the RL agent (from notebook 02)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load activations from 02_reinforcement_learning.ipynb\nimport pickle\nfrom pathlib import Path\n\nactivations_file = Path('../derivatives/activations/reduced_activations.pkl')\n\nif activations_file.exists():\n    with open(activations_file, 'rb') as f:\n        reduced_activations = pickle.load(f)\n    \n    print(\"\u2713 Loaded pre-computed activations\")\n    print(f\"  Layers: {list(reduced_activations.keys())}\")\n    for layer, acts in reduced_activations.items():\n        print(f\"  {layer}: {acts.shape}\")\nelse:\n    print(f\"\u2717 Activations file not found: {activations_file}\")\n    print(\"\\nPlease run 02_reinforcement_learning.ipynb first to extract CNN activations.\")\n    print(\"Alternatively, you can use proxy features for testing:\")\n    print(\"  (but results won't be meaningful)\")\n    \n    # Create minimal proxy for testing\n    import numpy as np\n    np.random.seed(42)\n    total_trs = sum(len(bold_img.get_fdata()[0,0,0,:]) for bold_img in bold_imgs)\n    \n    reduced_activations = {}\n    for layer in ['conv1', 'conv2', 'conv3', 'conv4', 'linear']:\n        reduced_activations[layer] = np.random.randn(total_trs, 50)\n    \n    print(\"\\n\u26a0 Using random proxy features (not meaningful!)\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and prepare BOLD data\n",
    "\n",
    "from encoding_utils import load_and_prepare_bold\n",
    "\n",
    "print(\"Cleaning BOLD (detrending, standardizing)...\\n\")\n",
    "\n",
    "bold_data = load_and_prepare_bold(\n",
    "    bold_imgs,\n",
    "    mask_img=common_mask,\n",
    "    confounds_list=None,  # Already cleaned in GLM\n",
    "    detrend=True,\n",
    "    standardize=True,\n",
    "    high_pass=1/128,\n",
    "    t_r=TR\n",
    ")\n",
    "\n",
    "print(f\"\u2713 BOLD prepared:\")\n",
    "print(f\"  Shape: {bold_data.shape}\")\n",
    "print(f\"  Timepoints: {bold_data.shape[0]}\")\n",
    "print(f\"  Voxels: {bold_data.shape[1]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display information about the current step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align timepoints between BOLD and activations\n",
    "\n",
    "n_bold = bold_data.shape[0]\n",
    "n_acts = list(reduced_activations.values())[0].shape[0]\n",
    "\n",
    "print(f\"BOLD timepoints: {n_bold}\")\n",
    "print(f\"Activations timepoints: {n_acts}\")\n",
    "\n",
    "# Take minimum (align)\n",
    "n_time = min(n_bold, n_acts)\n",
    "\n",
    "bold_data = bold_data[:n_time]\n",
    "for layer in reduced_activations.keys():\n",
    "    reduced_activations[layer] = reduced_activations[layer][:n_time]\n",
    "\n",
    "print(f\"\\n\u2713 Aligned to {n_time} timepoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display information about the current step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test split (80/20)\n",
    "\n",
    "n_train = int(n_time * 0.8)\n",
    "train_idx = np.arange(n_train)\n",
    "test_idx = np.arange(n_train, n_time)\n",
    "\n",
    "print(f\"Train/test split:\")\n",
    "print(f\"  Train: {len(train_idx)} timepoints\")\n",
    "print(f\"  Test: {len(test_idx)} timepoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit ridge regression encoding models\n",
    "\n",
    "from encoding_utils import fit_encoding_model_per_layer\n",
    "\n",
    "alphas = [0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "\n",
    "print(\"Fitting ridge regression (5 layers \u00d7 voxels)...\")\n",
    "print(\"This takes ~3-5 minutes\\n\")\n",
    "\n",
    "encoding_results = fit_encoding_model_per_layer(\n",
    "    reduced_activations, \n",
    "    bold_data, \n",
    "    common_mask,\n",
    "    train_idx, \n",
    "    test_idx, \n",
    "    alphas=alphas\n",
    ")\n",
    "\n",
    "print(\"\\n\u2713 Encoding complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare layer performance\n",
    "\n",
    "from encoding_utils import compare_layer_performance\n",
    "from encoding_viz_utils import plot_layer_comparison_bars\n",
    "\n",
    "comparison_df = compare_layer_performance(encoding_results)\n",
    "\n",
    "print(\"Layer Performance:\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_layer = comparison_df.iloc[0]['layer']\n",
    "best_r2 = comparison_df.iloc[0]['mean_r2']\n",
    "\n",
    "print(f\"\\n\u2b50 Best: {best_layer.upper()} (R\u00b2 = {best_r2:.4f})\")\n",
    "\n",
    "# Visualize\n",
    "layer_order = ['conv1', 'conv2', 'conv3', 'conv4', 'linear']\n",
    "fig = plot_layer_comparison_bars(encoding_results, layer_order)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize R\u00b2 brain maps (best layer)\n",
    "\n",
    "from encoding_viz_utils import plot_r2_brainmap\n",
    "\n",
    "best_layer = comparison_df.iloc[0]['layer']\n",
    "best_r2_map = encoding_results[best_layer]['r2_map']\n",
    "\n",
    "print(f\"Best layer: {best_layer.upper()}\\n\")\n",
    "\n",
    "fig = plot_r2_brainmap(\n",
    "    best_r2_map, \n",
    "    best_layer,\n",
    "    threshold=0.01,\n",
    "    vmax=0.2\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udccd Interpretation:\")\n",
    "print(\"  Hot regions = Well predicted by this layer\")\n",
    "print(\"  - Early layers \u2192 Visual cortex\")\n",
    "print(\"  - Middle layers \u2192 Motor/parietal\")\n",
    "print(\"  - Late layers \u2192 Frontal/executive\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}