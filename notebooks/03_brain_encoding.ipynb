{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579b22aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Brain Encoding with RL Features\n",
    "\n",
    "## Predicting Brain Activity from Agent Representations\n",
    "\n",
    "**Overview:**\n",
    "This notebook uses the CNN activations from the RL agent (notebook 02) to predict brain activity during gameplay.\n",
    "\n",
    "**What we'll cover:**\n",
    "1. Understanding the encoding model framework\n",
    "2. Loading and preparing BOLD data\n",
    "3. Loading CNN activations from the agent\n",
    "4. Aligning timepoints between BOLD and activations\n",
    "5. Fitting ridge regression encoding models\n",
    "6. Comparing layer performance\n",
    "7. Visualizing brain maps\n",
    "\n",
    "**Key question:** Which layer of the agent best predicts brain activity, and where?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "setup"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’» Detected Local Environment.\n",
      "âœ… Ready. Working directory: /home/hyruuk/GitHub/neuromod/mario_analysis/mario.tutorials\n"
     ]
    }
   ],
   "source": [
    "# @title Environment Setup\n",
    "# @markdown Run this cell to set up the environment and download the necessary data.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "REPO_URL = \"https://github.com/courtois-neuromod/mario.tutorials.git\"\n",
    "PROJECT_PATH = Path(\"/content/mario.tutorials\")\n",
    "REQUIREMENTS_FILE = \"notebooks/03_requirements.txt\"\n",
    "SUBJECT = \"sub-01\"\n",
    "SESSION = \"ses-001\"\n",
    "TR = 1.49\n",
    "DOWNLOAD_STIMULI = True\n",
    "\n",
    "def run_shell(cmd):\n",
    "    print(f\"Running: {cmd}\")\n",
    "    subprocess.check_call(cmd, shell=True)\n",
    "\n",
    "# Detect Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"ðŸš€ Detected Google Colab. Setting up ephemeral environment...\")\n",
    "    \n",
    "    # 1. Clone Repository\n",
    "    if not PROJECT_PATH.exists():\n",
    "        run_shell(f\"git clone {REPO_URL} {PROJECT_PATH}\")\n",
    "    else:\n",
    "        run_shell(f\"cd {PROJECT_PATH} && git pull\")\n",
    "    \n",
    "    os.chdir(PROJECT_PATH)\n",
    "    sys.path.insert(0, str(PROJECT_PATH / \"src\"))\n",
    "    \n",
    "    # 2. Run Setup\n",
    "    from setup_utils import setup_project\n",
    "    setup_project(REQUIREMENTS_FILE, SUBJECT, SESSION, download_stimuli_flag=DOWNLOAD_STIMULI)\n",
    "\n",
    "else:\n",
    "    print(\"ðŸ’» Detected Local Environment.\")\n",
    "    if Path.cwd().name == 'notebooks':\n",
    "        os.chdir(Path.cwd().parent)\n",
    "    sys.path.insert(0, str(Path.cwd() / \"src\"))\n",
    "    print(f\"âœ… Ready. Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "setup"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup utils not found. Please ensure src is in path.\n"
     ]
    }
   ],
   "source": [
    "# Silent Setup\n",
    "try:\n",
    "    from setup_utils import setup_all\n",
    "    # Ensure data is available (silently checks)\n",
    "    setup_all(subject=\"sub-01\", session=\"ses-010\")\n",
    "except ImportError:\n",
    "    print(\"Setup utils not found. Please ensure src is in path.\")\n",
    "except Exception as e:\n",
    "    print(f\"Setup warning: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5aaccc2",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup - imports and configuration\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "src_dir = Path('..') / 'src'\n",
    "sys.path.insert(0, str(src_dir))\n",
    "\n",
    "# Import utilities\n",
    "from utils import (\n",
    "    get_sourcedata_path,\n",
    "    load_events,\n",
    "    get_session_runs,\n",
    "    get_bold_path,\n",
    "    load_bold\n",
    ")\n",
    "\n",
    "# Import RL utilities\n",
    "from rl_utils import (\n",
    "    create_simple_proxy_features,\n",
    "    convolve_with_hrf,\n",
    "    apply_pca\n",
    ")\n",
    "\n",
    "# Import RL visualizations\n",
    "from rl_viz_utils import (\n",
    "    plot_pca_variance_per_layer,\n",
    "    plot_layer_activations_sample\n",
    ")\n",
    "\n",
    "# Import encoding utilities\n",
    "from encoding_utils import (\n",
    "    load_and_prepare_bold,\n",
    "    fit_encoding_model_per_layer,\n",
    "    compare_layer_performance\n",
    ")\n",
    "\n",
    "# Import encoding visualizations\n",
    "from encoding_viz_utils import (\n",
    "    plot_layer_comparison_bars,\n",
    "    plot_r2_brainmap\n",
    ")\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Get sourcedata path\n",
    "sourcedata_path = get_sourcedata_path()\n",
    "\n",
    "print(\"âœ“ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0ce3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. The Encoding Model Framework\n",
    "\n",
    "**Goal:** Predict BOLD activity from RL agent features\n",
    "\n",
    "**Model:** Ridge Regression (linear regression with L2 regularization)\n",
    "\n",
    "```\n",
    "BOLD(voxel, time) = Î£ Î²áµ¢ Â· Feature_i(time) + Îµ\n",
    "```\n",
    "\n",
    "**Why ridge regression?**\n",
    "- Handles high-dimensional features (50 PCA components)\n",
    "- L2 penalty prevents overfitting: `||Î²||Â² â‰¤ Î±`\n",
    "- Cross-validation selects optimal regularization strength Î±\n",
    "- Fast to fit (~5 mins for whole brain)\n",
    "\n",
    "**Alternative approaches:**\n",
    "- Lasso (L1): Sparse feature selection\n",
    "- Elastic net: L1 + L2\n",
    "- Nonlinear: Kernel ridge, neural networks\n",
    "\n",
    "**For interpretability and speed, we use ridge regression.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9bf216",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 runs: ['run-1', 'run-2', 'run-3', 'run-4']\n",
      "  run-1: 712 events\n",
      "  run-2: 1032 events\n",
      "  run-3: 1037 events\n",
      "  run-4: 1030 events\n",
      "\n",
      "Loading BOLD data...\n",
      "\n",
      "Creating common brain mask...\n"
     ]
    }
   ],
   "source": [
    "# Load prerequisites\n",
    "\n",
    "from nilearn.masking import compute_multi_epi_mask\n",
    "\n",
    "# Get runs\n",
    "runs = get_session_runs(SUBJECT, SESSION, sourcedata_path)\n",
    "print(f\"Found {len(runs)} runs: {runs}\")\n",
    "\n",
    "# Load events\n",
    "all_events = []\n",
    "for run in runs:\n",
    "    events = load_events(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    all_events.append(events)\n",
    "    print(f\"  {run}: {len(events)} events\")\n",
    "\n",
    "# Load BOLD images and paths\n",
    "print(\"\\nLoading BOLD data...\")\n",
    "bold_imgs = []\n",
    "bold_paths = []\n",
    "for run in runs:\n",
    "    bold_path = get_bold_path(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    bold_img = load_bold(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    bold_paths.append(str(bold_path))  # Convert Path to string for nilearn\n",
    "    bold_imgs.append(bold_img)\n",
    "\n",
    "# Create common mask\n",
    "print(\"\\nCreating common brain mask...\")\n",
    "common_mask = compute_multi_epi_mask(bold_imgs, n_jobs=1)\n",
    "n_voxels = int((common_mask.get_fdata() > 0).sum())\n",
    "print(f\"âœ“ Common mask: {n_voxels:,} voxels\")\n",
    "\n",
    "print(\"\\nâœ“ All prerequisites loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e1530c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Loading Prerequisites\n",
    "\n",
    "We need:\n",
    "- Subject/session info (sub-01, ses-010)\n",
    "- Run IDs (4 runs)\n",
    "- BOLD images (preprocessed fMRI data)\n",
    "- Event files (for alignment)\n",
    "- Common brain mask (from GLM analysis)\n",
    "\n",
    "**Note:** If you haven't run notebook 01, this will create a fresh mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f2d50",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Found trained model: models/mario_ppo_agent.pth\n",
      "\n",
      "Loading model...\n",
      "âœ“ Model loaded\n",
      "\n",
      "======================================================================\n",
      "Aligning RL activations to BOLD for sub-01 ses-001\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Processing run-1:\n",
      "--------------------------------------------------\n",
      "  Found 11 game trial(s)\n",
      "  Using actual BOLD length: 451 TRs\n",
      "\n",
      "  Repetition 0: Level1-1\n",
      "    Onset: 0.01s, Duration: 86.55s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l1_rep-000.bk2\n",
      "    Level format: Level1-1 -> Level1-1\n",
      "  Processing 4419 frames...\n",
      "    Extracted 4419 frames â†’ downsampling to TR...\n",
      "    â†’ 59 TRs (indices 0-59)\n",
      "\n",
      "  Repetition 1: Level1-1\n",
      "    Onset: 86.56s, Duration: 71.11s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l1_rep-001.bk2\n",
      "    Level format: Level1-1 -> Level1-1\n",
      "  Processing 3906 frames...\n",
      "    Extracted 3906 frames â†’ downsampling to TR...\n",
      "    â†’ 48 TRs (indices 58-106)\n",
      "\n",
      "  Repetition 2: Level1-1\n",
      "    Onset: 157.68s, Duration: 43.15s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l1_rep-002.bk2\n",
      "    Level format: Level1-1 -> Level1-1\n",
      "  Processing 2228 frames...\n",
      "    Extracted 2228 frames â†’ downsampling to TR...\n",
      "    â†’ 29 TRs (indices 105-134)\n",
      "\n",
      "  Repetition 3: Level1-1\n",
      "    Onset: 200.82s, Duration: 46.65s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l1_rep-003.bk2\n",
      "    Level format: Level1-1 -> Level1-1\n",
      "  Processing 2438 frames...\n",
      "    Extracted 2438 frames â†’ downsampling to TR...\n",
      "    â†’ 32 TRs (indices 134-166)\n",
      "\n",
      "  Repetition 4: Level1-1\n",
      "    Onset: 247.47s, Duration: 57.80s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l1_rep-004.bk2\n",
      "    Level format: Level1-1 -> Level1-1\n",
      "  Processing 3107 frames...\n",
      "    Extracted 3107 frames â†’ downsampling to TR...\n",
      "    â†’ 39 TRs (indices 166-205)\n",
      "\n",
      "  Repetition 5: Level1-1\n",
      "    Onset: 305.27s, Duration: 49.80s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l1_rep-005.bk2\n",
      "    Level format: Level1-1 -> Level1-1\n",
      "  Processing 2627 frames...\n",
      "    Extracted 2627 frames â†’ downsampling to TR...\n",
      "    â†’ 34 TRs (indices 204-238)\n",
      "\n",
      "  Repetition 6: Level1-1\n",
      "    Onset: 355.07s, Duration: 64.87s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l1_rep-006.bk2\n",
      "    Level format: Level1-1 -> Level1-1\n",
      "  Processing 3531 frames...\n",
      "    Extracted 3531 frames â†’ downsampling to TR...\n",
      "    â†’ 44 TRs (indices 238-282)\n",
      "\n",
      "  Repetition 7: Level1-1\n",
      "    Onset: 419.94s, Duration: 46.30s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l1_rep-007.bk2\n",
      "    Level format: Level1-1 -> Level1-1\n",
      "  Processing 2417 frames...\n",
      "    Extracted 2417 frames â†’ downsampling to TR...\n",
      "    â†’ 32 TRs (indices 281-313)\n",
      "\n",
      "  Repetition 8: Level1-1\n",
      "    Onset: 466.24s, Duration: 61.15s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l1_rep-008.bk2\n",
      "    Level format: Level1-1 -> Level1-1\n",
      "  Processing 3308 frames...\n",
      "    Extracted 3308 frames â†’ downsampling to TR...\n",
      "    â†’ 42 TRs (indices 312-354)\n",
      "\n",
      "  Repetition 9: Level1-1\n",
      "    Onset: 527.39s, Duration: 47.85s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l1_rep-009.bk2\n",
      "    Level format: Level1-1 -> Level1-1\n",
      "  Processing 2510 frames...\n",
      "    Extracted 2510 frames â†’ downsampling to TR...\n",
      "    â†’ 33 TRs (indices 353-386)\n",
      "\n",
      "  Repetition 10: Level1-1\n",
      "    Onset: 575.23s, Duration: 42.47s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l1_rep-010.bk2\n",
      "    Level format: Level1-1 -> Level1-1\n",
      "  Processing 2543 frames...\n",
      "    Extracted 2543 frames â†’ downsampling to TR...\n",
      "    â†’ 29 TRs (indices 386-415)\n",
      "\n",
      "  âœ“ run-1: 415/451 TRs with gameplay\n",
      "\n",
      "Processing run-2:\n",
      "--------------------------------------------------\n",
      "  Found 10 game trial(s)\n",
      "  Using actual BOLD length: 445 TRs\n",
      "\n",
      "  Repetition 0: Level1-2\n",
      "    Onset: 0.01s, Duration: 58.26s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l2_rep-000.bk2\n",
      "    Level format: Level1-2 -> Level1-2\n",
      "  Processing 3135 frames...\n",
      "    Extracted 3135 frames â†’ downsampling to TR...\n",
      "    â†’ 40 TRs (indices 0-40)\n",
      "\n",
      "  Repetition 1: Level1-2\n",
      "    Onset: 58.27s, Duration: 57.53s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l2_rep-001.bk2\n",
      "    Level format: Level1-2 -> Level1-2\n",
      "  Processing 3091 frames...\n",
      "    Extracted 3091 frames â†’ downsampling to TR...\n",
      "    â†’ 39 TRs (indices 39-78)\n",
      "\n",
      "  Repetition 2: Level1-2\n",
      "    Onset: 115.81s, Duration: 87.05s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l2_rep-002.bk2\n",
      "    Level format: Level1-2 -> Level1-2\n",
      "  Processing 4862 frames...\n",
      "    Extracted 4862 frames â†’ downsampling to TR...\n",
      "    â†’ 59 TRs (indices 77-136)\n",
      "\n",
      "  Repetition 3: Level1-2\n",
      "    Onset: 202.85s, Duration: 66.15s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l2_rep-003.bk2\n",
      "    Level format: Level1-2 -> Level1-2\n",
      "  Processing 3608 frames...\n",
      "    Extracted 3608 frames â†’ downsampling to TR...\n",
      "    â†’ 45 TRs (indices 136-181)\n",
      "\n",
      "  Repetition 4: Level1-2\n",
      "    Onset: 269.00s, Duration: 76.51s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l2_rep-004.bk2\n",
      "    Level format: Level1-2 -> Level1-2\n",
      "  Processing 4230 frames...\n",
      "    Extracted 4230 frames â†’ downsampling to TR...\n",
      "    â†’ 52 TRs (indices 180-232)\n",
      "\n",
      "  Repetition 5: Level1-2\n",
      "    Onset: 345.52s, Duration: 57.67s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l2_rep-005.bk2\n",
      "    Level format: Level1-2 -> Level1-2\n",
      "  Processing 3099 frames...\n",
      "    Extracted 3099 frames â†’ downsampling to TR...\n",
      "    â†’ 39 TRs (indices 231-270)\n",
      "\n",
      "  Repetition 6: Level1-2\n",
      "    Onset: 403.18s, Duration: 56.23s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l2_rep-006.bk2\n",
      "    Level format: Level1-2 -> Level1-2\n",
      "  Processing 3013 frames...\n",
      "    Extracted 3013 frames â†’ downsampling to TR...\n",
      "    â†’ 38 TRs (indices 270-308)\n",
      "\n",
      "  Repetition 7: Level1-2\n",
      "    Onset: 459.42s, Duration: 37.43s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l2_rep-007.bk2\n",
      "    Level format: Level1-2 -> Level1-2\n",
      "  Processing 1884 frames...\n",
      "    Extracted 1884 frames â†’ downsampling to TR...\n",
      "    â†’ 26 TRs (indices 308-334)\n",
      "\n",
      "  Repetition 8: Level1-2\n",
      "    Onset: 496.85s, Duration: 88.11s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l2_rep-008.bk2\n",
      "    Level format: Level1-2 -> Level1-2\n",
      "  Processing 4925 frames...\n",
      "    Extracted 4925 frames â†’ downsampling to TR...\n",
      "    â†’ 60 TRs (indices 333-393)\n",
      "\n",
      "  Repetition 9: Level1-2\n",
      "    Onset: 584.96s, Duration: 44.70s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l2_rep-009.bk2\n",
      "    Level format: Level1-2 -> Level1-2\n",
      "  Processing 2677 frames...\n",
      "    Extracted 2677 frames â†’ downsampling to TR...\n",
      "    â†’ 30 TRs (indices 392-422)\n",
      "\n",
      "  âœ“ run-2: 422/445 TRs with gameplay\n",
      "\n",
      "Processing run-3:\n",
      "--------------------------------------------------\n",
      "  Found 12 game trial(s)\n",
      "  Using actual BOLD length: 455 TRs\n",
      "\n",
      "  Repetition 0: Level1-3\n",
      "    Onset: 0.00s, Duration: 52.80s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l3_rep-000.bk2\n",
      "    Level format: Level1-3 -> Level1-3\n",
      "  Processing 2807 frames...\n",
      "    Extracted 2807 frames â†’ downsampling to TR...\n",
      "    â†’ 36 TRs (indices 0-36)\n",
      "\n",
      "  Repetition 1: Level1-3\n",
      "    Onset: 52.80s, Duration: 48.17s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l3_rep-001.bk2\n",
      "    Level format: Level1-3 -> Level1-3\n",
      "  Processing 2529 frames...\n",
      "    Extracted 2529 frames â†’ downsampling to TR...\n",
      "    â†’ 33 TRs (indices 35-68)\n",
      "\n",
      "  Repetition 2: Level1-3\n",
      "    Onset: 100.97s, Duration: 64.88s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l3_rep-002.bk2\n",
      "    Level format: Level1-3 -> Level1-3\n",
      "  Processing 3532 frames...\n",
      "    Extracted 3532 frames â†’ downsampling to TR...\n",
      "    â†’ 44 TRs (indices 67-111)\n",
      "\n",
      "  Repetition 3: Level1-3\n",
      "    Onset: 165.85s, Duration: 69.66s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l3_rep-003.bk2\n",
      "    Level format: Level1-3 -> Level1-3\n",
      "  Processing 3819 frames...\n",
      "    Extracted 3819 frames â†’ downsampling to TR...\n",
      "    â†’ 47 TRs (indices 111-158)\n",
      "\n",
      "  Repetition 4: Level1-3\n",
      "    Onset: 235.52s, Duration: 57.70s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l3_rep-004.bk2\n",
      "    Level format: Level1-3 -> Level1-3\n",
      "  Processing 3101 frames...\n",
      "    Extracted 3101 frames â†’ downsampling to TR...\n",
      "    â†’ 39 TRs (indices 158-197)\n",
      "\n",
      "  Repetition 5: Level1-3\n",
      "    Onset: 293.22s, Duration: 64.27s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l3_rep-005.bk2\n",
      "    Level format: Level1-3 -> Level1-3\n",
      "  Processing 3494 frames...\n",
      "    Extracted 3494 frames â†’ downsampling to TR...\n",
      "    â†’ 44 TRs (indices 196-240)\n",
      "\n",
      "  Repetition 6: Level1-3\n",
      "    Onset: 357.48s, Duration: 56.32s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l3_rep-006.bk2\n",
      "    Level format: Level1-3 -> Level1-3\n",
      "  Processing 3018 frames...\n",
      "    Extracted 3018 frames â†’ downsampling to TR...\n",
      "    â†’ 38 TRs (indices 239-277)\n",
      "\n",
      "  Repetition 7: Level1-3\n",
      "    Onset: 413.80s, Duration: 50.86s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l3_rep-007.bk2\n",
      "    Level format: Level1-3 -> Level1-3\n",
      "  Processing 2691 frames...\n",
      "    Extracted 2691 frames â†’ downsampling to TR...\n",
      "    â†’ 35 TRs (indices 277-312)\n",
      "\n",
      "  Repetition 8: Level1-3\n",
      "    Onset: 464.66s, Duration: 49.32s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l3_rep-008.bk2\n",
      "    Level format: Level1-3 -> Level1-3\n",
      "  Processing 2598 frames...\n",
      "    Extracted 2598 frames â†’ downsampling to TR...\n",
      "    â†’ 34 TRs (indices 311-345)\n",
      "\n",
      "  Repetition 9: Level1-3\n",
      "    Onset: 513.98s, Duration: 39.82s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l3_rep-009.bk2\n",
      "    Level format: Level1-3 -> Level1-3\n",
      "  Processing 2028 frames...\n",
      "    Extracted 2028 frames â†’ downsampling to TR...\n",
      "    â†’ 27 TRs (indices 344-371)\n",
      "\n",
      "  Repetition 10: Level1-3\n",
      "    Onset: 553.80s, Duration: 49.83s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l3_rep-010.bk2\n",
      "    Level format: Level1-3 -> Level1-3\n",
      "  Processing 2629 frames...\n",
      "    Extracted 2629 frames â†’ downsampling to TR...\n",
      "    â†’ 34 TRs (indices 371-405)\n",
      "\n",
      "  Repetition 11: Level1-3\n",
      "    Onset: 603.63s, Duration: 44.95s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w1l3_rep-011.bk2\n",
      "    Level format: Level1-3 -> Level1-3\n",
      "  Processing 2691 frames...\n",
      "    Extracted 2691 frames â†’ downsampling to TR...\n",
      "    â†’ 31 TRs (indices 405-436)\n",
      "\n",
      "  âœ“ run-3: 436/455 TRs with gameplay\n",
      "\n",
      "Processing run-4:\n",
      "--------------------------------------------------\n",
      "  Found 9 game trial(s)\n",
      "  Using actual BOLD length: 443 TRs\n",
      "\n",
      "  Repetition 0: Level2-1\n",
      "    Onset: 0.02s, Duration: 61.93s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w2l1_rep-000.bk2\n",
      "    Level format: Level2-1 -> Level2-1\n",
      "  Processing 3356 frames...\n",
      "    Extracted 3356 frames â†’ downsampling to TR...\n",
      "    â†’ 42 TRs (indices 0-42)\n",
      "\n",
      "  Repetition 1: Level2-1\n",
      "    Onset: 61.95s, Duration: 74.45s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w2l1_rep-001.bk2\n",
      "    Level format: Level2-1 -> Level2-1\n",
      "  Processing 4106 frames...\n",
      "    Extracted 4106 frames â†’ downsampling to TR...\n",
      "    â†’ 50 TRs (indices 41-91)\n",
      "\n",
      "  Repetition 2: Level2-1\n",
      "    Onset: 136.40s, Duration: 63.95s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w2l1_rep-002.bk2\n",
      "    Level format: Level2-1 -> Level2-1\n",
      "  Processing 3476 frames...\n",
      "    Extracted 3476 frames â†’ downsampling to TR...\n",
      "    â†’ 43 TRs (indices 91-134)\n",
      "\n",
      "  Repetition 3: Level2-1\n",
      "    Onset: 200.34s, Duration: 54.85s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w2l1_rep-003.bk2\n",
      "    Level format: Level2-1 -> Level2-1\n",
      "  Processing 2930 frames...\n",
      "    Extracted 2930 frames â†’ downsampling to TR...\n",
      "    â†’ 37 TRs (indices 134-171)\n",
      "\n",
      "  Repetition 4: Level2-1\n",
      "    Onset: 255.20s, Duration: 53.45s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w2l1_rep-004.bk2\n",
      "    Level format: Level2-1 -> Level2-1\n",
      "  Processing 2846 frames...\n",
      "    Extracted 2846 frames â†’ downsampling to TR...\n",
      "    â†’ 36 TRs (indices 171-207)\n",
      "\n",
      "  Repetition 5: Level2-1\n",
      "    Onset: 308.64s, Duration: 101.08s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w2l1_rep-005.bk2\n",
      "    Level format: Level2-1 -> Level2-1\n",
      "  Processing 5703 frames...\n",
      "    Extracted 5703 frames â†’ downsampling to TR...\n",
      "    â†’ 68 TRs (indices 207-275)\n",
      "\n",
      "  Repetition 6: Level2-1\n",
      "    Onset: 409.72s, Duration: 64.98s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w2l1_rep-006.bk2\n",
      "    Level format: Level2-1 -> Level2-1\n",
      "  Processing 3538 frames...\n",
      "    Extracted 3538 frames â†’ downsampling to TR...\n",
      "    â†’ 44 TRs (indices 274-318)\n",
      "\n",
      "  Repetition 7: Level2-1\n",
      "    Onset: 474.71s, Duration: 52.41s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w2l1_rep-007.bk2\n",
      "    Level format: Level2-1 -> Level2-1\n",
      "  Processing 2783 frames...\n",
      "    Extracted 2783 frames â†’ downsampling to TR...\n",
      "    â†’ 36 TRs (indices 318-354)\n",
      "\n",
      "  Repetition 8: Level2-1\n",
      "    Onset: 527.12s, Duration: 97.57s\n",
      "  Loading replay: sub-01_ses-001_task-mario_level-w2l1_rep-008.bk2\n",
      "    Level format: Level2-1 -> Level2-1\n",
      "  Processing 5849 frames...\n",
      "    Extracted 5849 frames â†’ downsampling to TR...\n",
      "    â†’ 66 TRs (indices 353-419)\n",
      "\n",
      "  âœ“ run-4: 419/443 TRs with gameplay\n",
      "\n",
      "======================================================================\n",
      "Concatenating runs...\n",
      "  Total TRs: 1794\n",
      "  Valid TRs (gameplay): 1692 (94.3%)\n",
      "\n",
      "Applying HRF convolution...\n",
      "  âœ“ HRF convolution complete\n",
      "\n",
      "======================================================================\n",
      "âœ“ Alignment complete!\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Alignment summary:\n",
      "  run-1: 415/451 TRs (11 game segments)\n",
      "  run-2: 422/445 TRs (10 game segments)\n",
      "  run-3: 436/455 TRs (12 game segments)\n",
      "  run-4: 419/443 TRs (9 game segments)\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and align activations from replays\n",
    "\n",
    "# First, check if we have a trained model\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_DIR = Path('models/')\n",
    "MODEL_PATH = MODEL_DIR / 'mario_ppo_agent.pth'\n",
    "\n",
    "if not MODEL_PATH.exists():\n",
    "    print(f\"âœ— No trained model found at: {MODEL_PATH}\")\n",
    "    print(\"\\nYou need a trained RL agent to extract activations.\")\n",
    "    print(\"Please train an agent first by running:\")\n",
    "    print(\"  python ../train_mario_agent.py --steps 5000000\")\n",
    "    print(\"\\nâš  Cannot proceed with encoding analysis without trained model\")\n",
    "    HAS_MODEL = False\n",
    "else:\n",
    "    print(f\"âœ“ Found trained model: {MODEL_PATH}\")\n",
    "    HAS_MODEL = True\n",
    "    \n",
    "    # Load the model\n",
    "    from rl_utils import load_pretrained_model, align_activations_to_bold\n",
    "    \n",
    "    print(\"\\nLoading model...\")\n",
    "    model = load_pretrained_model(MODEL_PATH, device='cpu')\n",
    "    print(\"âœ“ Model loaded\")\n",
    "    \n",
    "    # Align activations to BOLD\n",
    "    # This will:\n",
    "    # 1. Load replay files for each game segment\n",
    "    # 2. Extract RL activations at 60Hz  \n",
    "    # 3. Downsample to TR (1.49s)\n",
    "    # 4. Apply HRF convolution\n",
    "    # 5. Create NaN mask for non-gameplay periods\n",
    "    \n",
    "    alignment_results = align_activations_to_bold(\n",
    "        model=model,\n",
    "        subject=SUBJECT,\n",
    "        session=SESSION,\n",
    "        runs=runs,\n",
    "        sourcedata_path=sourcedata_path,\n",
    "        tr=TR,\n",
    "        device='cpu',\n",
    "        apply_hrf=True,  # Apply HRF convolution\n",
    "        bold_imgs=bold_imgs  # Pass BOLD images for exact TR count\n",
    "    )\n",
    "    \n",
    "    # Extract results\n",
    "    layer_activations = alignment_results['activations']\n",
    "    valid_mask = alignment_results['mask']\n",
    "    run_info = alignment_results['run_info']\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Alignment summary:\")\n",
    "    for info in run_info:\n",
    "        print(f\"  {info['run']}: {info['n_valid_trs']}/{info['n_trs']} TRs \"\n",
    "              f\"({info['n_segments']} game segments)\")\n",
    "    print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535d9cdd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Loading and Aligning RL Activations\n",
    "\n",
    "**NEW APPROACH:**\n",
    "\n",
    "Instead of using pre-extracted activations, we now:\n",
    "\n",
    "1. **Load replay files** from the human subject's actual gameplay\n",
    "   - Uses `.bk2` replay files from `sourcedata/mario/`\n",
    "   - Matches exact stimuli presented during fMRI scanning\n",
    "\n",
    "2. **Extract activations frame-by-frame** (60Hz)\n",
    "   - Pass replay frames through trained RL agent\n",
    "   - Collect CNN activations from all layers\n",
    "\n",
    "3. **Align to fMRI timing**\n",
    "   - Use `mario.annotations` files to get game segment timing\n",
    "   - Downsample from 60Hz to TR (1.49s)\n",
    "   - Apply HRF convolution\n",
    "\n",
    "4. **Handle multiple games per run**\n",
    "   - Concatenate gameplay segments\n",
    "   - Mask inter-game periods with NaN\n",
    "\n",
    "**This ensures perfect alignment between RL activations and BOLD data!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5490c7fd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning BOLD data...\n",
      "This performs:\n",
      "  1. Confound regression (motion, WM, CSF) - NO global signal\n",
      "  2. Detrending (remove linear drift)\n",
      "  3. Standardization (z-score each voxel)\n",
      "\n",
      "Note: High-pass filtering is handled by fMRIPrep confounds\n",
      "Note: Global signal regression removed (was too aggressive)\n",
      "\n",
      "âœ“ BOLD prepared:\n",
      "  Shape: (1794, 213371)\n",
      "  Timepoints: 1794\n",
      "  Voxels: 213,371\n"
     ]
    }
   ],
   "source": [
    "# Clean and prepare BOLD data\n",
    "\n",
    "from encoding_utils import load_and_prepare_bold\n",
    "\n",
    "print(\"Cleaning BOLD data...\")\n",
    "print(\"This performs:\")\n",
    "print(\"  1. Confound regression (motion, WM, CSF) - NO global signal\")\n",
    "print(\"  2. Detrending (remove linear drift)\")\n",
    "print(\"  3. Standardization (z-score each voxel)\")\n",
    "print(\"\\nNote: High-pass filtering is handled by fMRIPrep confounds\")\n",
    "print(\"Note: Global signal regression removed (was too aggressive)\\n\")\n",
    "\n",
    "bold_data = load_and_prepare_bold(\n",
    "    bold_paths,  # Use paths instead of images for confound loading\n",
    "    mask_img=common_mask,\n",
    "    detrend=True,\n",
    "    standardize=True,\n",
    "    t_r=TR,\n",
    "    load_confounds_from_fmriprep=True  # Automatically load confounds from fMRIPrep\n",
    ")\n",
    "\n",
    "print(f\"âœ“ BOLD prepared:\")\n",
    "print(f\"  Shape: {bold_data.shape}\")\n",
    "print(f\"  Timepoints: {bold_data.shape[0]}\")\n",
    "print(f\"  Voxels: {bold_data.shape[1]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec498b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Cleaning and Preparing BOLD Data\n",
    "\n",
    "**Preprocessing steps:**\n",
    "\n",
    "1. **Confound regression:** Remove nuisance signals from each voxel's timeseries\n",
    "   - Motion parameters (6 DOF: translation + rotation)\n",
    "   - White matter signal (non-neural tissue)\n",
    "   - CSF signal (physiological pulsations)\n",
    "   - Global signal (whole-brain average)\n",
    "   - High-pass filter components (from fMRIPrep, removes slow drifts <1/128 Hz)\n",
    "\n",
    "2. **Detrending:** Remove linear drift within each run\n",
    "\n",
    "3. **Standardization:** Z-score each voxel (mean=0, std=1)\n",
    "\n",
    "**What is confound regression?**\n",
    "\n",
    "Think of it as \"noise cancellation\" for fMRI:\n",
    "- BOLD signal = neural activity + artifacts (motion, heartbeat, breathing, scanner drift)\n",
    "- For each voxel, we fit a linear model: `BOLD = Î²â‚Â·motion + Î²â‚‚Â·WM + Î²â‚ƒÂ·CSF + ... + Îµ`\n",
    "- We keep only the residual (Îµ) = signal unexplained by confounds\n",
    "- This \"cleaned\" signal better reflects neural activity\n",
    "\n",
    "**Why is this important?**\n",
    "- Head motion creates spurious correlations between brain regions\n",
    "- Without cleaning, you might \"predict\" brain activity that's actually just head movement\n",
    "- Confound regression removes these artifacts while preserving neural signals\n",
    "\n",
    "**Output:** `(timepoints Ã— voxels)` matrix ready for regression, with artifacts removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485af949",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOLD timepoints: 1794\n",
      "Activations timepoints: 1794\n",
      "Valid (gameplay) timepoints: 1692\n",
      "Invalid (non-gameplay) timepoints: 102\n",
      "\n",
      "âœ“ Dimensions match!\n"
     ]
    }
   ],
   "source": [
    "# Check alignment between BOLD and activations\n",
    "\n",
    "if HAS_MODEL:\n",
    "    n_bold = bold_data.shape[0]\n",
    "    n_acts = list(layer_activations.values())[0].shape[0]\n",
    "    \n",
    "    print(f\"BOLD timepoints: {n_bold}\")\n",
    "    print(f\"Activations timepoints: {n_acts}\")\n",
    "    print(f\"Valid (gameplay) timepoints: {valid_mask.sum()}\")\n",
    "    print(f\"Invalid (non-gameplay) timepoints: {(~valid_mask).sum()}\")\n",
    "    \n",
    "    # Ensure dimensions match\n",
    "    if n_bold != n_acts:\n",
    "        print(f\"\\nâš  Dimension mismatch!\")\n",
    "        print(f\"  Truncating to minimum length: {min(n_bold, n_acts)}\")\n",
    "        n_time = min(n_bold, n_acts)\n",
    "        bold_data = bold_data[:n_time]\n",
    "        valid_mask = valid_mask[:n_time]\n",
    "        for layer in layer_activations.keys():\n",
    "            layer_activations[layer] = layer_activations[layer][:n_time]\n",
    "    else:\n",
    "        print(\"\\nâœ“ Dimensions match!\")\n",
    "else:\n",
    "    print(\"âš  No model available, skipping alignment check\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68a2b66",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Alignment Status\n",
    "\n",
    "**Automatic alignment completed!**\n",
    "\n",
    "The `align_activations_to_bold()` function has:\n",
    "\n",
    "1. âœ… **Loaded replay files** for each game segment\n",
    "2. âœ… **Extracted RL activations** at 60Hz from replay frames\n",
    "3. âœ… **Downsampled to TR** using temporal averaging within each TR window\n",
    "4. âœ… **Applied HRF convolution** to account for hemodynamic lag\n",
    "5. âœ… **Created validity mask** to mark gameplay vs non-gameplay periods\n",
    "\n",
    "**Key differences from old approach:**\n",
    "- OLD: Arbitrary agent gameplay, misaligned\n",
    "- NEW: Exact subject gameplay from replays, perfectly aligned\n",
    "\n",
    "**Dimensions should now match:**\n",
    "- BOLD: Number of TRs across all runs\n",
    "- Activations: Same number of TRs (with NaN for non-gameplay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f09db7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up run-based cross-validation...\n",
      "\n",
      "IMPORTANT: For proper generalization, we should use leave-one-run-out CV.\n",
      "This ensures the model is tested on completely unseen runs.\n",
      "\n",
      "Run boundaries (in concatenated array):\n",
      "  run-1: TRs 0-451 (451 TRs, 415 valid)\n",
      "  run-2: TRs 451-896 (445 TRs, 422 valid)\n",
      "  run-3: TRs 896-1351 (455 TRs, 436 valid)\n",
      "  run-4: TRs 1351-1794 (443 TRs, 419 valid)\n",
      "\n",
      "Run-based split:\n",
      "  Train runs: ['run-1', 'run-2', 'run-3']\n",
      "  Test run: run-4\n",
      "  Train TRs (gameplay only): 1273\n",
      "  Test TRs (gameplay only): 419\n",
      "\n",
      "âš  Note: For a full analysis, implement leave-one-run-out CV and average results!\n"
     ]
    }
   ],
   "source": [
    "# Create run-based train/test splits\n",
    "\n",
    "if HAS_MODEL:\n",
    "    print(\"Setting up run-based cross-validation...\")\n",
    "    print(\"\\nIMPORTANT: For proper generalization, we should use leave-one-run-out CV.\")\n",
    "    print(\"This ensures the model is tested on completely unseen runs.\\n\")\n",
    "    \n",
    "    # Calculate run boundaries in concatenated data\n",
    "    run_boundaries = [0]\n",
    "    for info in run_info:\n",
    "        run_boundaries.append(run_boundaries[-1] + info['n_trs'])\n",
    "    \n",
    "    print(\"Run boundaries (in concatenated array):\")\n",
    "    for i, (run, info) in enumerate(zip(runs, run_info)):\n",
    "        start_idx = run_boundaries[i]\n",
    "        end_idx = run_boundaries[i+1]\n",
    "        print(f\"  {run}: TRs {start_idx}-{end_idx} ({info['n_trs']} TRs, {info['n_valid_trs']} valid)\")\n",
    "    \n",
    "    # For simplicity in this tutorial, we'll use first 3 runs for training, last run for testing\n",
    "    # In a real analysis, you should do full leave-one-run-out cross-validation\n",
    "    test_run_idx = 3  # Use last run as test set\n",
    "    \n",
    "    # Get train indices (first 3 runs) and test indices (last run)\n",
    "    train_start = run_boundaries[0]\n",
    "    train_end = run_boundaries[test_run_idx]\n",
    "    test_start = run_boundaries[test_run_idx]\n",
    "    test_end = run_boundaries[test_run_idx + 1]\n",
    "    \n",
    "    # Get valid (gameplay) indices within train and test sets\n",
    "    all_indices = np.arange(len(valid_mask))\n",
    "    train_all_indices = all_indices[train_start:train_end]\n",
    "    test_all_indices = all_indices[test_start:test_end]\n",
    "    \n",
    "    # Filter to only valid (gameplay) TRs\n",
    "    train_valid_indices = train_all_indices[valid_mask[train_start:train_end]]\n",
    "    test_valid_indices = test_all_indices[valid_mask[test_start:test_end]]\n",
    "    \n",
    "    print(f\"\\nRun-based split:\")\n",
    "    print(f\"  Train runs: {runs[:test_run_idx]}\")\n",
    "    print(f\"  Test run: {runs[test_run_idx]}\")\n",
    "    print(f\"  Train TRs (gameplay only): {len(train_valid_indices)}\")\n",
    "    print(f\"  Test TRs (gameplay only): {len(test_valid_indices)}\")\n",
    "    \n",
    "    print(\"\\nâš  Note: For a full analysis, implement leave-one-run-out CV and average results!\")\n",
    "else:\n",
    "    print(\"âš  No model available, skipping train/test split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78103a6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6. Run-Based Train/Test Split\n",
    "\n",
    "**Critical methodological point:** We must use **run-based cross-validation**, not random splitting!\n",
    "\n",
    "**Why run-based?**\n",
    "- **Temporal autocorrelation**: Adjacent TRs are correlated (hemodynamic response spans ~15-20 seconds)\n",
    "- **Random split**: Train and test would contain adjacent TRs from the same run â†’ inflated performance\n",
    "- **Run-based split**: Test set is from completely unseen runs â†’ true generalization\n",
    "\n",
    "**Leave-One-Run-Out (LORO) Cross-Validation:**\n",
    "- Train on N-1 runs, test on 1 held-out run\n",
    "- Repeat for each run as test set\n",
    "- Average results across folds\n",
    "- This is the gold standard for fMRI encoding models\n",
    "\n",
    "**Simplified approach (this notebook):**\n",
    "- Train: Runs 1-3\n",
    "- Test: Run 4\n",
    "- For a real analysis, implement full LORO and average across all folds\n",
    "\n",
    "**Only use gameplay TRs:**\n",
    "- Both train and test only include TRs where the subject was actually playing\n",
    "- Non-gameplay periods (between games) are excluded using the valid_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07912745",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Random Projection with multiple dimensions...\n",
      "(Random projection is fit only on valid gameplay TRs)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Testing n_components = 1000\n",
      "======================================================================\n",
      "\n",
      "Applying Random Projection to layer activations (n_components=1000)...\n",
      "\n",
      "  conv1:\n",
      "    Original features: 56448\n",
      "    Random projection: 56448 â†’ 1000 components\n",
      "\n",
      "  conv2:\n",
      "    Original features: 14112\n",
      "    Random projection: 14112 â†’ 1000 components\n",
      "\n",
      "  conv3:\n",
      "    Original features: 3872\n",
      "    Random projection: 3872 â†’ 1000 components\n",
      "\n",
      "  conv4:\n",
      "    Original features: 1152\n",
      "    Random projection: 1152 â†’ 1000 components\n",
      "\n",
      "  linear:\n",
      "    Original features: 512\n",
      "    Random projection: 512 â†’ 512 components\n",
      "\n",
      "âœ“ Random projection complete\n",
      "\n",
      "Random projection summary (n=1000):\n",
      "  conv1: 1000 components\n",
      "  conv2: 1000 components\n",
      "  conv3: 1000 components\n",
      "  conv4: 1000 components\n",
      "  linear: 512 components\n",
      "\n",
      "======================================================================\n",
      "All random projection dimensions tested!\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply Random Projection to layer activations (testing multiple dimensions)\n",
    "\n",
    "if HAS_MODEL:\n",
    "    from rl_utils import apply_random_projection_with_nan_handling\n",
    "    \n",
    "    # Test multiple projection dimensions\n",
    "    projection_dims = [10, 50, 100, 1000]\n",
    "    \n",
    "    print(\"Testing Random Projection with multiple dimensions...\")\n",
    "    print(\"(Random projection is fit only on valid gameplay TRs)\\n\")\n",
    "    \n",
    "    all_projection_results = {}\n",
    "    \n",
    "    for n_comp in projection_dims:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Testing n_components = {n_comp}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        projection_results = apply_random_projection_with_nan_handling(\n",
    "            layer_activations,\n",
    "            valid_mask,\n",
    "            n_components=n_comp,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        all_projection_results[n_comp] = projection_results\n",
    "        \n",
    "        print(f\"\\nRandom projection summary (n={n_comp}):\")\n",
    "        for layer, acts in projection_results['reduced_activations'].items():\n",
    "            print(f\"  {layer}: {acts.shape[1]} components\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"All random projection dimensions tested!\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "else:\n",
    "    print(\"âš  No model available, skipping random projection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394c681a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 7. Fitting Ridge Regression Encoding Models with Random Projections\n",
    "\n",
    "**NEW APPROACH: Random Projection instead of PCA**\n",
    "\n",
    "**Why Random Projection?**\n",
    "- Computationally efficient (no eigendecomposition)\n",
    "- Preserves distances approximately (Johnson-Lindenstrauss lemma)\n",
    "- Works well for high-dimensional data\n",
    "- No need to fit on training data (just random matrix)\n",
    "\n",
    "**Testing multiple dimensions:**\n",
    "- 10 components: Very low dimensional\n",
    "- 50 components: Similar to original PCA\n",
    "- 100 components: Medium dimensional\n",
    "- 1000 components: High dimensional\n",
    "\n",
    "**For each dimension and layer:**\n",
    "1. Use random-projected activations\n",
    "2. Cross-validate to find optimal Î± (regularization strength)\n",
    "3. Fit ridge regression on training data (gameplay TRs only)\n",
    "4. Predict BOLD on test data\n",
    "5. Compute RÂ² per voxel\n",
    "\n",
    "**Hyperparameter search:** Î± âˆˆ [0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "\n",
    "**Question:** Does the number of projection dimensions affect encoding performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0kcwmp9zb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Fitting encoding models for n_components = 1000\n",
      "======================================================================\n",
      "\n",
      "Fitting ridge regression (5 layers Ã— 1000 components Ã— voxels)...\n",
      "This may take a few minutes...\n",
      "\n",
      "Fitting encoding model for layer: conv1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best alpha: 100000.0\n",
      "  Mean RÂ² (train): 0.0160\n",
      "  Mean RÂ² (test): 0.0001\n",
      "\n",
      "Fitting encoding model for layer: conv2\n",
      "  Best alpha: 100000.0\n",
      "  Mean RÂ² (train): 0.0163\n",
      "  Mean RÂ² (test): 0.0001\n",
      "\n",
      "Fitting encoding model for layer: conv3\n",
      "  Best alpha: 100000.0\n",
      "  Mean RÂ² (train): 0.0170\n",
      "  Mean RÂ² (test): 0.0003\n",
      "\n",
      "Fitting encoding model for layer: conv4\n",
      "  Best alpha: 100000.0\n",
      "  Mean RÂ² (train): 0.0169\n",
      "  Mean RÂ² (test): 0.0003\n",
      "\n",
      "Fitting encoding model for layer: linear\n",
      "  Best alpha: 100000.0\n",
      "  Mean RÂ² (train): 0.0089\n",
      "  Mean RÂ² (test): 0.0004\n",
      "\n",
      "\n",
      "âœ“ Encoding complete for n_components = 1000!\n",
      "\n",
      "======================================================================\n",
      "All encoding models fitted!\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit ridge regression encoding models for each projection dimension\n",
    "\n",
    "if HAS_MODEL:\n",
    "    from encoding_utils import fit_encoding_model_per_layer\n",
    "    \n",
    "    alphas = [0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "    \n",
    "    # Store results for each dimension\n",
    "    all_encoding_results = {}\n",
    "    \n",
    "    for n_comp in projection_dims:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Fitting encoding models for n_components = {n_comp}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        reduced_activations = all_projection_results[n_comp]['reduced_activations']\n",
    "        \n",
    "        print(f\"Fitting ridge regression (5 layers Ã— {n_comp} components Ã— voxels)...\")\n",
    "        print(\"This may take a few minutes...\\n\")\n",
    "        \n",
    "        encoding_results = fit_encoding_model_per_layer(\n",
    "            reduced_activations,\n",
    "            bold_data,\n",
    "            common_mask,\n",
    "            train_valid_indices,\n",
    "            test_valid_indices,\n",
    "            alphas=alphas,\n",
    "            valid_mask=valid_mask\n",
    "        )\n",
    "        \n",
    "        all_encoding_results[n_comp] = encoding_results\n",
    "        print(f\"\\nâœ“ Encoding complete for n_components = {n_comp}!\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"All encoding models fitted!\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "else:\n",
    "    print(\"âš  No model available, skipping encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93b1c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Check shapes at every step\n",
    "  print(\"Shape diagnostics:\")\n",
    "  print(f\"valid_mask length: {len(valid_mask)}\")\n",
    "  print(f\"valid_mask.sum(): {valid_mask.sum()}\")\n",
    "  print(f\"bold_data shape: {bold_data.shape}\")\n",
    "  print(f\"projected_acts shape: {projected_acts.shape}\")\n",
    "  print(f\"train_valid_indices length: {len(train_valid_indices)}\")\n",
    "  print(f\"test_valid_indices length: {len(test_valid_indices)}\")\n",
    "  print(f\"train + test: {len(train_valid_indices) + len(test_valid_indices)}\")\n",
    "\n",
    "  print(\"\\nFiltered shapes:\")\n",
    "  print(f\"projected_acts[valid_mask] shape: {projected_acts[valid_mask].shape}\")\n",
    "  print(f\"bold_data[valid_mask] shape: {bold_data[valid_mask].shape}\")\n",
    "\n",
    "  print(\"\\nIndexing into projected_acts:\")\n",
    "  print(f\"projected_acts[train_valid_indices] shape: {projected_acts[train_valid_indices].shape}\")\n",
    "  print(f\"Has NaN: {np.isnan(projected_acts[train_valid_indices]).any()}\")\n",
    "\n",
    "  print(\"\\nIndexing into bold_data:\")\n",
    "  print(f\"bold_data[train_valid_indices] shape: {bold_data[train_valid_indices].shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dbb1ff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Compare performance across projection dimensions and layers\n",
    "\n",
    "if HAS_MODEL:\n",
    "    from encoding_utils import compare_layer_performance\n",
    "    from encoding_viz_utils import plot_layer_comparison_bars\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"COMPARISON: Performance across different random projection dimensions\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Store all comparisons\n",
    "    all_comparisons = {}\n",
    "    \n",
    "    # Get total number of voxels for percentage calculation\n",
    "    n_total_voxels = bold_data.shape[1]\n",
    "    \n",
    "    for n_comp in projection_dims:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"n_components = {n_comp}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        comparison_df = compare_layer_performance(all_encoding_results[n_comp])\n",
    "        all_comparisons[n_comp] = comparison_df\n",
    "        \n",
    "        print(comparison_df.to_string(index=False))\n",
    "        \n",
    "        best_layer = comparison_df.iloc[0]['layer']\n",
    "        best_r2 = comparison_df.iloc[0]['mean_r2']\n",
    "        print(f\"\\nâ­ Best: {best_layer.upper()} (RÂ² = {best_r2:.4f})\")\n",
    "    \n",
    "    # Create comparison plot across dimensions\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    layer_order = ['conv1', 'conv2', 'conv3', 'conv4', 'linear']\n",
    "    \n",
    "    for idx, n_comp in enumerate(projection_dims):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Plot for this dimension\n",
    "        encoding_results = all_encoding_results[n_comp]\n",
    "        \n",
    "        # Extract mean RÂ² for each layer\n",
    "        layer_r2 = []\n",
    "        for layer in layer_order:\n",
    "            r2_map = encoding_results[layer]['r2_map']\n",
    "            # Extract data from NIfTI image if needed\n",
    "            if hasattr(r2_map, 'get_fdata'):\n",
    "                r2_data = r2_map.get_fdata().flatten()\n",
    "            else:\n",
    "                r2_data = r2_map.flatten() if hasattr(r2_map, 'flatten') else r2_map\n",
    "            \n",
    "            mean_r2 = np.mean(r2_data[r2_data > 0])  # Mean of positive RÂ²\n",
    "            layer_r2.append(mean_r2)\n",
    "        \n",
    "        # Bar plot\n",
    "        bars = ax.bar(range(len(layer_order)), layer_r2, color='steelblue', alpha=0.8)\n",
    "        ax.set_xticks(range(len(layer_order)))\n",
    "        ax.set_xticklabels(layer_order, rotation=45)\n",
    "        ax.set_ylabel('Mean RÂ² (positive voxels)')\n",
    "        ax.set_title(f'n_components = {n_comp}', fontsize=14, fontweight='bold')\n",
    "        ax.set_ylim([0, max(0.1, max(layer_r2) * 1.2)])\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Highlight best layer\n",
    "        best_idx = np.argmax(layer_r2)\n",
    "        bars[best_idx].set_color('darkorange')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary table: best RÂ² for each dimension\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SUMMARY: Best performance across dimensions\")\n",
    "    print(f\"{'='*80}\")\n",
    "    summary_data = []\n",
    "    for n_comp in projection_dims:\n",
    "        comparison_df = all_comparisons[n_comp]\n",
    "        best_row = comparison_df.iloc[0]\n",
    "        summary_data.append({\n",
    "            'n_components': n_comp,\n",
    "            'best_layer': best_row['layer'],\n",
    "            'mean_r2': best_row['mean_r2'],\n",
    "            'median_r2': best_row['median_r2'],\n",
    "            'pct_positive': (best_row['n_positive_voxels'] / n_total_voxels) * 100\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš  No model available, skipping layer comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e8bac1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 8. Comparing Performance Across Dimensions\n",
    "\n",
    "**Key Questions:**\n",
    "\n",
    "1. **Does dimensionality matter?** \n",
    "   - Do more components always lead to better performance?\n",
    "   - Is there a sweet spot, or does performance plateau?\n",
    "\n",
    "2. **Which layer is best?**\n",
    "   - Does the best layer change with dimensionality?\n",
    "   - Are results consistent across projection dimensions?\n",
    "\n",
    "3. **Overfitting vs Underfitting:**\n",
    "   - Too few components (10): May lose important information\n",
    "   - Too many components (1000): May introduce noise, harder to regularize\n",
    "   - Medium (50-100): Potentially optimal balance\n",
    "\n",
    "**Expected patterns:**\n",
    "- Performance should increase from 10 â†’ 50 â†’ 100 components\n",
    "- Beyond 100-1000, performance may plateau or decrease (overfitting)\n",
    "- Ridge regularization should help prevent overfitting with high dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b0c840",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize RÂ² brain maps for best performing dimension\n",
    "\n",
    "if HAS_MODEL:\n",
    "    from encoding_viz_utils import plot_r2_brainmap\n",
    "    \n",
    "    # Find best overall dimension\n",
    "    best_n_comp = None\n",
    "    best_overall_r2 = -np.inf\n",
    "    \n",
    "    for n_comp in projection_dims:\n",
    "        comparison_df = all_comparisons[n_comp]\n",
    "        top_r2 = comparison_df.iloc[0]['mean_r2']\n",
    "        if top_r2 > best_overall_r2:\n",
    "            best_overall_r2 = top_r2\n",
    "            best_n_comp = n_comp\n",
    "    \n",
    "    print(f\"Best overall performance: n_components = {best_n_comp} (RÂ² = {best_overall_r2:.4f})\")\n",
    "    print(f\"\\nShowing brain maps for n_components = {best_n_comp}:\\n\")\n",
    "    \n",
    "    # Get best layer for this dimension\n",
    "    comparison_df = all_comparisons[best_n_comp]\n",
    "    best_layer = comparison_df.iloc[0]['layer']\n",
    "    encoding_results = all_encoding_results[best_n_comp]\n",
    "    best_r2_map = encoding_results[best_layer]['r2_map']\n",
    "    \n",
    "    print(f\"Best layer: {best_layer.upper()}\\n\")\n",
    "    \n",
    "    fig = plot_r2_brainmap(\n",
    "        best_r2_map, \n",
    "        f\"{best_layer} (n={best_n_comp})\",\n",
    "        threshold=0.01,\n",
    "        vmax=0.2\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    # Also show comparison for a specific layer across dimensions\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Comparison: conv2 layer across all dimensions\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, n_comp in enumerate(projection_dims):\n",
    "        ax = axes[idx]\n",
    "        encoding_results = all_encoding_results[n_comp]\n",
    "        r2_map = encoding_results['conv2']['r2_map']\n",
    "        \n",
    "        # Extract data from NIfTI image\n",
    "        if hasattr(r2_map, 'get_fdata'):\n",
    "            r2_data = r2_map.get_fdata().flatten()\n",
    "        else:\n",
    "            r2_data = r2_map.flatten() if hasattr(r2_map, 'flatten') else r2_map\n",
    "        \n",
    "        # Create simple histogram of RÂ² values\n",
    "        r2_positive = r2_data[r2_data > 0]\n",
    "        ax.hist(r2_positive, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "        ax.set_xlabel('RÂ² value')\n",
    "        ax.set_ylabel('Number of voxels')\n",
    "        ax.set_title(f'conv2 RÂ² distribution (n={n_comp})', fontweight='bold')\n",
    "        ax.axvline(np.mean(r2_positive), color='red', linestyle='--', \n",
    "                   label=f'Mean = {np.mean(r2_positive):.4f}')\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nðŸ“ Interpretation:\")\n",
    "    print(\"  - Higher n_components may capture more information\")\n",
    "    print(\"  - But also may introduce more noise\")\n",
    "    print(\"  - Ridge regularization helps balance this trade-off\")\n",
    "    print(\"  - Optimal dimension depends on data complexity and sample size\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš  No model available, skipping brain map visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763ffbbf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary: Brain Encoding with Random Projections\n",
    "\n",
    "**What we accomplished:**\n",
    "\n",
    "1. âœ… **Loaded RL model:** Trained PPO agent\n",
    "2. âœ… **Extracted activations from replays:** Used actual gameplay .bk2 files\n",
    "3. âœ… **Proper temporal alignment:**\n",
    "   - Matched replay frames to fMRI TRs using annotations\n",
    "   - Downsampled from 60Hz to TR (1.49s)\n",
    "   - Applied HRF convolution\n",
    "   - Masked non-gameplay periods with NaN\n",
    "4. âœ… **Applied Random Projection:** Tested multiple dimensions (10, 50, 100, 1000 components)\n",
    "5. âœ… **Fit encoding models:** Ridge regression with NaN-aware training\n",
    "6. âœ… **Compared dimensions:** Evaluated how dimensionality affects prediction\n",
    "7. âœ… **Visualized brain maps:** Localized where each layer is encoded\n",
    "\n",
    "---\n",
    "\n",
    "### Random Projection vs PCA\n",
    "\n",
    "**Why we switched from PCA to Random Projection:**\n",
    "\n",
    "1. **Computational efficiency:**\n",
    "   - PCA: Requires eigendecomposition (O(nÂ³) complexity)\n",
    "   - Random Projection: Just matrix multiplication (O(nÂ²) complexity)\n",
    "\n",
    "2. **Theoretical foundation:**\n",
    "   - Johnson-Lindenstrauss Lemma: Random projections preserve distances\n",
    "   - No need to fit on training data\n",
    "   - Works well for high-dimensional spaces\n",
    "\n",
    "3. **Flexibility:**\n",
    "   - Easy to test multiple dimensions\n",
    "   - No need to compute variance explained\n",
    "   - Same random seed ensures reproducibility\n",
    "\n",
    "4. **Performance comparison:**\n",
    "   - Random projection often performs similarly to PCA for encoding tasks\n",
    "   - Sometimes better when data is noisy or sample size is small\n",
    "\n",
    "**Trade-offs:**\n",
    "- PCA: Optimal variance preservation, interpretable components\n",
    "- Random Projection: Faster, simpler, non-interpretable but effective\n",
    "\n",
    "---\n",
    "\n",
    "### Key Findings: Effect of Dimensionality\n",
    "\n",
    "**Expected patterns:**\n",
    "\n",
    "1. **Too few components (10):**\n",
    "   - Information loss from compression\n",
    "   - May miss important features\n",
    "   - Lower RÂ² values expected\n",
    "\n",
    "2. **Medium components (50-100):**\n",
    "   - Good balance between compression and information\n",
    "   - Likely optimal for this dataset\n",
    "   - Ridge regularization helps prevent overfitting\n",
    "\n",
    "3. **High components (1000):**\n",
    "   - More capacity to capture variance\n",
    "   - But also more noise\n",
    "   - Ridge regularization becomes critical\n",
    "   - May not improve over medium if signal-to-noise is low\n",
    "\n",
    "**Interpretation checklist:**\n",
    "- Did performance increase monotonically with dimensions?\n",
    "- Is there a plateau or sweet spot?\n",
    "- Does the best layer change across dimensions?\n",
    "- Are results consistent or noisy?\n",
    "\n",
    "---\n",
    "\n",
    "### Methodological Insights\n",
    "\n",
    "**What determines optimal dimensionality?**\n",
    "\n",
    "1. **Sample size:** \n",
    "   - More TRs â†’ Can support higher dimensions\n",
    "   - Our dataset: ~100-200 valid TRs per run\n",
    "   - Limited sample size may favor lower dimensions\n",
    "\n",
    "2. **Signal-to-noise ratio:**\n",
    "   - Clean signal â†’ Higher dimensions helpful\n",
    "   - Noisy data â†’ Lower dimensions better (acts as regularization)\n",
    "\n",
    "3. **Feature redundancy:**\n",
    "   - Highly correlated features â†’ PCA/projection removes redundancy\n",
    "   - Independent features â†’ Need more dimensions\n",
    "\n",
    "4. **Regularization:**\n",
    "   - Ridge regression compensates for high dimensionality\n",
    "   - Stronger regularization (higher Î±) â†’ Can handle more dimensions\n",
    "\n",
    "**Recommendations:**\n",
    "- For small datasets (<1000 samples): Use 50-100 components\n",
    "- For medium datasets (1000-10000): Try 100-500 components  \n",
    "- For large datasets (>10000): Can go higher (500-1000+)\n",
    "- Always validate with cross-validation!\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison to Original PCA Approach\n",
    "\n",
    "**Original notebook (PCA with 50 components):**\n",
    "- Fixed at 50 components based on variance threshold\n",
    "- Interpretable components (ordered by variance)\n",
    "- Computational cost moderate\n",
    "\n",
    "**Current approach (Random Projection with 10/50/100/1000):**\n",
    "- Tested multiple dimensions systematically\n",
    "- Non-interpretable but effective\n",
    "- Faster computation\n",
    "- Reveals dimensionality-performance relationship\n",
    "\n",
    "**Which is better?**\n",
    "- PCA: When you need interpretability and optimal compression\n",
    "- Random Projection: When speed matters and you want to test many dimensions\n",
    "- Both: Often give similar encoding performance!\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps & Extensions\n",
    "\n",
    "**To improve these results:**\n",
    "\n",
    "1. **More sophisticated dimensionality reduction:**\n",
    "   - Sparse random projections\n",
    "   - Locality-sensitive hashing\n",
    "   - Autoencoders\n",
    "   - t-SNE or UMAP (for visualization)\n",
    "\n",
    "2. **Cross-validation:**\n",
    "   - Leave-one-run-out for all 4 runs\n",
    "   - Average results across folds\n",
    "   - More robust estimate of optimal dimension\n",
    "\n",
    "3. **Statistical testing:**\n",
    "   - Permutation tests for significance\n",
    "   - Confidence intervals on RÂ² estimates\n",
    "   - Compare to null models (shuffled features)\n",
    "\n",
    "4. **Alternative encodingmodels:**\n",
    "   - Kernel ridge regression\n",
    "   - Elastic net (L1 + L2)\n",
    "   - Neural network encoders\n",
    "   - Bayesian models with automatic relevance determination\n",
    "\n",
    "5. **Feature analysis:**\n",
    "   - Which projected features are most predictive?\n",
    "   - Can we interpret random projections post-hoc?\n",
    "   - Stability analysis (bootstrap over random seeds)\n",
    "\n",
    "---\n",
    "\n",
    "### Practical Takeaways\n",
    "\n",
    "**When should you use random projection?**\n",
    "\n",
    "âœ… **Good for:**\n",
    "- Quick exploratory analysis\n",
    "- Testing multiple dimensions\n",
    "- Very high-dimensional data (>10,000 features)\n",
    "- When computational resources are limited\n",
    "- When interpretability is not critical\n",
    "\n",
    "âŒ **Not ideal for:**\n",
    "- When you need interpretable components\n",
    "- Small feature sets (<100 features)\n",
    "- When optimal variance preservation is critical\n",
    "- Publication-quality analyses (PCA more standard)\n",
    "\n",
    "**Best practices:**\n",
    "1. Always test multiple random seeds and average results\n",
    "2. Use cross-validation to select optimal dimension\n",
    "3. Compare to PCA as a baseline\n",
    "4. Report both methods if results differ substantially\n",
    "\n",
    "---\n",
    "\n",
    "### Research Questions Enabled\n",
    "\n",
    "**This systematic comparison allows us to ask:**\n",
    "\n",
    "1. **Does the brain encoding depend on feature dimensionality?**\n",
    "   - If yes â†’ Brain representation is low-dimensional\n",
    "   - If no â†’ Suggests high-dimensional distributed code\n",
    "\n",
    "2. **Is there redundancy in RL representations?**\n",
    "   - Strong compression (10 dims) works â†’ High redundancy\n",
    "   - Need many dimensions â†’ Features are diverse/independent\n",
    "\n",
    "3. **What's the information bottleneck?**\n",
    "   - Performance plateaus early â†’ Limited by BOLD signal quality\n",
    "   - Keeps improving â†’ Limited by feature richness\n",
    "\n",
    "**This tutorial demonstrates a complete, systematic approach to evaluating dimensionality in encoding models!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
