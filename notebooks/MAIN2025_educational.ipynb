{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mario fMRI Tutorial\n",
    "## Complete Analysis Pipeline: From GLM to Brain Encoding\n",
    "\n",
    "<br>\n",
    "\n",
    "### Overview of the CNeuromod Mario Dataset\n",
    "\n",
    "**What we'll cover:**\n",
    "- Dataset exploration and behavioral annotations\n",
    "- GLM analysis: Actions and game events\n",
    "- RL agent: Learning representations from gameplay\n",
    "- Brain encoding: Predicting fMRI from learned features\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "*CNeuromod 2025*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Setup - hidden from presentation\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "src_dir = Path('..') / 'src'\n",
    "sys.path.insert(0, str(src_dir))\n",
    "\n",
    "from utils import (\n",
    "    get_sourcedata_path,\n",
    "    get_derivatives_path,\n",
    "    load_events,\n",
    "    get_session_runs,\n",
    "    load_lowlevel_confounds\n",
    ")\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Define constants\n",
    "SUBJECT = 'sub-01'\n",
    "SESSION = 'ses-010'\n",
    "TR = 1.49\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 1: Introduction\n",
    "\n",
    "## The CNeuromod Mario Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The CNeuromod Mario Dataset\n",
    "\n",
    "### A Naturalistic fMRI Paradigm\n",
    "\n",
    "**Participants:** 5 subjects playing Super Mario Bros (NES) in the scanner\n",
    "\n",
    "**Task:** Natural gameplay - no constraints on strategy or behavior\n",
    "\n",
    "**Levels:**\n",
    "- **22 levels:** exclusion of waterworld and Bowser levels for gameplay consistency.\n",
    "\n",
    "**Acquisition:**\n",
    "- TR = 1.49s (multiband fMRI)\n",
    "- ~5 runs per session\n",
    "- ~5 minutes per run (~200 volumes)\n",
    "- ~25 minutes total gameplay per session\n",
    "\n",
    "**Key insight:** Real-world complexity with rich behavioral structure\n",
    "\n",
    "<div style=\"background-color: #e8f4f8; padding: 10px; border-radius: 5px; margin-top: 20px;\">\n",
    "<b>Why naturalistic paradigms?</b><br>\n",
    "Traditional fMRI uses simple, repetitive tasks. Naturalistic paradigms like gameplay capture complex, dynamic behavior closer to real-world cognition.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Analysis Pipeline Overview\n",
    "\n",
    "### Two Complementary Approaches\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         fMRI Data                                â”‚\n",
    "â”‚                    (BOLD time series)                            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚                            â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚   GLM Analysis   â”‚         â”‚   RL Agent        â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚                            â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Hypothesis-drivenâ”‚         â”‚ Learned features  â”‚\n",
    "    â”‚ contrasts        â”‚         â”‚ (CNN activations) â”‚\n",
    "    â”‚ - LEFT vs RIGHT  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\n",
    "             â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "             â”‚                   â”‚ Ridge Encoding    â”‚\n",
    "             â”‚                   â”‚ (Predict BOLD)    â”‚\n",
    "             â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚                            â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚         Brain Activity Maps                    â”‚\n",
    "    â”‚    Which regions? What representations?        â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**GLM:** Hand-crafted regressors â†’ Interpretable contrasts\n",
    "\n",
    "**Encoding:** Learned representations â†’ Predictive power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Today's Focus: sub-01, ses-010\n",
    "\n",
    "### Single Session Deep Dive\n",
    "\n",
    "**Why single session?**\n",
    "- Laptop-friendly analysis (~30-45 min runtime)\n",
    "- Complete pipeline demonstration\n",
    "- Easy to extend to multiple subjects/sessions\n",
    "\n",
    "**Session details:**\n",
    "- 5 runs Ã— ~5 minutes = ~25 minutes gameplay\n",
    "- ~1000 fMRI volumes\n",
    "- ~200+ behavioral events\n",
    "\n",
    "**BIDS structure:**\n",
    "```\n",
    "sourcedata/\n",
    "â”œâ”€â”€ mario/                    # Raw fMRI\n",
    "â”œâ”€â”€ mario.fmriprep/          # Preprocessed BOLD\n",
    "â”œâ”€â”€ mario.annotations/       # Behavioral events\n",
    "â”œâ”€â”€ mario.replays/           # Game recordings (.bk2)\n",
    "â””â”€â”€ cneuromod.processed/     # Anatomical templates\n",
    "    â””â”€â”€ smriprep/\n",
    "        â””â”€â”€ sub-01/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 2: Dataset Exploration\n",
    "\n",
    "## Rich Behavioral Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Behavioral Annotations\n",
    "\n",
    "The `mario.annotations` dataset provides three types of events:\n",
    "\n",
    "**1. Action events (button presses):**\n",
    "- A, B, LEFT, RIGHT, UP, DOWN\n",
    "- Precise onset and duration\n",
    "- **Button mappings:**\n",
    "  - **A = JUMP** (short taps, mean duration ~0.3s)\n",
    "  - **B = RUN/FIREBALL** (held continuously, mean duration ~12s)\n",
    "  - LEFT/RIGHT = Movement\n",
    "  - UP = Enter pipe, DOWN = Crouch\n",
    "\n",
    "**2. Game events:**\n",
    "- Kill/stomp, Kill/kick (defeating enemies)\n",
    "- Hit/life_lost (player damage)\n",
    "- Powerup_collected, Coin_collected (rewards)\n",
    "- Flag_reached (level completion)\n",
    "\n",
    "**3. Scene information:**\n",
    "- Level segmentation\n",
    "- Unique scene codes for each game section\n",
    "\n",
    "Let's load and visualize these events!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Load events for all runs in the session\n",
    "\n",
    "sourcedata_path = get_sourcedata_path()\n",
    "\n",
    "try:\n",
    "    runs = get_session_runs(SUBJECT, SESSION, sourcedata_path)\n",
    "    print(f\"Found {len(runs)} runs: {runs}\\n\")\n",
    "    \n",
    "    # Load all events\n",
    "    all_events = []\n",
    "    for run in runs:\n",
    "        events = load_events(SUBJECT, SESSION, run, sourcedata_path)\n",
    "        all_events.append(events)\n",
    "        print(f\"{run}: {len(events)} events\")\n",
    "    \n",
    "    session_events = pd.concat(all_events, ignore_index=True)\n",
    "    print(f\"\\nTotal events: {len(session_events)}\")\n",
    "    \n",
    "    # Categorize\n",
    "    button_events = ['A', 'B', 'LEFT', 'RIGHT', 'UP', 'DOWN']\n",
    "    game_events = ['Kill/stomp', 'Kill/kick', 'Hit/life_lost', \n",
    "                   'Powerup_collected', 'Coin_collected']\n",
    "    \n",
    "    n_buttons = len(session_events[session_events['trial_type'].isin(button_events)])\n",
    "    n_game = len(session_events[session_events['trial_type'].isin(game_events)])\n",
    "    \n",
    "    print(f\"\\nButton presses: {n_buttons}\")\n",
    "    print(f\"Game events: {n_game}\")\n",
    "    \n",
    "    # Top events\n",
    "    print(\"\\nTop 10 most frequent events:\")\n",
    "    print(session_events['trial_type'].value_counts().head(10))\n",
    "    \n",
    "    EVENTS_LOADED = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading events: {e}\")\n",
    "    print(\"Using demo data...\")\n",
    "    EVENTS_LOADED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize event frequencies\n",
    "\n",
    "from glm_utils import plot_event_frequencies\n",
    "\n",
    "fig = plot_event_frequencies(\n",
    "    session_events, n_buttons, n_game,\n",
    "    SUBJECT, SESSION\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Timeline Visualization\n",
    "\n",
    "**Goal:** Understand the temporal structure of gameplay\n",
    "\n",
    "We'll visualize:\n",
    "- Button press patterns over time\n",
    "- Game event occurrences\n",
    "- Event density (actions per second)\n",
    "\n",
    "**What to look for:**\n",
    "- Clusters of activity (intense gameplay moments)\n",
    "- Gaps (deaths, level transitions)\n",
    "- Relationships between buttons and game events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Event timeline for first run\n",
    "\n",
    "from glm_utils import plot_event_timeline\n",
    "\n",
    "if EVENTS_LOADED and len(all_events) > 0:\n",
    "    fig = plot_event_timeline(all_events[0], runs[0], button_events)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Timeline not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Game Replay Data\n",
    "\n",
    "### Frame-by-frame recordings (.bk2 files)\n",
    "\n",
    "**What's in a replay?**\n",
    "- 60 Hz game frames\n",
    "- Button states for each frame\n",
    "- RAM variables: player position, score, lives, time, power-up state\n",
    "\n",
    "**Uses:**\n",
    "1. **RL training:** Extract frames as visual input for CNN\n",
    "2. **Validation:** Verify behavioral annotations\n",
    "3. **Visualization:** Show actual gameplay moments\n",
    "\n",
    "**For this tutorial:** We'll use simplified proxy features instead of full frame extraction (faster for demonstration)\n",
    "\n",
    "<div style=\"background-color: #fff3cd; padding: 10px; border-radius: 5px; margin-top: 20px;\">\n",
    "<b>Note:</b> Full replay processing requires BizHawk emulator and can extract ~18,000 frames per run. For efficiency, we use pre-computed features.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 3: GLM Analysis\n",
    "\n",
    "## Finding Brain Regions for Actions and Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLM Fundamentals\n",
    "\n",
    "### The General Linear Model for fMRI\n",
    "\n",
    "**Basic idea:** Model brain activity as a weighted sum of explanatory variables\n",
    "\n",
    "```\n",
    "BOLD(t) = Î²â‚Â·Regressorâ‚(t) + Î²â‚‚Â·Regressorâ‚‚(t) + ... + Îµ(t)\n",
    "```\n",
    "\n",
    "**Steps:**\n",
    "1. **Event timing** â†’ Neural activity (stick functions)\n",
    "2. **Add confounds** â†’ Motion, physiology, low-level features\n",
    "3. **HRF convolution** â†’ Expected BOLD response\n",
    "4. **Fit model** â†’ Estimate Î² weights (using nilearn's FirstLevelModel)\n",
    "5. **Compute contrasts** â†’ Test hypotheses\n",
    "\n",
    "**Our confound strategy (following shinobi_fmri):**\n",
    "- **Motion:** 6 motion parameters from fMRIPrep\n",
    "- **Physiology:** WM, CSF signals (loaded via load_confounds)\n",
    "- **Task:** Button press counts (manual psychophysics confound)\n",
    "- **Standardization:** Done by nilearn (`clean_img` with `standardize=True`)\n",
    "\n",
    "**Models we'll fit:**\n",
    "1. **Hand lateralization:** LEFT_THUMB (D-pad) vs RIGHT_THUMB (A+B buttons)\n",
    "\n",
    "**Multi-run fitting:**\n",
    "- nilearn's FirstLevelModel can fit all runs simultaneously\n",
    "- Provides list of BOLD images + list of design matrices\n",
    "- More efficient than fitting runs separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load BOLD image\n",
    "from utils import load_bold\n",
    "from nilearn.image import clean_img\n",
    "\n",
    "bold_img = load_bold(SUBJECT, SESSION, runs[0], sourcedata_path)\n",
    "bold_clean = clean_img(\n",
    "    bold_img,\n",
    "    standardize=True,\n",
    "    detrend=True,\n",
    "    high_pass=None,\n",
    "    t_r=TR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load fMRIPrep confounds\n",
    "\n",
    "from nilearn.interfaces.fmriprep import load_confounds\n",
    "\n",
    "def load_fmriprep_confounds(subject, session, run):\n",
    "    \"\"\"\n",
    "    Load confounds from fMRIPrep for a given run.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subject : str\n",
    "        Subject ID (e.g., 'sub-01')\n",
    "    session : str\n",
    "        Session ID (e.g., 'ses-010')\n",
    "    run : str\n",
    "        Run ID (e.g., 'run-1')\n",
    "    derivatives_path : Path\n",
    "        Path to derivatives directory\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    confounds_df : pd.DataFrame\n",
    "        Confounds dataframe with selected regressors\n",
    "    sample_mask : np.ndarray or None\n",
    "        Sample mask (if applicable)\n",
    "    \"\"\"\n",
    "    sourcedata_path = get_sourcedata_path()\n",
    "    # Path to preprocessed BOLD image\n",
    "    bold_path = (\n",
    "        sourcedata_path / 'mario.fmriprep' / subject / session / 'func' /\n",
    "        f'{subject}_{session}_task-mario_{run}_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz'\n",
    "    )\n",
    "    \n",
    "    # Load confounds using nilearn's fMRIPrep interface\n",
    "    # This automatically finds the corresponding *_desc-confounds_timeseries.tsv file\n",
    "    confounds_df, sample_mask = load_confounds(\n",
    "        str(bold_path),\n",
    "        strategy=(\"motion\", \"high_pass\", \"wm_csf\"),\n",
    "        motion=\"full\",       # Motion parameters (6 params + derivatives + powers)\n",
    "        wm_csf=\"basic\",      # White matter + CSF signals\n",
    "        global_signal=\"full\", # Global signal\n",
    "        demean=False, # Because we will standardize later\n",
    "    )\n",
    "\n",
    "    return confounds_df, sample_mask\n",
    "\n",
    "# Test on first run\n",
    "confounds_df, sample_mask = load_fmriprep_confounds(\n",
    "    SUBJECT, SESSION, runs[0]\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Loaded {confounds_df.shape[1]} confounds for {runs[0]}\")\n",
    "print(f\"  Timepoints: {confounds_df.shape[0]}\")\n",
    "print(f\"\\nConfound types:\")\n",
    "motion_cols = [c for c in confounds_df.columns if 'trans' in c or 'rot' in c]\n",
    "physio_cols = [c for c in confounds_df.columns if 'csf' in c or 'white_matter' in c or 'global_signal' in c]\n",
    "drift_cols = [c for c in confounds_df.columns if 'cosine' in c]\n",
    "print(f\"  Motion: {len(motion_cols)} (6 params + derivatives + quadratic)\")\n",
    "print(f\"  Physiology: {len(physio_cols)} (WM, CSF, global signal)\")\n",
    "print(f\"  Drift: {len(drift_cols)} (cosine basis for high-pass filtering)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Add psychophysics confounds (following shinobi_fmri)\n",
    "\n",
    "def add_psychophysics_confounds(confounds_df, lowlevel_features, tr):\n",
    "    \"\"\"\n",
    "    Add psychophysics (low-level game features) as confounds.\n",
    "    \n",
    "    Following shinobi_fmri: downsample 60Hz features to TR.\n",
    "    These capture low-level visual/game state that we want to regress out.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    confounds_df : pd.DataFrame\n",
    "        Existing confounds\n",
    "    lowlevel_features : dict\n",
    "        Dictionary of feature name -> 60Hz signal arrays\n",
    "    tr : float\n",
    "        Repetition time in seconds\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Confounds with added psychophysics columns\n",
    "    \"\"\"\n",
    "    if lowlevel_features is None or len(lowlevel_features) == 0:\n",
    "        return confounds_df\n",
    "    \n",
    "    confounds_df = confounds_df.copy()\n",
    "    n_scans = len(confounds_df)\n",
    "    \n",
    "    # Downsample each feature from 60Hz to TR\n",
    "    for feature_name, signal_60hz in lowlevel_features.items():\n",
    "        # Convert 60Hz signal to TR-sampled\n",
    "        n_frames = len(signal_60hz)\n",
    "        frame_times = np.arange(n_frames) / 60.0  # 60 fps\n",
    "        \n",
    "        # Bin frames by TR\n",
    "        tr_bins = (frame_times // tr).astype(int)\n",
    "        \n",
    "        # Average within each TR\n",
    "        signal_tr = np.zeros(n_scans)\n",
    "        for tr_idx in range(n_scans):\n",
    "            frames_in_tr = signal_60hz[tr_bins == tr_idx]\n",
    "            if len(frames_in_tr) > 0:\n",
    "                signal_tr[tr_idx] = np.mean(frames_in_tr)\n",
    "        \n",
    "        # Add to confounds\n",
    "        confounds_df[f'{feature_name}'] = signal_tr\n",
    "    \n",
    "    return confounds_df\n",
    "\n",
    "lowlevel_features = load_lowlevel_confounds(SUBJECT, SESSION, runs[0], sourcedata_path)\n",
    "\n",
    "confounds_df = add_psychophysics_confounds(confounds_df, lowlevel_features, TR)\n",
    "print(f\"âœ“ Added {len(lowlevel_features.keys())} psychophysics confounds:\")\n",
    "for key in lowlevel_features.keys():\n",
    "    print(f\"  - {key}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Total confounds: {confounds_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Add button press confounds (following shinobi_fmri)\n",
    "\n",
    "def add_button_press_confounds(confounds_df, events_df, tr):\n",
    "    \"\"\"\n",
    "    Add button press counts as confounds.\n",
    "    \n",
    "    Following shinobi_fmri: count button presses in each TR.\n",
    "    This captures motor-related noise from button pressing.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    confounds_df : pd.DataFrame\n",
    "        Existing confounds\n",
    "    events_df : pd.DataFrame\n",
    "        Events dataframe with trial_type, onset columns\n",
    "    tr : float\n",
    "        Repetition time in seconds\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Confounds with added button_count_press column\n",
    "    \"\"\"\n",
    "    confounds_df = confounds_df.copy()\n",
    "    n_scans = len(confounds_df)\n",
    "    \n",
    "    # Initialize button press count\n",
    "    button_count = np.zeros(n_scans)\n",
    "    \n",
    "    # Button types\n",
    "    button_types = ['A', 'B', 'LEFT', 'RIGHT', 'UP', 'DOWN']\n",
    "    \n",
    "    # Count presses in each TR\n",
    "    for _, event in events_df.iterrows():\n",
    "        if event['trial_type'] in button_types:\n",
    "            tr_idx = int(np.floor(event['onset'] / tr))\n",
    "            if 0 <= tr_idx < n_scans:\n",
    "                button_count[tr_idx] += 1\n",
    "    \n",
    "    confounds_df['button_count_press'] = button_count\n",
    "    \n",
    "    return confounds_df\n",
    "\n",
    "# Test on first run\n",
    "events = load_events(SUBJECT, SESSION, runs[0], sourcedata_path)\n",
    "confounds_df = add_button_press_confounds(confounds_df, events, TR)\n",
    "\n",
    "button_count = confounds_df['button_count_press'].values\n",
    "print(f\"âœ“ Added button_count_press confound\")\n",
    "print(f\"  Mean: {button_count.mean():.2f} presses/TR\")\n",
    "print(f\"  Max: {int(button_count.max())} presses in a single TR\")\n",
    "print(f\"  Non-zero TRs: {(button_count > 0).sum()}/{len(button_count)}\")\n",
    "print(f\"\\nðŸ“Š Total confounds: {confounds_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Standardize confounds\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "confounds_df.loc[:, confounds_df.columns] = scaler.fit_transform(confounds_df)\n",
    "print(f\"âœ“ Standardized confounds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confound structure\n",
    "from glm_utils import plot_confounds_structure\n",
    "\n",
    "fig = plot_confounds_structure(confounds_df, runs[0], button_count=button_count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create hand lateralization events\n",
    "\n",
    "def create_hand_lateralization_events(events_df):\n",
    "    \"\"\"\n",
    "    Create hand lateralization events from button presses.\n",
    "    \n",
    "    LEFT_THUMB: D-pad buttons (LEFT, RIGHT, UP, DOWN) - controlled by left thumb\n",
    "    RIGHT_THUMB: Action buttons (A, B) - controlled by right thumb\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    events_df : pd.DataFrame\n",
    "        Events with trial_type, onset, duration\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Events with LEFT_THUMB and RIGHT_THUMB conditions\n",
    "    \"\"\"\n",
    "    hand_events = []\n",
    "    \n",
    "    for _, event in events_df.iterrows():\n",
    "        trial_type = event['trial_type']\n",
    "        \n",
    "        if trial_type in ['LEFT', 'RIGHT', 'UP', 'DOWN']:\n",
    "            hand_events.append({\n",
    "                'onset': event['onset'],\n",
    "                'duration': event['duration'],\n",
    "                'trial_type': 'LEFT_THUMB'\n",
    "            })\n",
    "        elif trial_type in ['A', 'B']:\n",
    "            hand_events.append({\n",
    "                'onset': event['onset'],\n",
    "                'duration': event['duration'],\n",
    "                'trial_type': 'RIGHT_THUMB'\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(hand_events)\n",
    "\n",
    "# Create for first run\n",
    "hand_events = create_hand_lateralization_events(events)\n",
    "\n",
    "# Show summary\n",
    "n_left = (hand_events['trial_type'] == 'LEFT_THUMB').sum()\n",
    "n_right = (hand_events['trial_type'] == 'RIGHT_THUMB').sum()\n",
    "\n",
    "print(f\"Hand lateralization events for {runs[0]}:\")\n",
    "print(f\"  LEFT_THUMB: {n_left} events\")\n",
    "print(f\"  RIGHT_THUMB: {n_right} events\")\n",
    "print(f\"\\nFirst few events:\")\n",
    "print(hand_events.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Create design matrix for one run\n",
    "\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix\n",
    "from nilearn.plotting import plot_design_matrix\n",
    "\n",
    "def create_design_matrix(bold_img, hand_events, confounds_df, tr):\n",
    "    \"\"\"\n",
    "    Create design matrix for GLM analysis.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bold_img : Nifti1Image\n",
    "        Preprocessed BOLD image\n",
    "    hand_events : pd.DataFrame\n",
    "        Hand lateralization events (LEFT_THUMB, RIGHT_THUMB)\n",
    "    confounds_df : pd.DataFrame\n",
    "        Confounds (motion, physiology, task)\n",
    "    tr : float\n",
    "        Repetition time in seconds\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    design_matrix : pd.DataFrame\n",
    "        Design matrix with task regressors + confounds + drift\n",
    "    \"\"\"\n",
    "    # Get frame times\n",
    "    n_scans = bold_img.shape[-1]\n",
    "    frame_times = np.arange(n_scans) * tr\n",
    "    \n",
    "    # Create design matrix\n",
    "    design_matrix = make_first_level_design_matrix(\n",
    "        frame_times,\n",
    "        events=hand_events,\n",
    "        hrf_model='spm',           # SPM canonical HRF\n",
    "        drift_model=None,          # Handled by confounds\n",
    "        high_pass=None,            # Handled by confounds\n",
    "        add_regs=confounds_df,\n",
    "        add_reg_names=list(confounds_df.columns)\n",
    "    )\n",
    "    \n",
    "    return design_matrix\n",
    "\n",
    "# Create design matrix for first run\n",
    "design_matrix = create_design_matrix(\n",
    "    bold_img, hand_events, confounds_df, TR\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Created design matrix for {runs[0]}\\n\")\n",
    "print(f\"Design matrix structure:\")\n",
    "print(f\"  Shape: {design_matrix.shape}\")\n",
    "print(f\"  Timepoints: {design_matrix.shape[0]}\")\n",
    "print(f\"  Regressors: {design_matrix.shape[1]}\")\n",
    "print(f\"\\nRegressor breakdown:\")\n",
    "print(f\"  Task: 2 (LEFT_THUMB, RIGHT_THUMB)\")\n",
    "print(f\"  Confounds: {len(confounds_df.columns)}\")\n",
    "print(f\"  Constant: 1\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "plot_design_matrix(design_matrix, ax=ax, rescale=True);\n",
    "ax.set_title(f'Design Matrix - {runs[0]}', fontsize=14, fontweight='bold')\n",
    "ax.grid(False)  # Deactivate grid\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Scrub volumes between repetitions.\n",
    "def add_scrub_regressors(run_events, design_matrix):\n",
    "    \"\"\"Creates a scrub regressor that indicates when a repetition (with available bk2)\n",
    "    is ongoing or not. If no bk2 is available, the repetition will be ignored and\n",
    "    scrubbed as 0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    run_events : DataFrame\n",
    "        The original dataframe created with create_runevents, containing all the events of one run\n",
    "    design_matrix : DataFrame\n",
    "        Design matrix as created by Nilearn's make_first_level_design_matrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    design_matrix : DataFrame\n",
    "        Same design_matrix as above, with the addition of a scrub confounds.\n",
    "    \"\"\"\n",
    "    reps = []\n",
    "    # Get repetition segments\n",
    "    for i in range(len(run_events)):\n",
    "        if run_events['trial_type'][i] == \"gym-retro_game\":\n",
    "            reps.append(run_events.iloc[i,:])\n",
    "\n",
    "    # Get time vector\n",
    "    time = np.array(design_matrix.index)\n",
    "\n",
    "    to_keep = np.zeros(len(time))\n",
    "    # Generate binary regressor\n",
    "    for i in range(len(time)):\n",
    "        for rep in reps:\n",
    "            if type(rep[\"stim_file\"]) == str and rep[\"stim_file\"] != \"Missing file\" and type(rep[\"stim_file\"]) != float:\n",
    "                if time[i] >= rep['onset'] and time[i] <= rep['onset'] + rep['duration']:\n",
    "                    to_keep[i] = 1.0\n",
    "\n",
    "    scrub_idx = 1\n",
    "    for idx, timepoint in enumerate(to_keep):\n",
    "        if timepoint == 0.0: # If to_keep is zero create a scrub regressor to remove this frame\n",
    "            scrub_regressor = np.zeros(len(time))\n",
    "            scrub_regressor[idx] = 1.0\n",
    "            design_matrix[f'scrub{scrub_idx}'] = scrub_regressor\n",
    "            scrub_idx += 1\n",
    "    return design_matrix\n",
    "design_matrix = add_scrub_regressors(events, design_matrix)\n",
    "print(f\"âœ“ Added scrub regressors\")\n",
    "print(f\"\\nðŸ“Š Total regressors: {design_matrix.shape[1]}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "plot_design_matrix(design_matrix, ax=ax, rescale=True)\n",
    "ax.set_title(f'Design Matrix - {runs[0]}', fontsize=14, fontweight='bold')\n",
    "ax.grid(False)  # Deactivate grid\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Prepare inputs of each run for FirstLevel GLM\n",
    "\n",
    "from nilearn.image import clean_img\n",
    "from nilearn.masking import compute_multi_epi_mask\n",
    "from utils import load_bold, load_brain_mask\n",
    "\n",
    "print(\"Preparing data for all runs...\\n\")\n",
    "\n",
    "bold_imgs = []\n",
    "design_matrices = []\n",
    "confounds_list = []\n",
    "events_list = []\n",
    "hand_events_list = []\n",
    "DESIGN_READY = False\n",
    "bold_imgs_for_mask = []\n",
    "\n",
    "for run_idx, run in enumerate(runs):\n",
    "    print(f\"{run}...\")\n",
    "    bold_img = load_bold(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    bold_imgs_for_mask.append(bold_img) # Keep original for mask computation\n",
    "    bold_clean = clean_img(\n",
    "        bold_img,\n",
    "        standardize=True,\n",
    "        detrend=True,\n",
    "        high_pass=None,\n",
    "        t_r=TR\n",
    "    )\n",
    "    bold_imgs.append(bold_clean)\n",
    "\n",
    "    confounds_df, _ = load_fmriprep_confounds(SUBJECT, SESSION, run)\n",
    "    events = all_events[run_idx] if EVENTS_LOADED else load_events(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    events_list.append(events)\n",
    "\n",
    "    confounds_df = add_button_press_confounds(confounds_df, events, TR)\n",
    "    lowlevel = load_lowlevel_confounds(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    confounds_df = add_psychophysics_confounds(confounds_df, lowlevel, TR)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    confounds_df.loc[:, confounds_df.columns] = scaler.fit_transform(confounds_df)\n",
    "    confounds_list.append(confounds_df)\n",
    "\n",
    "    hand_events = create_hand_lateralization_events(events)\n",
    "    hand_events_list.append(hand_events)\n",
    "\n",
    "    design_matrix = create_design_matrix(bold_clean, hand_events, confounds_df, TR)\n",
    "    design_matrix = add_scrub_regressors(events, design_matrix)\n",
    "    design_matrices.append(design_matrix)\n",
    "\n",
    "    print(f\"  âœ“ {bold_clean.shape[-1]} TRs, {len(confounds_df.columns)} confounds, {design_matrix.shape[1]} regressors\")\n",
    "\n",
    "print(\"\\nCreating common mask...\")\n",
    "# Create mask from original images\n",
    "common_mask = compute_multi_epi_mask(bold_imgs_for_mask, n_jobs=1)\n",
    "n_voxels_mask = int((common_mask.get_fdata() > 0).sum())\n",
    "print(f\"âœ“ Common mask: {n_voxels_mask:,} voxels\")\n",
    "\n",
    "\n",
    "DESIGN_READY = len(design_matrices) == len(bold_imgs) and len(bold_imgs) > 0\n",
    "\n",
    "print(f\"\\nâœ“ {len(bold_imgs)} runs prepared\")\n",
    "print(f\"âœ“ Design matrices ready: {DESIGN_READY}\")\n",
    "print(f\"âœ“ Hand event tables: {len(hand_events_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit GLM on all runs\n",
    "\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "\n",
    "# Define model\n",
    "glm = FirstLevelModel(\n",
    "    t_r=TR,\n",
    "    mask_img=common_mask,\n",
    "    noise_model='ar1',        # AR(1) autocorrelation\n",
    "    standardize=False,        # Already standardized\n",
    "    smoothing_fwhm=5,         # 5mm FWHM spatial smoothing\n",
    "    minimize_memory=True\n",
    ")\n",
    "\n",
    "# Fit on all runs simultaneously\n",
    "print(\"Fitting GLM...\")\n",
    "glm.fit(bold_imgs, design_matrices=design_matrices)\n",
    "\n",
    "print(f\"\\nâœ“ GLM fitted on {len(bold_imgs)} runs\")\n",
    "print(f\"  Model: AR(1) noise, SPM HRF\")\n",
    "print(f\"  Voxels: {n_voxels_mask:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute contrast and visualize\n",
    "\n",
    "from glm_utils import plot_contrast_surfaces\n",
    "\n",
    "# Compute LEFT_THUMB - RIGHT_THUMB contrast\n",
    "z_map = glm.compute_contrast('LEFT_THUMB - RIGHT_THUMB', output_type='z_score')\n",
    "\n",
    "# Plot on surface (using helper function)\n",
    "fig = plot_contrast_surfaces(\n",
    "    z_map, \n",
    "    contrast_name='Hand Lateralization: LEFT_THUMB - RIGHT_THUMB',\n",
    "    stat_threshold=4.0,\n",
    "    cluster_threshold=10\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Interpretation:\")\n",
    "print(\"  Red: LEFT_THUMB > RIGHT_THUMB â†’ Right motor cortex (contralateral)\")\n",
    "print(\"  Blue: RIGHT_THUMB > LEFT_THUMB â†’ Left motor cortex (contralateral)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Cluster-level FWE correction for multiple comparisons\n",
    "\n",
    "from nilearn.glm import cluster_level_inference\n",
    "\n",
    "print(\"Applying cluster-level FWE correction (alpha=0.05)...\\n\")\n",
    "\n",
    "# Perform cluster-level inference with FWE correction\n",
    "# Returns a Nifti1Image with proportion of true discoveries per voxel\n",
    "proportion_true_discoveries = cluster_level_inference(\n",
    "    z_map,\n",
    "    mask_img=common_mask,\n",
    "    threshold=3,  # Cluster-forming threshold\n",
    "    alpha=0.05      # Family-wise error rate\n",
    ")\n",
    "\n",
    "print(f\"âœ“ FWE correction complete\")\n",
    "print(f\"  Cluster-forming threshold: Z > 3.1\")\n",
    "print(f\"  FWE-corrected alpha: 0.05\")\n",
    "\n",
    "# Create FWE-corrected z-map by masking original z-map\n",
    "# Only keep voxels with proportion > 0 (significant clusters)\n",
    "from nilearn.image import math_img\n",
    "\n",
    "z_map_fwe = math_img(\n",
    "    \"img1 * (img2 > 0)\",\n",
    "    img1=z_map,\n",
    "    img2=proportion_true_discoveries\n",
    ")\n",
    "\n",
    "# Count significant voxels\n",
    "fwe_data = z_map_fwe.get_fdata()\n",
    "n_sig_voxels = np.sum(np.abs(fwe_data) > 0)\n",
    "\n",
    "print(f\"  Significant voxels: {n_sig_voxels:,}\")\n",
    "\n",
    "# Plot FWE-corrected results\n",
    "print(\"\\nPlotting FWE-corrected contrast...\")\n",
    "fig = plot_contrast_surfaces(\n",
    "    z_map_fwe,\n",
    "    contrast_name='Hand Lateralization (FWE-corrected, p<0.05)',\n",
    "    stat_threshold=0.1,  # Lower threshold since already FWE-corrected\n",
    "    cluster_threshold=0\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Multiple comparison correction applied!\")\n",
    "print(\"   Compare this with the uncorrected map above.\")\n",
    "print(\"   FWE correction is more conservative but controls false positives.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLM Analysis for Naturalistic Paradigms\n",
    "\n",
    "### The Complete Pipeline\n",
    "\n",
    "We just performed a **session-level GLM analysis** on naturalistic fMRI data (video game playing). Let's understand what happened at each step:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Data Preparation\n",
    "\n",
    "**BOLD Preprocessing:**\n",
    "```python\n",
    "clean_img(bold_img, standardize=True, detrend=True, high_pass=None, t_r=TR)\n",
    "```\n",
    "- **Standardization (z-score):** Removes mean, scales to unit variance\n",
    "  - Makes BOLD comparable across voxels and runs\n",
    "  - Necessary because raw BOLD has arbitrary units (scanner-dependent)\n",
    "- **Linear detrending:** Removes linear drift within each run\n",
    "  - Scanner signal drifts over time (heating, subject movement)\n",
    "  - Detrending isolates neural signal from scanner drift\n",
    "- **No high-pass filtering here:** Handled by cosine basis functions from fMRIPrep confounds\n",
    "\n",
    "**Common Masking:**\n",
    "```python\n",
    "compute_multi_epi_mask(bold_imgs_for_mask, n_jobs=1)\n",
    "```\n",
    "- Creates **intersection** of all run masks\n",
    "- Only analyzes voxels that are valid in **ALL** runs\n",
    "- **Why necessary?**\n",
    "  - Subject moves between runs â†’ brain edges shift\n",
    "  - Without common mask: some voxels missing data in some runs\n",
    "  - Result: invalid statistics at brain boundaries\n",
    "- **Trade-off:** Smaller mask, but more reliable\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Confound Strategy\n",
    "\n",
    "**What are confounds?**\n",
    "Signals that affect BOLD but aren't related to task/cognition.\n",
    "\n",
    "**Our confound sources (via nilearn's `load_confounds`):**\n",
    "```python\n",
    "load_confounds(bold_path, strategy=(\"motion\", \"high_pass\", \"wm_csf\"), ...)\n",
    "```\n",
    "- **Motion (24 parameters):** 6 motion params + derivatives + quadratic terms\n",
    "- **Physiology:** White matter + CSF signals (basic)\n",
    "- **Global signal:** Full global signal regressors\n",
    "- **High-pass filtering:** Cosine basis functions from fMRIPrep\n",
    "\n",
    "**Additional task confounds:**\n",
    "- **Button press counts:** Total presses per TR (motor noise)\n",
    "- **Low-level game features:** Player position (x, y), score, time, lives, powerup state\n",
    "  - Downsampled from 60Hz to TR resolution\n",
    "  - Captures visual/game state variance we want to control for\n",
    "\n",
    "**Confound standardization:**\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "confounds_df.loc[:, confounds_df.columns] = scaler.fit_transform(confounds_df)\n",
    "```\n",
    "- All confounds z-scored before adding to design matrix\n",
    "- Ensures comparable scaling across different confound types\n",
    "\n",
    "**How confounds are handled:**\n",
    "```python\n",
    "make_first_level_design_matrix(..., add_regs=confounds_df, add_reg_names=list(confounds_df.columns))\n",
    "```\n",
    "- Confounds become **nuisance regressors** in design matrix\n",
    "- GLM estimates their contribution and removes it\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Scrubbing Strategy\n",
    "\n",
    "**Problem:** Not all timepoints have valid replay data (.bk2 files)\n",
    "\n",
    "**Solution:** Scrub regressors for invalid timepoints\n",
    "```python\n",
    "add_scrub_regressors(run_events, design_matrix)\n",
    "```\n",
    "- Creates one-hot regressors for each invalid TR\n",
    "- Timepoints outside valid game repetitions are regressed out\n",
    "- Ensures we only analyze TRs with corresponding behavioral data\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Multi-Run GLM Fitting\n",
    "\n",
    "**Traditional approach:**\n",
    "```\n",
    "For each run:\n",
    "  Fit GLM â†’ Get run-level betas\n",
    "Aggregate run-level betas â†’ Session-level betas\n",
    "```\n",
    "\n",
    "**Nilearn's multi-run approach:**\n",
    "```python\n",
    "glm.fit(bold_imgs, design_matrices=design_matrices)\n",
    "```\n",
    "- Fits all runs **simultaneously**\n",
    "- Models run-to-run variance properly\n",
    "- More efficient (one optimization, not 5 separate ones)\n",
    "- Produces session-level effects directly\n",
    "\n",
    "**GLM settings:**\n",
    "```python\n",
    "FirstLevelModel(\n",
    "    t_r=TR,\n",
    "    mask_img=common_mask,\n",
    "    noise_model='ar1',        # AR(1) autocorrelation\n",
    "    standardize=False,        # Already done in clean_img\n",
    "    smoothing_fwhm=5,         # 5mm FWHM spatial smoothing\n",
    "    minimize_memory=True\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. The Hand Lateralization Contrast\n",
    "\n",
    "**Hypothesis:** Contralateral motor control\n",
    "- **LEFT_THUMB** (D-pad: LEFT, RIGHT, UP, DOWN) â†’ Right motor cortex\n",
    "- **RIGHT_THUMB** (A, B buttons) â†’ Left motor cortex\n",
    "\n",
    "**What the contrast does:**\n",
    "```python\n",
    "z_map = glm.compute_contrast('LEFT_THUMB - RIGHT_THUMB', output_type='z_score')\n",
    "```\n",
    "- For each voxel: (Î²_LEFT - Î²_RIGHT) / SE\n",
    "- Positive z-scores: more active for left thumb â†’ right motor cortex\n",
    "- Negative z-scores: more active for right thumb â†’ left motor cortex\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Statistical Thresholding\n",
    "\n",
    "**Uncorrected visualization:**\n",
    "```python\n",
    "plot_contrast_surfaces(z_map, stat_threshold=4.0, cluster_threshold=10)\n",
    "```\n",
    "- **Z > 4.0:** Stringent uncorrected threshold\n",
    "- **Cluster size > 10 voxels:** Reduces isolated false positives\n",
    "\n",
    "**FWE-corrected inference:**\n",
    "```python\n",
    "cluster_level_inference(z_map, mask_img=common_mask, threshold=3, alpha=0.05)\n",
    "```\n",
    "- **Cluster-forming threshold:** Z > 3\n",
    "- **Family-wise error rate:** Î± = 0.05\n",
    "- Controls false positive rate across the whole brain\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Visualization \n",
    "\n",
    "**What we're seeing:**\n",
    "- Red regions: LEFT_THUMB > RIGHT_THUMB\n",
    "- Blue regions: RIGHT_THUMB > LEFT_THUMB\n",
    "- Background sulci: anatomical reference\n",
    "\n",
    "---\n",
    "\n",
    "### Strengths of This Approach\n",
    "\n",
    "âœ… **Event-related design:** Models individual button presses\n",
    "âœ… **Multi-run aggregation:** Robust session-level statistics\n",
    "âœ… **Comprehensive confound control:** Motion, physiology, task, low-level features\n",
    "âœ… **Scrubbing:** Excludes invalid timepoints\n",
    "âœ… **FWE correction:** Proper multiple comparison correction\n",
    "âœ… **Interpretable:** Clear link between behavior and brain\n",
    "\n",
    "---\n",
    "\n",
    "### Limitations and Considerations\n",
    "\n",
    "**1. Sparse Event Coverage**\n",
    "- Only models button presses (a few events per second)\n",
    "- Ignores continuous gameplay (visual flow, cognitive planning, value)\n",
    "- **Alternative:** Encoding models with continuous features (see Section 5)\n",
    "\n",
    "**2. Hand-Crafted Regressors**\n",
    "- LEFT_THUMB vs RIGHT_THUMB is our hypothesis\n",
    "- What if brain codes actions differently? (e.g., spatial vs non-spatial)\n",
    "- **Alternative:** Data-driven approaches (MVPA, RSA, encoding models)\n",
    "\n",
    "**3. Session-Level Analysis Only**\n",
    "- 5 runs, ~25 minutes of data\n",
    "- Subject-specific activations, not generalizable yet\n",
    "- **Better:** Multi-session or multi-subject analysis\n",
    "\n",
    "**4. Hemodynamic Response Assumptions**\n",
    "- Used canonical SPM HRF (standard shape)\n",
    "- Real HRF varies across regions and subjects\n",
    "- **Alternative:** FIR models (finite impulse response) - no HRF assumption\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use GLM vs Other Approaches\n",
    "\n",
    "**GLM is best for:**\n",
    "- Testing specific hypotheses (e.g., \"Does motor cortex code hand laterality?\")\n",
    "- Event-related designs with known trial timing\n",
    "- Interpreting results in terms of experimental conditions\n",
    "- Publication in cognitive neuroscience journals\n",
    "\n",
    "**GLM is limited for:**\n",
    "- Discovering unexpected brain representations\n",
    "- Continuous naturalistic stimuli (movies, games)\n",
    "- Modeling latent variables (value, prediction error, uncertainty)\n",
    "- Predicting brain activity from complex features\n",
    "\n",
    "**Complementary approaches:**\n",
    "- **Encoding models:** Predict BOLD from stimulus/behavior features\n",
    "- **MVPA (decoding):** Predict behavior from BOLD patterns\n",
    "- **RSA:** Compare representational geometry\n",
    "- **ICA/PCA:** Discover data-driven networks\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In **Section 5 (Brain Encoding)**, we'll address GLM's limitations by:\n",
    "1. Using **continuous RL features** (not just button presses)\n",
    "2. **Predicting BOLD** from learned representations\n",
    "3. Discovering which brain regions encode **hierarchical game features**\n",
    "4. Using **data-driven** (not hypothesis-driven) approach\n",
    "\n",
    "**Key difference:**\n",
    "- GLM: \"Does LEFT_THUMB activate motor cortex?\" (test hypothesis)\n",
    "- Encoding: \"What features predict motor cortex activity?\" (explore)\n",
    "\n",
    "Both are valuable! GLM for **interpretation**, encoding for **prediction**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 6: Synthesis\n",
    "\n",
    "## Bringing It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Comparing GLM vs Encoding\n",
    "\n",
    "### Complementary Approaches to Understanding Brain Activity\n",
    "\n",
    "| Aspect | GLM | Encoding |\n",
    "|--------|-----|----------|\n",
    "| **Philosophy** | Hypothesis-driven | Data-driven |\n",
    "| **Features** | Hand-crafted (LEFT, RIGHT, Reward) | Learned (CNN activations) |\n",
    "| **Interpretability** | High (direct behavioral mapping) | Medium (requires interpretation) |\n",
    "| **Coverage** | Sparse (only modeled events) | Dense (continuous representations) |\n",
    "| **Prediction** | Moderate (known variables) | High (latent variables) |\n",
    "| **Discovery** | Tests hypotheses | Generates hypotheses |\n",
    "\n",
    "### Convergent Evidence\n",
    "\n",
    "**Motor cortex:**\n",
    "- GLM: LEFT-RIGHT contrast â†’ Lateralized activation\n",
    "- Encoding: conv3/conv4 â†’ Motor regions\n",
    "- **Conclusion:** Both methods identify action-related areas\n",
    "\n",
    "**Reward system:**\n",
    "- GLM: Powerup-Hit contrast â†’ Striatum\n",
    "- Encoding: linear layer â†’ Frontal/striatal\n",
    "- **Conclusion:** Value representations in expected regions\n",
    "\n",
    "**Unique insights:**\n",
    "- GLM reveals *which specific events* activate regions\n",
    "- Encoding reveals *what computational level* (layer) is represented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "**1. Dataset richness**\n",
    "- Naturalistic fMRI captures complex, dynamic behavior\n",
    "- Rich annotations enable detailed GLM analysis\n",
    "- Replay data supports computational modeling\n",
    "\n",
    "**2. GLM reveals functional specialization**\n",
    "- Motor cortex: Action-specific, lateralized\n",
    "- Reward system: Striatum for positive outcomes\n",
    "- Interpretable contrasts link behavior to brain\n",
    "\n",
    "**3. RL features capture hierarchical processing**\n",
    "- Early layers (conv1/conv2) â†’ Visual cortex\n",
    "- Middle layers (conv3/conv4) â†’ Motor/parietal  \n",
    "- Late layers (linear) â†’ Frontal/executive\n",
    "- Mirrors visual hierarchy (V1 â†’ V4 â†’ IT â†’ PFC)\n",
    "\n",
    "**4. Encoding models offer predictive power**\n",
    "- Explain variance beyond task-evoked responses\n",
    "- Capture latent variables (value, predictions)\n",
    "- Layer comparison reveals computational depth\n",
    "\n",
    "**5. Complementary methods**\n",
    "- GLM: Interpretable, hypothesis-testing\n",
    "- Encoding: Predictive, hypothesis-generating\n",
    "- Together: Comprehensive understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Extensions & Future Directions\n",
    "\n",
    "### Expand the Analysis\n",
    "\n",
    "**1. Multi-subject analysis**\n",
    "- Aggregate across 5 subjects\n",
    "- Group-level statistics\n",
    "- Inter-subject correlation\n",
    "- Individual differences in strategies\n",
    "\n",
    "**2. Out-of-distribution generalization**\n",
    "- Train on w1l1, w1l2, w4l1, w4l2, w5l1, w5l2\n",
    "- Test on w2l1, w3l1 (unseen levels)\n",
    "- Does brain encoding generalize?\n",
    "- RL agent transfer learning\n",
    "\n",
    "**3. MVPA & RSA**\n",
    "- **MVPA:** Decode actions from BOLD patterns\n",
    "- **RSA:** Correlate BOLD similarity with RL similarity\n",
    "- Compare representational geometry\n",
    "\n",
    "**4. Temporal dynamics**\n",
    "- Trial-by-trial encoding (LSS method)\n",
    "- Learning effects across sessions\n",
    "- Adaptation to level structure\n",
    "- Time-resolved decoding\n",
    "\n",
    "**5. Advanced encoding**\n",
    "- Voxel-wise Î± optimization\n",
    "- Elastic net regularization\n",
    "- Compare with DNN encoding toolboxes\n",
    "- Hierarchical models\n",
    "\n",
    "**6. Computational psychiatry**\n",
    "- Individual differences in RL parameters\n",
    "- Link brain encoding to behavioral metrics\n",
    "- Clinical populations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Thank You!\n",
    "\n",
    "## Resources & Next Steps\n",
    "\n",
    "### Code & Data\n",
    "\n",
    "**Repositories:**\n",
    "- **mario.tutorials** (this tutorial): Complete pipeline notebooks\n",
    "- **shinobi_fmri**: Session-level GLM analysis framework\n",
    "- **mario_generalization**: Full RL training and encoding\n",
    "\n",
    "**CNeuromod Data:**\n",
    "- Main portal: https://www.cneuromod.ca/\n",
    "- Documentation: https://docs.cneuromod.ca/\n",
    "- Mario dataset: BIDS format on Canadian Open Neuroscience Platform\n",
    "\n",
    "### Tutorial Notebooks\n",
    "\n",
    "1. **01_dataset_exploration.ipynb** - Detailed data exploration\n",
    "2. **02_session_glm.ipynb** - Complete GLM pipeline\n",
    "3. **03_brain_visualization.ipynb** - Advanced visualization\n",
    "4. **04_rl_agent.ipynb** - RL training and activation extraction\n",
    "5. **05_brain_encoding.ipynb** - Ridge regression encoding\n",
    "6. **06_summary.ipynb** - Synthesis and extensions\n",
    "\n",
    "### Questions?\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: center; font-size: 24px; padding: 30px;\">\n",
    "<b>Thank you for your attention!</b>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "*CNeuromod 2025 | Mario fMRI Tutorial*"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "rise": {
   "autolaunch": false,
   "enable_chalkboard": true,
   "footer": "<h3>CNeuromod 2025</h3>",
   "header": "<h2>Mario fMRI Tutorial</h2>",
   "scroll": true,
   "slideNumber": true,
   "theme": "simple",
   "transition": "fade"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
