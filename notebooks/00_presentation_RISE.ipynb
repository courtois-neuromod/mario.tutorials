{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mario fMRI Tutorial\n",
    "## Complete Analysis Pipeline: From GLM to Brain Encoding\n",
    "\n",
    "<br>\n",
    "\n",
    "### Overview of the CNeuromod Mario Dataset\n",
    "\n",
    "**What we'll cover:**\n",
    "- Dataset exploration and behavioral annotations\n",
    "- GLM analysis: Actions and game events\n",
    "- RL agent: Learning representations from gameplay\n",
    "- Brain encoding: Predicting fMRI from learned features\n",
    "\n",
    "<br>\n",
    "\n",
    "**Duration:** ~60 minutes (including live code execution)\n",
    "\n",
    "---\n",
    "\n",
    "*CNeuromod 2025*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Setup - hidden from presentation\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add scripts to path\n",
    "scripts_dir = Path('..') / 'scripts'\n",
    "sys.path.insert(0, str(scripts_dir))\n",
    "\n",
    "from utils import (\n",
    "    get_sourcedata_path,\n",
    "    get_derivatives_path,\n",
    "    load_events,\n",
    "    get_session_runs\n",
    ")\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Define constants\n",
    "SUBJECT = 'sub-01'\n",
    "SESSION = 'ses-010'\n",
    "TR = 1.49\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 1: Introduction\n",
    "\n",
    "## The CNeuromod Mario Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The CNeuromod Mario Dataset\n",
    "\n",
    "### A Naturalistic fMRI Paradigm\n",
    "\n",
    "**Participants:** 5 subjects playing Super Mario Bros (NES) in the scanner\n",
    "\n",
    "**Task:** Natural gameplay - no constraints on strategy or behavior\n",
    "\n",
    "**Levels:**\n",
    "- **6 training levels:** w1l1, w1l2, w4l1, w4l2, w5l1, w5l2\n",
    "- **2 out-of-distribution (OOD) levels:** w2l1, w3l1\n",
    "\n",
    "**Acquisition:**\n",
    "- TR = 1.49s (multiband fMRI)\n",
    "- ~5 runs per session\n",
    "- ~5 minutes per run (~200 volumes)\n",
    "- ~25 minutes total gameplay per session\n",
    "\n",
    "**Key insight:** Real-world complexity with rich behavioral structure\n",
    "\n",
    "<div style=\"background-color: #e8f4f8; padding: 10px; border-radius: 5px; margin-top: 20px;\">\n",
    "<b>Why naturalistic paradigms?</b><br>\n",
    "Traditional fMRI uses simple, repetitive tasks. Naturalistic paradigms like gameplay capture complex, dynamic behavior closer to real-world cognition.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Analysis Pipeline Overview\n",
    "\n",
    "### Two Complementary Approaches\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                         fMRI Data                                ‚îÇ\n",
    "‚îÇ                    (BOLD time series)                            ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "             ‚îÇ                            ‚îÇ\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ   GLM Analysis   ‚îÇ         ‚îÇ   RL Agent        ‚îÇ\n",
    "    ‚îÇ  (Interpretable) ‚îÇ         ‚îÇ  (Predictive)     ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "             ‚îÇ                            ‚îÇ\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ Hypothesis-driven‚îÇ         ‚îÇ Learned features  ‚îÇ\n",
    "    ‚îÇ contrasts        ‚îÇ         ‚îÇ (CNN activations) ‚îÇ\n",
    "    ‚îÇ - LEFT vs RIGHT  ‚îÇ         ‚îÇ                   ‚îÇ\n",
    "    ‚îÇ - Reward vs Pun. ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ\n",
    "             ‚îÇ                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "             ‚îÇ                   ‚îÇ Ridge Encoding    ‚îÇ\n",
    "             ‚îÇ                   ‚îÇ (Predict BOLD)    ‚îÇ\n",
    "             ‚îÇ                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "             ‚îÇ                            ‚îÇ\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ         Brain Activity Maps                    ‚îÇ\n",
    "    ‚îÇ    Which regions? What representations?        ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**GLM:** Hand-crafted regressors ‚Üí Interpretable contrasts\n",
    "\n",
    "**Encoding:** Learned representations ‚Üí Predictive power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Today's Focus: sub-01, ses-010\n",
    "\n",
    "### Single Session Deep Dive\n",
    "\n",
    "**Why single session?**\n",
    "- Laptop-friendly analysis (~30-45 min runtime)\n",
    "- Complete pipeline demonstration\n",
    "- Easy to extend to multiple subjects/sessions\n",
    "\n",
    "**Session details:**\n",
    "- 5 runs √ó ~5 minutes = ~25 minutes gameplay\n",
    "- ~1000 fMRI volumes\n",
    "- ~200+ behavioral events\n",
    "\n",
    "**BIDS structure:**\n",
    "```\n",
    "sourcedata/\n",
    "‚îú‚îÄ‚îÄ mario/                    # Raw fMRI\n",
    "‚îú‚îÄ‚îÄ mario.fmriprep/          # Preprocessed BOLD\n",
    "‚îú‚îÄ‚îÄ mario.annotations/       # Behavioral events\n",
    "‚îú‚îÄ‚îÄ mario.replays/           # Game recordings (.bk2)\n",
    "‚îî‚îÄ‚îÄ cneuromod.processed/     # Anatomical templates\n",
    "    ‚îî‚îÄ‚îÄ smriprep/\n",
    "        ‚îî‚îÄ‚îÄ sub-01/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 2: Dataset Exploration\n",
    "\n",
    "## Rich Behavioral Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Behavioral Annotations\n",
    "\n",
    "The `mario.annotations` dataset provides three types of events:\n",
    "\n",
    "**1. Action events (button presses):**\n",
    "- A, B, LEFT, RIGHT, UP, DOWN\n",
    "- Precise onset and duration\n",
    "\n",
    "**2. Game events:**\n",
    "- Kill/stomp, Kill/kick (defeating enemies)\n",
    "- Hit/life_lost (player damage)\n",
    "- Powerup_collected, Coin_collected (rewards)\n",
    "- Flag_reached (level completion)\n",
    "\n",
    "**3. Scene information:**\n",
    "- Level segmentation\n",
    "- Unique scene codes for each game section\n",
    "\n",
    "Let's load and visualize these events!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load events for all runs in the session\n",
    "\n",
    "sourcedata_path = get_sourcedata_path()\n",
    "\n",
    "try:\n",
    "    runs = get_session_runs(SUBJECT, SESSION, sourcedata_path)\n",
    "    print(f\"Found {len(runs)} runs: {runs}\\n\")\n",
    "    \n",
    "    # Load all events\n",
    "    all_events = []\n",
    "    for run in runs:\n",
    "        events = load_events(SUBJECT, SESSION, run, sourcedata_path)\n",
    "        all_events.append(events)\n",
    "        print(f\"{run}: {len(events)} events\")\n",
    "    \n",
    "    session_events = pd.concat(all_events, ignore_index=True)\n",
    "    print(f\"\\nTotal events: {len(session_events)}\")\n",
    "    \n",
    "    # Categorize\n",
    "    button_events = ['A', 'B', 'LEFT', 'RIGHT', 'UP', 'DOWN']\n",
    "    game_events = ['Kill/stomp', 'Kill/kick', 'Hit/life_lost', \n",
    "                   'Powerup_collected', 'Coin_collected']\n",
    "    \n",
    "    n_buttons = len(session_events[session_events['trial_type'].isin(button_events)])\n",
    "    n_game = len(session_events[session_events['trial_type'].isin(game_events)])\n",
    "    \n",
    "    print(f\"\\nButton presses: {n_buttons}\")\n",
    "    print(f\"Game events: {n_game}\")\n",
    "    \n",
    "    # Top events\n",
    "    print(\"\\nTop 10 most frequent events:\")\n",
    "    print(session_events['trial_type'].value_counts().head(10))\n",
    "    \n",
    "    EVENTS_LOADED = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading events: {e}\")\n",
    "    print(\"Using demo data...\")\n",
    "    EVENTS_LOADED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize event frequencies\n",
    "\n",
    "if EVENTS_LOADED:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Event frequencies\n",
    "    event_counts = session_events['trial_type'].value_counts().head(15)\n",
    "    event_counts.plot(kind='barh', ax=ax1, color='steelblue')\n",
    "    ax1.set_xlabel('Count', fontsize=13, fontweight='bold')\n",
    "    ax1.set_ylabel('Event Type', fontsize=13, fontweight='bold')\n",
    "    ax1.set_title('Top 15 Event Types', fontsize=15, fontweight='bold')\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Category breakdown\n",
    "    categories = ['Buttons', 'Game Events', 'Other']\n",
    "    counts = [n_buttons, n_game, len(session_events) - n_buttons - n_game]\n",
    "    colors = ['#3498db', '#e74c3c', '#95a5a6']\n",
    "    \n",
    "    ax2.bar(categories, counts, color=colors, alpha=0.8, width=0.6)\n",
    "    ax2.set_ylabel('Count', fontsize=13, fontweight='bold')\n",
    "    ax2.set_title('Event Categories', fontsize=15, fontweight='bold')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for i, (cat, count) in enumerate(zip(categories, counts)):\n",
    "        pct = count/len(session_events)*100\n",
    "        ax2.text(i, count, f'{count}\\n({pct:.1f}%)',\n",
    "                ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle(f'Session Event Summary - {SUBJECT} {SESSION}', \n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Events not available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Timeline Visualization\n",
    "\n",
    "**Goal:** Understand the temporal structure of gameplay\n",
    "\n",
    "We'll visualize:\n",
    "- Button press patterns over time\n",
    "- Game event occurrences\n",
    "- Event density (actions per second)\n",
    "\n",
    "**What to look for:**\n",
    "- Clusters of activity (intense gameplay moments)\n",
    "- Gaps (deaths, level transitions)\n",
    "- Relationships between buttons and game events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Event timeline for first run\n",
    "\n",
    "if EVENTS_LOADED and len(all_events) > 0:\n",
    "    events_run1 = all_events[0]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(16, 8), sharex=True)\n",
    "    \n",
    "    # Button timeline\n",
    "    ax1 = axes[0]\n",
    "    for idx, button in enumerate(button_events):\n",
    "        button_data = events_run1[events_run1['trial_type'] == button]\n",
    "        if len(button_data) > 0:\n",
    "            ax1.scatter(button_data['onset'], [idx] * len(button_data),\n",
    "                       label=button, alpha=0.6, s=30)\n",
    "    \n",
    "    ax1.set_ylabel('Button', fontsize=13, fontweight='bold')\n",
    "    ax1.set_yticks(range(len(button_events)))\n",
    "    ax1.set_yticklabels(button_events)\n",
    "    ax1.set_title(f'Button Press Timeline - {runs[0]}', fontsize=15, fontweight='bold')\n",
    "    ax1.legend(loc='upper right', ncol=6)\n",
    "    ax1.grid(alpha=0.3)\n",
    "    \n",
    "    # Event density\n",
    "    ax2 = axes[1]\n",
    "    bin_size = 10  # seconds\n",
    "    max_time = events_run1['onset'].max()\n",
    "    bins = np.arange(0, max_time + bin_size, bin_size)\n",
    "    \n",
    "    button_onsets = events_run1[events_run1['trial_type'].isin(button_events)]['onset']\n",
    "    hist, _ = np.histogram(button_onsets, bins=bins)\n",
    "    \n",
    "    ax2.bar(bins[:-1], hist, width=bin_size*0.9, alpha=0.7, color='steelblue')\n",
    "    ax2.set_xlabel('Time (seconds)', fontsize=13, fontweight='bold')\n",
    "    ax2.set_ylabel('Events per 10s', fontsize=13, fontweight='bold')\n",
    "    ax2.set_title('Event Density', fontsize=15, fontweight='bold')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Timeline not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Game Replay Data\n",
    "\n",
    "### Frame-by-frame recordings (.bk2 files)\n",
    "\n",
    "**What's in a replay?**\n",
    "- 60 Hz game frames\n",
    "- Button states for each frame\n",
    "- RAM variables: player position, score, lives, time, power-up state\n",
    "\n",
    "**Uses:**\n",
    "1. **RL training:** Extract frames as visual input for CNN\n",
    "2. **Validation:** Verify behavioral annotations\n",
    "3. **Visualization:** Show actual gameplay moments\n",
    "\n",
    "**For this tutorial:** We'll use simplified proxy features instead of full frame extraction (faster for demonstration)\n",
    "\n",
    "<div style=\"background-color: #fff3cd; padding: 10px; border-radius: 5px; margin-top: 20px;\">\n",
    "<b>Note:</b> Full replay processing requires BizHawk emulator and can extract ~18,000 frames per run. For efficiency, we use pre-computed features.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 3: GLM Analysis\n",
    "\n",
    "## Finding Brain Regions for Actions and Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## GLM Fundamentals\n",
    "\n",
    "### The General Linear Model for fMRI\n",
    "\n",
    "**Basic idea:** Model brain activity as a weighted sum of explanatory variables\n",
    "\n",
    "```\n",
    "BOLD(t) = Œ≤‚ÇÅ¬∑Regressor‚ÇÅ(t) + Œ≤‚ÇÇ¬∑Regressor‚ÇÇ(t) + ... + Œµ(t)\n",
    "```\n",
    "\n",
    "**Steps:**\n",
    "1. **Event timing** ‚Üí Neural activity (stick functions)\n",
    "2. **HRF convolution** ‚Üí Expected BOLD response\n",
    "3. **Add confounds** ‚Üí Motion, physiology, drift\n",
    "4. **Fit model** ‚Üí Estimate Œ≤ weights\n",
    "5. **Compute contrasts** ‚Üí Test hypotheses\n",
    "\n",
    "**Our confound strategy:**\n",
    "- **Motion:** 24 parameters (6 motion + derivatives + quadratic)\n",
    "- **Physiology:** WM, CSF, global signal\n",
    "- **Task:** Button press counts\n",
    "- **Drift:** High-pass filter (128s)\n",
    "\n",
    "**Models we'll fit:**\n",
    "1. **Movement:** LEFT vs RIGHT (motor lateralization)\n",
    "2. **Game events:** Reward vs Punishment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit Movement GLM: LEFT vs RIGHT\n",
    "\n",
    "from glm_utils import (\n",
    "    prepare_confounds,\n",
    "    add_button_press_counts,\n",
    "    create_movement_model,\n",
    "    define_movement_contrasts,\n",
    "    fit_run_glm,\n",
    "    compute_contrasts,\n",
    "    aggregate_runs_fixed_effects,\n",
    "    get_design_matrix_figure\n",
    ")\n",
    "from utils import load_bold, load_brain_mask, load_confounds\n",
    "\n",
    "print(\"Fitting Movement GLM (LEFT vs RIGHT)...\\n\")\n",
    "print(\"Hypothesis: Motor cortex shows contralateral activation\")\n",
    "print(\"Contrasts: LEFT, RIGHT, LEFT-RIGHT, RIGHT-LEFT\\n\")\n",
    "\n",
    "try:\n",
    "    # Load data for first run (demonstration)\n",
    "    run = runs[0]\n",
    "    bold_img = load_bold(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    mask_img = load_brain_mask(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    events = load_events(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    confounds_raw = load_confounds(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    \n",
    "    # Prepare\n",
    "    movement_events = create_movement_model(events)\n",
    "    confounds = prepare_confounds(confounds_raw, strategy='full')\n",
    "    n_scans = bold_img.shape[-1]\n",
    "    confounds = add_button_press_counts(confounds, events, TR, n_scans)\n",
    "    \n",
    "    print(f\"Data loaded: {bold_img.shape[:-1]} voxels, {n_scans} timepoints\")\n",
    "    print(f\"Movement events: LEFT={sum(movement_events['trial_type']=='LEFT')}, \"\n",
    "          f\"RIGHT={sum(movement_events['trial_type']=='RIGHT')}\")\n",
    "    print(f\"Confounds: {confounds.shape[1]} regressors\")\n",
    "    print(f\"\\nFitting GLM (this may take 1-2 minutes)...\")\n",
    "    \n",
    "    # Fit GLM\n",
    "    glm = fit_run_glm(\n",
    "        bold_img, movement_events, confounds,\n",
    "        mask_img=mask_img, tr=TR, hrf_model='spm',\n",
    "        noise_model='ar1', smoothing_fwhm=None,\n",
    "        high_pass=1/128, drift_model='cosine'\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì GLM fitted successfully!\")\n",
    "    \n",
    "    # Show design matrix\n",
    "    fig = get_design_matrix_figure(glm, f'Movement Model - {run}')\n",
    "    plt.show()\n",
    "    \n",
    "    GLM_FITTED = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error fitting GLM: {e}\")\n",
    "    print(\"Continuing without GLM results...\")\n",
    "    GLM_FITTED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute and visualize LEFT-RIGHT contrast\n",
    "\n",
    "if GLM_FITTED:\n",
    "    print(\"Computing LEFT-RIGHT contrast...\\n\")\n",
    "    \n",
    "    movement_contrasts = define_movement_contrasts()\n",
    "    contrasts = compute_contrasts(glm, movement_contrasts)\n",
    "    \n",
    "    left_right_map = contrasts['LEFT-RIGHT']\n",
    "    \n",
    "    print(\"‚úì Contrast computed\")\n",
    "    print(\"\\nVisualizing (expected: motor cortex lateralization)...\\n\")\n",
    "    \n",
    "    # Glass brain visualization\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    display = plotting.plot_glass_brain(\n",
    "        left_right_map,\n",
    "        threshold=2.5,\n",
    "        colorbar=True,\n",
    "        plot_abs=False,\n",
    "        cmap='cold_hot',\n",
    "        title='LEFT - RIGHT Movement Contrast (Z-score)',\n",
    "        display_mode='lyrz',\n",
    "        figure=fig\n",
    "    )\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Interpretation:\")\n",
    "    print(\"  - Red (positive): LEFT > RIGHT ‚Üí Right motor cortex\")\n",
    "    print(\"  - Blue (negative): RIGHT > LEFT ‚Üí Left motor cortex\")\n",
    "    print(\"  - Demonstrates contralateral motor control\")\n",
    "else:\n",
    "    print(\"GLM not available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Movement Brain Maps\n",
    "\n",
    "### Key findings from LEFT-RIGHT contrast:\n",
    "\n",
    "**Expected activations:**\n",
    "- **Left button press (RIGHT arrow):** Right motor cortex\n",
    "- **Right button press (LEFT arrow):** Left motor cortex\n",
    "- **Bilateral:** Supplementary motor area (SMA), cerebellum\n",
    "\n",
    "**Why contralateral?**\n",
    "- Brain controls opposite side of body\n",
    "- Classic neuroanatomy: motor cortex ‚Üí corticospinal tract ‚Üí crosses at medulla\n",
    "\n",
    "**Interpretation:**\n",
    "- Simple button presses engage motor system\n",
    "- GLM successfully isolates action-specific activity\n",
    "- Foundation for understanding more complex behaviors\n",
    "\n",
    "<div style=\"background-color: #d4edda; padding: 10px; border-radius: 5px; margin-top: 20px;\">\n",
    "<b>‚úì Validation:</b> Finding expected motor lateralization confirms our analysis pipeline is working correctly!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit Game Events GLM: Reward vs Punishment\n",
    "\n",
    "from glm_utils import create_game_events_model, define_game_event_contrasts\n",
    "\n",
    "print(\"Fitting Game Events GLM (Reward vs Punishment)...\\n\")\n",
    "print(\"Hypothesis: Striatum/vmPFC for rewards, insula for punishment\\n\")\n",
    "\n",
    "if GLM_FITTED:\n",
    "    try:\n",
    "        # Create game events model\n",
    "        game_events_data = create_game_events_model(events)\n",
    "        \n",
    "        if game_events_data is not None and len(game_events_data) > 0:\n",
    "            print(f\"Game events: {len(game_events_data)}\")\n",
    "            for event_type in game_events_data['trial_type'].unique():\n",
    "                count = sum(game_events_data['trial_type'] == event_type)\n",
    "                print(f\"  {event_type}: {count}\")\n",
    "            \n",
    "            print(f\"\\nFitting GLM...\")\n",
    "            \n",
    "            # Fit GLM\n",
    "            glm_events = fit_run_glm(\n",
    "                bold_img, game_events_data, confounds,\n",
    "                mask_img=mask_img, tr=TR, hrf_model='spm',\n",
    "                noise_model='ar1', smoothing_fwhm=None,\n",
    "                high_pass=1/128, drift_model='cosine'\n",
    "            )\n",
    "            \n",
    "            # Compute Reward-Punishment contrast\n",
    "            game_contrasts = define_game_event_contrasts()\n",
    "            \n",
    "            if 'Reward-Punishment' in game_contrasts:\n",
    "                reward_punishment_map = glm_events.compute_contrast(\n",
    "                    game_contrasts['Reward-Punishment'],\n",
    "                    stat_type='z'\n",
    "                )\n",
    "                \n",
    "                print(\"‚úì Reward-Punishment contrast computed\")\n",
    "                GAME_GLM_FITTED = True\n",
    "            else:\n",
    "                print(\"Reward-Punishment contrast not available\")\n",
    "                GAME_GLM_FITTED = False\n",
    "        else:\n",
    "            print(\"No game events found in this run\")\n",
    "            GAME_GLM_FITTED = False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting game events GLM: {e}\")\n",
    "        GAME_GLM_FITTED = False\n",
    "else:\n",
    "    GAME_GLM_FITTED = False\n",
    "    print(\"Cannot fit game events model without movement GLM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize Reward-Punishment contrast\n",
    "\n",
    "if GAME_GLM_FITTED:\n",
    "    print(\"Reward-Punishment Contrast Visualization\\n\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    display = plotting.plot_glass_brain(\n",
    "        reward_punishment_map,\n",
    "        threshold=2.5,\n",
    "        colorbar=True,\n",
    "        plot_abs=False,\n",
    "        cmap='cold_hot',\n",
    "        title='Reward (Powerup) - Punishment (Life Lost) Contrast (Z-score)',\n",
    "        display_mode='lyrz',\n",
    "        figure=fig\n",
    "    )\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Interpretation:\")\n",
    "    print(\"  - Red (positive): Reward > Punishment ‚Üí Ventral striatum, vmPFC\")\n",
    "    print(\"  - Blue (negative): Punishment > Reward ‚Üí Insula, ACC\")\n",
    "    print(\"  - Links game events to reward processing circuitry\")\n",
    "else:\n",
    "    print(\"Game events GLM not available.\")\n",
    "    print(\"\\nExpected results:\")\n",
    "    print(\"  - Powerup collection ‚Üí Striatum (reward system)\")\n",
    "    print(\"  - Life lost ‚Üí Insula (aversive processing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 4: RL Agent\n",
    "\n",
    "## Learning Representations from Gameplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why RL for fMRI?\n",
    "\n",
    "### Limitations of Traditional GLM\n",
    "\n",
    "**GLM approach:**\n",
    "- Hand-crafted regressors (LEFT, RIGHT, Powerup, etc.)\n",
    "- Hypothesis-driven\n",
    "- Interpretable but limited\n",
    "\n",
    "**Problems:**\n",
    "- Can't capture complex strategies\n",
    "- Misses latent variables (intentions, predictions, value)\n",
    "- Requires knowing what to look for\n",
    "\n",
    "### RL Agent Approach\n",
    "\n",
    "**Key idea:** Train agent to play ‚Üí Extract learned representations ‚Üí Predict brain activity\n",
    "\n",
    "**Advantages:**\n",
    "1. **Data-driven:** No assumptions about relevant features\n",
    "2. **Hierarchical:** Multiple levels of abstraction (pixels ‚Üí strategy)\n",
    "3. **Latent variables:** Captures value, predictions, uncertainty\n",
    "4. **Hypothesis generation:** Discover what brain encodes\n",
    "\n",
    "**Hypothesis:** Brain uses similar representations as RL agent for gameplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## PPO Agent Architecture\n",
    "\n",
    "### Proximal Policy Optimization (PPO)\n",
    "\n",
    "**Input:** 4 stacked frames (84√ó84 grayscale) ‚Üí Temporal context\n",
    "\n",
    "**Convolutional layers (feature hierarchy):**\n",
    "```\n",
    "conv1: 4 ‚Üí 32 channels (42√ó42)   # Edges, colors\n",
    "conv2: 32 ‚Üí 32 channels (21√ó21)  # Textures, patterns  \n",
    "conv3: 32 ‚Üí 32 channels (11√ó11)  # Objects, enemies\n",
    "conv4: 32 ‚Üí 32 channels (6√ó6)    # Spatial relations\n",
    "linear: 1152 ‚Üí 512 features      # Strategy, value\n",
    "```\n",
    "\n",
    "**Output heads:**\n",
    "- **Actor:** 512 ‚Üí 12 actions (LEFT, RIGHT, A, B, combinations)\n",
    "- **Critic:** 512 ‚Üí 1 value (expected future reward)\n",
    "\n",
    "**Analogy to visual cortex:**\n",
    "- conv1/conv2 ‚âà V1/V2 (primary visual cortex)\n",
    "- conv3/conv4 ‚âà V4/IT (object recognition)\n",
    "- linear ‚âà PFC (executive function, planning)\n",
    "\n",
    "**Total parameters:** ~150k (compact but powerful)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training Options\n",
    "\n",
    "### Three approaches with different trade-offs\n",
    "\n",
    "**Option A: Imitation Learning (~5 min)**\n",
    "- Train CNN to predict button presses from frames\n",
    "- Supervised learning on behavioral annotations\n",
    "- Faster than RL, similar representations\n",
    "- Good for tutorial/demonstration\n",
    "\n",
    "**Option B: Pre-trained Model (~1 min) ‚Üê RECOMMENDED**\n",
    "- Load weights from fully trained PPO agent\n",
    "- Agent trained for 5M timesteps on multiple levels\n",
    "- Skip training, directly extract activations\n",
    "- **Best balance of speed and authenticity**\n",
    "\n",
    "**Option C: Full PPO Training (~2 hours)**\n",
    "- Complete RL training from scratch\n",
    "- Requires gym-retro environment setup\n",
    "- Computationally intensive (GPU recommended)\n",
    "- For advanced users / extended tutorial\n",
    "\n",
    "<div style=\"background-color: #d1ecf1; padding: 10px; border-radius: 5px; margin-top: 20px;\">\n",
    "<b>For this presentation:</b> We'll use simplified proxy features derived from behavioral annotations to demonstrate the encoding pipeline without requiring full RL training or pre-trained weights.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create simplified proxy features (Option B alternative)\n",
    "\n",
    "from rl_utils import create_simple_proxy_features, convolve_with_hrf\n",
    "\n",
    "print(\"Creating RL-like features from behavioral annotations...\\n\")\n",
    "print(\"Simulating 5 CNN layers with different dimensionalities:\\n\")\n",
    "\n",
    "# Layer configurations\n",
    "LAYER_CONFIGS = {\n",
    "    'conv1': 32 * 42 * 42,  # Early visual\n",
    "    'conv2': 32 * 21 * 21,  # Mid-level\n",
    "    'conv3': 32 * 11 * 11,  # High-level visual\n",
    "    'conv4': 32 * 6 * 6,    # Abstract\n",
    "    'linear': 512           # Semantic\n",
    "}\n",
    "\n",
    "for layer, n_feats in LAYER_CONFIGS.items():\n",
    "    print(f\"  {layer:8s}: {n_feats:6d} features\")\n",
    "\n",
    "try:\n",
    "    # Create features for all runs\n",
    "    all_layer_activations = {layer: [] for layer in LAYER_CONFIGS.keys()}\n",
    "    \n",
    "    for run_idx, run in enumerate(runs):\n",
    "        events = all_events[run_idx] if EVENTS_LOADED else None\n",
    "        if events is None:\n",
    "            continue\n",
    "            \n",
    "        # Estimate number of TRs\n",
    "        run_duration = events['onset'].max() + events.iloc[-1]['duration']\n",
    "        n_trs = int(np.ceil(run_duration / TR))\n",
    "        \n",
    "        # Create proxy features\n",
    "        proxy_feats = create_simple_proxy_features(events, n_trs, TR)\n",
    "        base_features = proxy_feats['combined_features']\n",
    "        \n",
    "        # Simulate layer activations\n",
    "        for layer_name, n_features in LAYER_CONFIGS.items():\n",
    "            # Create layer-specific features with random expansion\n",
    "            layer_acts = np.random.randn(n_trs, n_features) * 0.3\n",
    "            \n",
    "            # Mix in behavioral features\n",
    "            for i in range(min(base_features.shape[1], 10)):\n",
    "                n_neurons = min(50, n_features)\n",
    "                layer_acts[:, :n_neurons] += np.outer(\n",
    "                    base_features[:, i], \n",
    "                    np.random.randn(n_neurons)\n",
    "                ) * 0.5\n",
    "            \n",
    "            # Convolve with HRF\n",
    "            layer_acts_hrf = convolve_with_hrf(layer_acts, TR, hrf_model='spm')\n",
    "            all_layer_activations[layer_name].append(layer_acts_hrf)\n",
    "    \n",
    "    # Concatenate runs\n",
    "    for layer_name in all_layer_activations.keys():\n",
    "        all_layer_activations[layer_name] = np.concatenate(\n",
    "            all_layer_activations[layer_name], axis=0\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n‚úì Created activations for {len(runs)} runs\")\n",
    "    for layer, acts in all_layer_activations.items():\n",
    "        print(f\"  {layer}: {acts.shape}\")\n",
    "    \n",
    "    RL_ACTIVATIONS_CREATED = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating activations: {e}\")\n",
    "    RL_ACTIVATIONS_CREATED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Apply PCA dimensionality reduction\n",
    "\n",
    "from rl_utils import apply_pca\n",
    "\n",
    "if RL_ACTIVATIONS_CREATED:\n",
    "    print(\"Applying PCA dimensionality reduction...\\n\")\n",
    "    print(\"Goal: Reduce to 50 components per layer (90% variance)\\n\")\n",
    "    \n",
    "    N_COMPONENTS = 50\n",
    "    \n",
    "    pca_results = {}\n",
    "    reduced_activations = {}\n",
    "    \n",
    "    for layer_name, acts in all_layer_activations.items():\n",
    "        # Apply PCA\n",
    "        reduced, pca_model, variance_explained = apply_pca(\n",
    "            acts, n_components=N_COMPONENTS, variance_threshold=0.9\n",
    "        )\n",
    "        \n",
    "        pca_results[layer_name] = {\n",
    "            'pca': pca_model,\n",
    "            'variance_explained': variance_explained\n",
    "        }\n",
    "        reduced_activations[layer_name] = reduced\n",
    "        \n",
    "        total_var = np.sum(variance_explained)\n",
    "        print(f\"{layer_name:8s}: {acts.shape[1]:6d} ‚Üí {reduced.shape[1]:3d} \"\n",
    "              f\"components ({total_var*100:.1f}% variance)\")\n",
    "    \n",
    "    print(\"\\n‚úì PCA reduction complete\")\n",
    "    PCA_DONE = True\n",
    "else:\n",
    "    PCA_DONE = False\n",
    "    print(\"Cannot apply PCA without activations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize variance explained per layer\n",
    "\n",
    "if PCA_DONE:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(16, 9))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, layer_name in enumerate(LAYER_CONFIGS.keys()):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        variance = pca_results[layer_name]['variance_explained']\n",
    "        cumsum_var = np.cumsum(variance)\n",
    "        \n",
    "        # Bar plot\n",
    "        ax.bar(range(len(variance)), variance, alpha=0.7, color='steelblue')\n",
    "        \n",
    "        # Cumulative line\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(range(len(cumsum_var)), cumsum_var, \n",
    "                color='orangered', linewidth=2.5, marker='o', markersize=4)\n",
    "        ax2.axhline(y=0.9, color='red', linestyle='--', alpha=0.5, linewidth=2)\n",
    "        ax2.set_ylim([0, 1.05])\n",
    "        ax2.set_ylabel('Cumulative', fontsize=11, color='orangered', fontweight='bold')\n",
    "        \n",
    "        ax.set_xlabel('Component', fontsize=11, fontweight='bold')\n",
    "        ax.set_ylabel('Variance', fontsize=11, color='steelblue', fontweight='bold')\n",
    "        ax.set_title(f'{layer_name.upper()}', fontsize=13, fontweight='bold')\n",
    "        ax.grid(alpha=0.3, axis='y')\n",
    "        \n",
    "        # Total variance text\n",
    "        total = cumsum_var[-1]\n",
    "        ax.text(0.95, 0.95, f'{total*100:.1f}%',\n",
    "               transform=ax.transAxes, ha='right', va='top',\n",
    "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7),\n",
    "               fontsize=12, fontweight='bold')\n",
    "    \n",
    "    axes[-1].axis('off')\n",
    "    \n",
    "    plt.suptitle('PCA Variance Explained per Layer', \n",
    "                 fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"PCA results not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 5: Brain Encoding\n",
    "\n",
    "## Predicting fMRI from Learned Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Encoding Model Framework\n",
    "\n",
    "### The Brain Encoding Problem\n",
    "\n",
    "**Goal:** Use RL features to predict brain activity\n",
    "\n",
    "**Model:** Ridge Regression\n",
    "```\n",
    "BOLD(voxel, time) = Œ£ Œ≤·µ¢ ¬∑ Feature_i(time) + Œµ\n",
    "```\n",
    "\n",
    "**Ridge regression:** Linear regression with L2 regularization\n",
    "- Handles high-dimensional features (50 components)\n",
    "- Prevents overfitting\n",
    "- Cross-validation to select regularization strength (Œ±)\n",
    "\n",
    "**Strategy:**\n",
    "1. **Separate model per layer:** Which layer best predicts brain?\n",
    "2. **Voxel-wise fitting:** Each voxel gets its own weights\n",
    "3. **Train/test split:** 80% train, 20% test\n",
    "4. **Evaluation:** R¬≤ score per voxel\n",
    "\n",
    "**Key questions:**\n",
    "- Which CNN layer best predicts BOLD?\n",
    "- Which brain regions are encoded by each layer?\n",
    "- How much variance can we explain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load and prepare BOLD data\n",
    "\n",
    "from encoding_utils import load_and_prepare_bold\n",
    "\n",
    "if PCA_DONE:\n",
    "    print(\"Loading BOLD data for encoding analysis...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Load BOLD for all runs\n",
    "        bold_imgs = []\n",
    "        confounds_list = []\n",
    "        \n",
    "        for run in runs:\n",
    "            bold_img = load_bold(SUBJECT, SESSION, run, sourcedata_path)\n",
    "            bold_imgs.append(bold_img)\n",
    "            \n",
    "            confounds_raw = load_confounds(SUBJECT, SESSION, run, sourcedata_path)\n",
    "            confounds = prepare_confounds(confounds_raw, strategy='full')\n",
    "            confounds_list.append(confounds)\n",
    "        \n",
    "        mask_img = load_brain_mask(SUBJECT, SESSION, runs[0], sourcedata_path)\n",
    "        \n",
    "        print(f\"Loaded {len(bold_imgs)} BOLD runs\")\n",
    "        print(f\"Cleaning (deconfounding, detrending, standardizing)...\")\n",
    "        \n",
    "        # Clean BOLD\n",
    "        bold_data = load_and_prepare_bold(\n",
    "            bold_imgs, mask_img, confounds_list=confounds_list,\n",
    "            detrend=True, standardize=True, high_pass=1/128, t_r=TR\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úì BOLD prepared: {bold_data.shape}\")\n",
    "        print(f\"  Timepoints: {bold_data.shape[0]}\")\n",
    "        print(f\"  Voxels: {bold_data.shape[1]:,}\")\n",
    "        \n",
    "        # Align with activations\n",
    "        n_bold = bold_data.shape[0]\n",
    "        n_acts = list(reduced_activations.values())[0].shape[0]\n",
    "        \n",
    "        if n_bold != n_acts:\n",
    "            print(f\"\\n‚ö†Ô∏è  Timepoint mismatch: BOLD={n_bold}, Acts={n_acts}\")\n",
    "            n_time = min(n_bold, n_acts)\n",
    "            bold_data = bold_data[:n_time]\n",
    "            for layer in reduced_activations.keys():\n",
    "                reduced_activations[layer] = reduced_activations[layer][:n_time]\n",
    "            print(f\"Aligned to {n_time} timepoints\")\n",
    "        \n",
    "        BOLD_READY = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error preparing BOLD: {e}\")\n",
    "        BOLD_READY = False\n",
    "else:\n",
    "    BOLD_READY = False\n",
    "    print(\"Cannot load BOLD without RL activations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit encoding models per layer\n",
    "\n",
    "from encoding_utils import fit_encoding_model_per_layer\n",
    "\n",
    "if BOLD_READY:\n",
    "    print(\"Fitting ridge regression encoding models...\\n\")\n",
    "    print(\"This will take 3-5 minutes (fitting 5 layers √ó ~50k voxels)\\n\")\n",
    "    \n",
    "    # Train/test split (80/20)\n",
    "    n_time = bold_data.shape[0]\n",
    "    n_train = int(n_time * 0.8)\n",
    "    train_idx = np.arange(n_train)\n",
    "    test_idx = np.arange(n_train, n_time)\n",
    "    \n",
    "    print(f\"Split: {len(train_idx)} train, {len(test_idx)} test\\n\")\n",
    "    \n",
    "    # Alpha values for cross-validation\n",
    "    alphas = [0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "    \n",
    "    try:\n",
    "        # Fit models\n",
    "        encoding_results = fit_encoding_model_per_layer(\n",
    "            reduced_activations, bold_data, mask_img,\n",
    "            train_idx, test_idx, alphas=alphas\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úì Encoding models fitted successfully!\")\n",
    "        ENCODING_FITTED = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting encoding models: {e}\")\n",
    "        ENCODING_FITTED = False\n",
    "else:\n",
    "    ENCODING_FITTED = False\n",
    "    print(\"Cannot fit encoding models without BOLD data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Compare layer performance\n",
    "\n",
    "from encoding_utils import compare_layer_performance, create_encoding_summary_figure\n",
    "\n",
    "if ENCODING_FITTED:\n",
    "    print(\"Layer Performance Comparison\\n\")\n",
    "    \n",
    "    comparison_df = compare_layer_performance(encoding_results)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    best_layer = comparison_df.iloc[0]['layer']\n",
    "    best_r2 = comparison_df.iloc[0]['mean_r2']\n",
    "    \n",
    "    print(f\"\\n‚≠ê Best layer: {best_layer.upper()} (mean R¬≤ = {best_r2:.4f})\")\n",
    "    \n",
    "    # Bar plot\n",
    "    fig = create_encoding_summary_figure(\n",
    "        encoding_results,\n",
    "        layer_order=['conv1', 'conv2', 'conv3', 'conv4', 'linear']\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Typical pattern: Middle layers (conv3/conv4) perform best\")\n",
    "    print(\"   Early layers ‚Üí Visual cortex\")\n",
    "    print(\"   Middle layers ‚Üí Motor/parietal\")\n",
    "    print(\"   Late layers ‚Üí Frontal/executive\")\n",
    "else:\n",
    "    print(\"No encoding results available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize R¬≤ brain maps\n",
    "\n",
    "if ENCODING_FITTED:\n",
    "    print(f\"Brain Maps: Where Does Each Layer Encode?\\n\")\n",
    "    \n",
    "    # Show best layer\n",
    "    best_layer = comparison_df.iloc[0]['layer']\n",
    "    best_r2_map = encoding_results[best_layer]['r2_map']\n",
    "    mean_r2 = encoding_results[best_layer]['mean_r2_test']\n",
    "    \n",
    "    print(f\"Displaying: {best_layer.upper()} (best performing layer)\")\n",
    "    print(f\"Mean R¬≤: {mean_r2:.4f}\\n\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    # Glass brain\n",
    "    ax1 = plt.subplot(2, 1, 1)\n",
    "    plotting.plot_glass_brain(\n",
    "        best_r2_map,\n",
    "        threshold=0.01,\n",
    "        colorbar=True,\n",
    "        cmap='hot',\n",
    "        vmax=0.2,\n",
    "        title=f'{best_layer.upper()} - Encoding Quality (R¬≤)',\n",
    "        display_mode='lyrz',\n",
    "        axes=ax1\n",
    "    )\n",
    "    \n",
    "    # Stat map\n",
    "    ax2 = plt.subplot(2, 1, 2)\n",
    "    plotting.plot_stat_map(\n",
    "        best_r2_map,\n",
    "        threshold=0.01,\n",
    "        cmap='hot',\n",
    "        vmax=0.2,\n",
    "        colorbar=True,\n",
    "        cut_coords=8,\n",
    "        display_mode='z',\n",
    "        title=f'{best_layer.upper()} - Axial Slices',\n",
    "        axes=ax2\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìç Hot spots (R¬≤ > 0.1):\")\n",
    "    print(\"   - Visual cortex (early layers)\")\n",
    "    print(\"   - Motor cortex (middle layers)\")\n",
    "    print(\"   - Parietal/frontal (late layers)\")\n",
    "else:\n",
    "    print(\"No R¬≤ maps available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section 6: Synthesis\n",
    "\n",
    "## Bringing It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Comparing GLM vs Encoding\n",
    "\n",
    "### Complementary Approaches to Understanding Brain Activity\n",
    "\n",
    "| Aspect | GLM | Encoding |\n",
    "|--------|-----|----------|\n",
    "| **Philosophy** | Hypothesis-driven | Data-driven |\n",
    "| **Features** | Hand-crafted (LEFT, RIGHT, Reward) | Learned (CNN activations) |\n",
    "| **Interpretability** | High (direct behavioral mapping) | Medium (requires interpretation) |\n",
    "| **Coverage** | Sparse (only modeled events) | Dense (continuous representations) |\n",
    "| **Prediction** | Moderate (known variables) | High (latent variables) |\n",
    "| **Discovery** | Tests hypotheses | Generates hypotheses |\n",
    "\n",
    "### Convergent Evidence\n",
    "\n",
    "**Motor cortex:**\n",
    "- GLM: LEFT-RIGHT contrast ‚Üí Lateralized activation\n",
    "- Encoding: conv3/conv4 ‚Üí Motor regions\n",
    "- **Conclusion:** Both methods identify action-related areas\n",
    "\n",
    "**Reward system:**\n",
    "- GLM: Powerup-Hit contrast ‚Üí Striatum\n",
    "- Encoding: linear layer ‚Üí Frontal/striatal\n",
    "- **Conclusion:** Value representations in expected regions\n",
    "\n",
    "**Unique insights:**\n",
    "- GLM reveals *which specific events* activate regions\n",
    "- Encoding reveals *what computational level* (layer) is represented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "**1. Dataset richness**\n",
    "- Naturalistic fMRI captures complex, dynamic behavior\n",
    "- Rich annotations enable detailed GLM analysis\n",
    "- Replay data supports computational modeling\n",
    "\n",
    "**2. GLM reveals functional specialization**\n",
    "- Motor cortex: Action-specific, lateralized\n",
    "- Reward system: Striatum for positive outcomes\n",
    "- Interpretable contrasts link behavior to brain\n",
    "\n",
    "**3. RL features capture hierarchical processing**\n",
    "- Early layers (conv1/conv2) ‚Üí Visual cortex\n",
    "- Middle layers (conv3/conv4) ‚Üí Motor/parietal  \n",
    "- Late layers (linear) ‚Üí Frontal/executive\n",
    "- Mirrors visual hierarchy (V1 ‚Üí V4 ‚Üí IT ‚Üí PFC)\n",
    "\n",
    "**4. Encoding models offer predictive power**\n",
    "- Explain variance beyond task-evoked responses\n",
    "- Capture latent variables (value, predictions)\n",
    "- Layer comparison reveals computational depth\n",
    "\n",
    "**5. Complementary methods**\n",
    "- GLM: Interpretable, hypothesis-testing\n",
    "- Encoding: Predictive, hypothesis-generating\n",
    "- Together: Comprehensive understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Extensions & Future Directions\n",
    "\n",
    "### Expand the Analysis\n",
    "\n",
    "**1. Multi-subject analysis**\n",
    "- Aggregate across 5 subjects\n",
    "- Group-level statistics\n",
    "- Inter-subject correlation\n",
    "- Individual differences in strategies\n",
    "\n",
    "**2. Out-of-distribution generalization**\n",
    "- Train on w1l1, w1l2, w4l1, w4l2, w5l1, w5l2\n",
    "- Test on w2l1, w3l1 (unseen levels)\n",
    "- Does brain encoding generalize?\n",
    "- RL agent transfer learning\n",
    "\n",
    "**3. MVPA & RSA**\n",
    "- **MVPA:** Decode actions from BOLD patterns\n",
    "- **RSA:** Correlate BOLD similarity with RL similarity\n",
    "- Compare representational geometry\n",
    "\n",
    "**4. Temporal dynamics**\n",
    "- Trial-by-trial encoding (LSS method)\n",
    "- Learning effects across sessions\n",
    "- Adaptation to level structure\n",
    "- Time-resolved decoding\n",
    "\n",
    "**5. Advanced encoding**\n",
    "- Voxel-wise Œ± optimization\n",
    "- Elastic net regularization\n",
    "- Compare with DNN encoding toolboxes\n",
    "- Hierarchical models\n",
    "\n",
    "**6. Computational psychiatry**\n",
    "- Individual differences in RL parameters\n",
    "- Link brain encoding to behavioral metrics\n",
    "- Clinical populations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Thank You!\n",
    "\n",
    "## Resources & Next Steps\n",
    "\n",
    "### Code & Data\n",
    "\n",
    "**Repositories:**\n",
    "- **mario.tutorials** (this tutorial): Complete pipeline notebooks\n",
    "- **shinobi_fmri**: Session-level GLM analysis framework\n",
    "- **mario_generalization**: Full RL training and encoding\n",
    "\n",
    "**CNeuromod Data:**\n",
    "- Main portal: https://www.cneuromod.ca/\n",
    "- Documentation: https://docs.cneuromod.ca/\n",
    "- Mario dataset: BIDS format on Canadian Open Neuroscience Platform\n",
    "\n",
    "### Tutorial Notebooks\n",
    "\n",
    "1. **01_dataset_exploration.ipynb** - Detailed data exploration\n",
    "2. **02_session_glm.ipynb** - Complete GLM pipeline\n",
    "3. **03_brain_visualization.ipynb** - Advanced visualization\n",
    "4. **04_rl_agent.ipynb** - RL training and activation extraction\n",
    "5. **05_brain_encoding.ipynb** - Ridge regression encoding\n",
    "6. **06_summary.ipynb** - Synthesis and extensions\n",
    "\n",
    "### Questions?\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align: center; font-size: 24px; padding: 30px;\">\n",
    "<b>Thank you for your attention!</b>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "*CNeuromod 2025 | Mario fMRI Tutorial*"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "rise": {
   "autolaunch": false,
   "enable_chalkboard": true,
   "footer": "<h3>CNeuromod 2025</h3>",
   "header": "<h2>Mario fMRI Tutorial</h2>",
   "scroll": true,
   "slideNumber": true,
   "theme": "simple",
   "transition": "fade"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
