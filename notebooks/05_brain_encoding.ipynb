{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Brain Encoding Analysis\n",
    "\n",
    "**Duration**: ~15 minutes\n",
    "\n",
    "**Objective**: Use RL agent representations to predict brain activity via ridge regression\n",
    "\n",
    "In this notebook, we'll:\n",
    "- Load and prepare BOLD data (deconfounding, masking)\n",
    "- Fit ridge regression models per CNN layer\n",
    "- Evaluate with cross-validation\n",
    "- Compare layer performance\n",
    "- Create R² brain maps showing encoding quality\n",
    "- Visualize which brain regions are best predicted by each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nibabel as nib\n",
    "from nilearn import plotting\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "src_dir = Path('..') / 'src'\n",
    "sys.path.insert(0, str(src_dir))\n",
    "\n",
    "from utils import (\n",
    "    get_sourcedata_path,\n",
    "    get_derivatives_path,\n",
    "    load_bold,\n",
    "    load_brain_mask,\n",
    "    load_confounds,\n",
    "    get_session_runs,\n",
    "    create_output_dir\n",
    ")\n",
    "\n",
    "from glm_utils import prepare_confounds\n",
    "\n",
    "from encoding_utils import (\n",
    "    RidgeEncodingModel,\n",
    "    load_and_prepare_bold,\n",
    "    fit_encoding_model_per_layer,\n",
    "    compare_layer_performance,\n",
    "    create_encoding_summary_figure\n",
    ")\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"Imports complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define subject and session\n",
    "SUBJECT = 'sub-01'\n",
    "SESSION = 'ses-010'\n",
    "TR = 1.49  # seconds\n",
    "\n",
    "# Get paths\n",
    "sourcedata_path = get_sourcedata_path()\n",
    "derivatives_path = get_derivatives_path()\n",
    "encoding_output_dir = create_output_dir(SUBJECT, SESSION, 'encoding')\n",
    "\n",
    "print(f\"Analyzing: {SUBJECT}, {SESSION}\")\n",
    "print(f\"TR: {TR}s\")\n",
    "print(f\"Output directory: {encoding_output_dir}\")\n",
    "\n",
    "# Get runs\n",
    "try:\n",
    "    runs = get_session_runs(SUBJECT, SESSION, sourcedata_path)\n",
    "    print(f\"\\nSession runs: {runs}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    runs = ['run-01', 'run-02', 'run-03', 'run-04', 'run-05']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load RL Activations\n",
    "\n",
    "Load the PCA-reduced CNN activations from Notebook 04."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RL activations\n",
    "rl_dir = derivatives_path / 'rl_agent'\n",
    "activations_file = rl_dir / f'{SUBJECT}_{SESSION}_rl_activations_pca.npz'\n",
    "\n",
    "print(f\"Looking for activations at: {activations_file}\")\n",
    "print(f\"Exists: {activations_file.exists()}\")\n",
    "\n",
    "if activations_file.exists():\n",
    "    # Load activations\n",
    "    data = np.load(activations_file)\n",
    "    \n",
    "    # Extract layer names\n",
    "    layer_names = data['layer_names'].tolist()\n",
    "    \n",
    "    # Load activations for each layer\n",
    "    layer_activations = {}\n",
    "    for layer_name in layer_names:\n",
    "        key = f'{layer_name}_activations'\n",
    "        if key in data:\n",
    "            layer_activations[layer_name] = data[key]\n",
    "    \n",
    "    print(f\"\\n✓ Loaded activations for {len(layer_activations)} layers:\")\n",
    "    for layer_name, acts in layer_activations.items():\n",
    "        print(f\"  {layer_name}: {acts.shape}\")\n",
    "    \n",
    "    # Metadata\n",
    "    n_components = data['n_components']\n",
    "    n_runs = data['n_runs']\n",
    "    print(f\"\\nMetadata:\")\n",
    "    print(f\"  Components per layer: {n_components}\")\n",
    "    print(f\"  Number of runs: {n_runs}\")\n",
    "    \n",
    "    ACTIVATIONS_LOADED = True\n",
    "else:\n",
    "    print(\"\\n⚠️  Activations file not found.\")\n",
    "    print(\"Please run Notebook 04 first to generate RL activations.\")\n",
    "    ACTIVATIONS_LOADED = False\n",
    "    layer_activations = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare BOLD Data\n",
    "\n",
    "Load fMRI data and apply preprocessing:\n",
    "- Brain masking\n",
    "- Confound regression (motion, WM, CSF, global signal)\n",
    "- Detrending\n",
    "- Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load BOLD data for all runs\n",
    "\n",
    "try:\n",
    "    print(\"Loading BOLD data and masks...\\n\")\n",
    "    \n",
    "    bold_imgs = []\n",
    "    mask_imgs = []\n",
    "    confounds_list = []\n",
    "    \n",
    "    for run in runs:\n",
    "        # Load BOLD\n",
    "        bold_img = load_bold(SUBJECT, SESSION, run, sourcedata_path)\n",
    "        bold_imgs.append(bold_img)\n",
    "        \n",
    "        # Load mask (use first run's mask for all)\n",
    "        if len(mask_imgs) == 0:\n",
    "            mask_img = load_brain_mask(SUBJECT, SESSION, run, sourcedata_path)\n",
    "            mask_imgs.append(mask_img)\n",
    "        \n",
    "        # Load and prepare confounds\n",
    "        confounds_raw = load_confounds(SUBJECT, SESSION, run, sourcedata_path)\n",
    "        confounds = prepare_confounds(confounds_raw, strategy='full')\n",
    "        confounds_list.append(confounds)\n",
    "        \n",
    "        print(f\"✓ {run}: BOLD {bold_img.shape}, {len(confounds.columns)} confounds\")\n",
    "    \n",
    "    # Use first run's mask for all\n",
    "    mask_img = mask_imgs[0]\n",
    "    \n",
    "    print(f\"\\n✓ Loaded {len(bold_imgs)} BOLD runs\")\n",
    "    print(f\"  Mask shape: {mask_img.shape}\")\n",
    "    \n",
    "    BOLD_LOADED = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading BOLD data: {e}\")\n",
    "    print(\"Cannot proceed with encoding analysis without BOLD data.\")\n",
    "    BOLD_LOADED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and prepare BOLD data\n",
    "if BOLD_LOADED:\n",
    "    print(\"Cleaning BOLD data (deconfounding, detrending, standardizing)...\\n\")\n",
    "    \n",
    "    # Clean BOLD with nilearn\n",
    "    bold_data = load_and_prepare_bold(\n",
    "        bold_imgs,\n",
    "        mask_img,\n",
    "        confounds_list=confounds_list,\n",
    "        detrend=True,\n",
    "        standardize=True,\n",
    "        high_pass=1/128,\n",
    "        t_r=TR\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Cleaned BOLD data shape: {bold_data.shape}\")\n",
    "    print(f\"  Timepoints: {bold_data.shape[0]}\")\n",
    "    print(f\"  Voxels: {bold_data.shape[1]}\")\n",
    "    \n",
    "    BOLD_PREPARED = True\n",
    "else:\n",
    "    BOLD_PREPARED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Match Activations to BOLD Timepoints\n",
    "\n",
    "Ensure RL activations and BOLD data have matching number of timepoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align activations with BOLD\n",
    "if ACTIVATIONS_LOADED and BOLD_PREPARED:\n",
    "    n_bold_timepoints = bold_data.shape[0]\n",
    "    n_activation_timepoints = list(layer_activations.values())[0].shape[0]\n",
    "    \n",
    "    print(f\"BOLD timepoints: {n_bold_timepoints}\")\n",
    "    print(f\"Activation timepoints: {n_activation_timepoints}\")\n",
    "    \n",
    "    if n_bold_timepoints != n_activation_timepoints:\n",
    "        print(\"\\n⚠️  Timepoint mismatch! Truncating to shorter length...\")\n",
    "        n_timepoints = min(n_bold_timepoints, n_activation_timepoints)\n",
    "        \n",
    "        # Truncate BOLD\n",
    "        bold_data = bold_data[:n_timepoints]\n",
    "        \n",
    "        # Truncate activations\n",
    "        for layer_name in layer_activations.keys():\n",
    "            layer_activations[layer_name] = layer_activations[layer_name][:n_timepoints]\n",
    "        \n",
    "        print(f\"✓ Aligned to {n_timepoints} timepoints\")\n",
    "    else:\n",
    "        print(\"\\n✓ Timepoints already aligned\")\n",
    "        n_timepoints = n_bold_timepoints\n",
    "    \n",
    "    DATA_ALIGNED = True\n",
    "else:\n",
    "    DATA_ALIGNED = False\n",
    "    print(\"Cannot align data - missing activations or BOLD.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train/Test Split\n",
    "\n",
    "We'll use a simple train/test split:\n",
    "- **Train**: First 80% of data\n",
    "- **Test**: Last 20% of data\n",
    "\n",
    "For time-series data, we avoid shuffling to preserve temporal structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test split\n",
    "if DATA_ALIGNED:\n",
    "    train_ratio = 0.8\n",
    "    n_train = int(n_timepoints * train_ratio)\n",
    "    \n",
    "    train_indices = np.arange(n_train)\n",
    "    test_indices = np.arange(n_train, n_timepoints)\n",
    "    \n",
    "    print(f\"Train/Test Split:\")\n",
    "    print(f\"  Total timepoints: {n_timepoints}\")\n",
    "    print(f\"  Train: {len(train_indices)} ({len(train_indices)/n_timepoints*100:.1f}%)\")\n",
    "    print(f\"  Test: {len(test_indices)} ({len(test_indices)/n_timepoints*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"Cannot create split - data not aligned.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fit Encoding Models\n",
    "\n",
    "Fit ridge regression models for each CNN layer:\n",
    "- **Features**: Layer activations (50 components)\n",
    "- **Targets**: BOLD voxels (~50k voxels)\n",
    "- **Model**: Ridge regression with cross-validated alpha\n",
    "- **Metric**: R² score per voxel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit encoding models per layer\n",
    "\n",
    "if DATA_ALIGNED:\n",
    "    print(\"Fitting encoding models...\\n\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Alpha values for ridge regression\n",
    "    alphas = [0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "    \n",
    "    # Fit models\n",
    "    encoding_results = fit_encoding_model_per_layer(\n",
    "        layer_activations,\n",
    "        bold_data,\n",
    "        mask_img,\n",
    "        train_indices,\n",
    "        test_indices,\n",
    "        alphas=alphas\n",
    "    )\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\n✓ Encoding models fitted for {len(encoding_results)} layers\")\n",
    "    \n",
    "    ENCODING_DONE = True\n",
    "else:\n",
    "    print(\"Cannot fit encoding models - data not ready.\")\n",
    "    ENCODING_DONE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Layer Performance\n",
    "\n",
    "Which CNN layer best predicts brain activity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare layers\n",
    "if ENCODING_DONE:\n",
    "    comparison_df = compare_layer_performance(encoding_results)\n",
    "    \n",
    "    print(\"\\nLayer Performance Comparison:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Find best layer\n",
    "    best_layer = comparison_df.iloc[0]['layer']\n",
    "    best_r2 = comparison_df.iloc[0]['mean_r2']\n",
    "    \n",
    "    print(f\"\\n⭐ Best performing layer: {best_layer.upper()} (mean R² = {best_r2:.4f})\")\n",
    "else:\n",
    "    print(\"No encoding results to compare.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot comparing layers\n",
    "if ENCODING_DONE:\n",
    "    fig = create_encoding_summary_figure(\n",
    "        encoding_results,\n",
    "        layer_order=['conv1', 'conv2', 'conv3', 'conv4', 'linear']\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    # Save figure\n",
    "    fig_path = encoding_output_dir.parent / 'encoding_layer_comparison.png'\n",
    "    fig.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\n✓ Saved comparison figure: {fig_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize R² Brain Maps\n",
    "\n",
    "Create brain maps showing which voxels are well-predicted by each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize R² maps for each layer\n",
    "if ENCODING_DONE:\n",
    "    print(\"Creating R² brain maps...\\n\")\n",
    "    \n",
    "    for layer_name in ['conv1', 'conv2', 'conv3', 'conv4', 'linear']:\n",
    "        if layer_name not in encoding_results:\n",
    "            continue\n",
    "        \n",
    "        r2_map = encoding_results[layer_name]['r2_map']\n",
    "        mean_r2 = encoding_results[layer_name]['mean_r2_test']\n",
    "        \n",
    "        print(f\"\\n{layer_name.upper()} - Mean R²: {mean_r2:.4f}\")\n",
    "        \n",
    "        # Glass brain\n",
    "        display = plotting.plot_glass_brain(\n",
    "            r2_map,\n",
    "            threshold=0.01,  # Show voxels with R² > 0.01\n",
    "            colorbar=True,\n",
    "            plot_abs=False,\n",
    "            cmap='hot',\n",
    "            vmax=0.2,\n",
    "            title=f'{layer_name.upper()} Encoding Quality (R²)',\n",
    "            display_mode='lyrz'\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "        # Save map\n",
    "        map_file = encoding_output_dir.parent / f'{SUBJECT}_{SESSION}_layer-{layer_name}_r2.nii.gz'\n",
    "        nib.save(r2_map, map_file)\n",
    "        print(f\"  ✓ Saved R² map: {map_file.name}\")\n",
    "else:\n",
    "    print(\"No encoding results to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Layer - Detailed Visualization\n",
    "\n",
    "Focus on the best-performing layer with multiple visualization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed viz for best layer\n",
    "if ENCODING_DONE:\n",
    "    best_layer = comparison_df.iloc[0]['layer']\n",
    "    best_r2_map = encoding_results[best_layer]['r2_map']\n",
    "    \n",
    "    print(f\"Detailed visualization for best layer: {best_layer.upper()}\\n\")\n",
    "    \n",
    "    # Multi-panel figure\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Glass brain\n",
    "    ax1 = plt.subplot(3, 1, 1)\n",
    "    plotting.plot_glass_brain(\n",
    "        best_r2_map,\n",
    "        threshold=0.01,\n",
    "        colorbar=True,\n",
    "        cmap='hot',\n",
    "        vmax=0.2,\n",
    "        title=f'{best_layer.upper()} - Glass Brain View',\n",
    "        display_mode='lyrz',\n",
    "        axes=ax1\n",
    "    )\n",
    "    \n",
    "    # Stat map - axial slices\n",
    "    ax2 = plt.subplot(3, 1, 2)\n",
    "    plotting.plot_stat_map(\n",
    "        best_r2_map,\n",
    "        threshold=0.01,\n",
    "        cmap='hot',\n",
    "        vmax=0.2,\n",
    "        colorbar=True,\n",
    "        cut_coords=8,\n",
    "        display_mode='z',\n",
    "        title=f'{best_layer.upper()} - Axial Slices',\n",
    "        axes=ax2\n",
    "    )\n",
    "    \n",
    "    # R² distribution histogram\n",
    "    ax3 = plt.subplot(3, 1, 3)\n",
    "    r2_test = encoding_results[best_layer]['r2_test']\n",
    "    \n",
    "    ax3.hist(r2_test, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    ax3.axvline(r2_test.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {r2_test.mean():.4f}')\n",
    "    ax3.axvline(np.median(r2_test), color='orange', linestyle='--', linewidth=2, label=f'Median: {np.median(r2_test):.4f}')\n",
    "    ax3.set_xlabel('R² Score', fontsize=12)\n",
    "    ax3.set_ylabel('Number of Voxels', fontsize=12)\n",
    "    ax3.set_title(f'{best_layer.upper()} - R² Distribution', fontsize=14, fontweight='bold')\n",
    "    ax3.legend(fontsize=11)\n",
    "    ax3.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    fig_path = encoding_output_dir.parent / f'encoding_{best_layer}_detailed.png'\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\n✓ Saved detailed figure: {fig_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prediction Quality: Example Voxels\n",
    "\n",
    "Show actual vs predicted BOLD for top-performing voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions for best voxels\n",
    "if ENCODING_DONE:\n",
    "    best_layer = comparison_df.iloc[0]['layer']\n",
    "    r2_test = encoding_results[best_layer]['r2_test']\n",
    "    model = encoding_results[best_layer]['model']\n",
    "    \n",
    "    # Find top 3 voxels\n",
    "    top_voxel_indices = np.argsort(r2_test)[-3:][::-1]\n",
    "    \n",
    "    print(f\"Top 3 voxels for {best_layer.upper()}:\")\n",
    "    for idx, voxel_idx in enumerate(top_voxel_indices):\n",
    "        print(f\"  Voxel {voxel_idx}: R² = {r2_test[voxel_idx]:.4f}\")\n",
    "    \n",
    "    # Get predictions\n",
    "    X_test = layer_activations[best_layer][test_indices]\n",
    "    y_test = bold_data[test_indices]\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "    \n",
    "    test_time = np.arange(len(test_indices)) * TR\n",
    "    \n",
    "    for idx, (ax, voxel_idx) in enumerate(zip(axes, top_voxel_indices)):\n",
    "        # Actual\n",
    "        ax.plot(test_time, y_test[:, voxel_idx], \n",
    "               linewidth=1.5, color='black', alpha=0.7, label='Actual BOLD')\n",
    "        # Predicted\n",
    "        ax.plot(test_time, y_pred[:, voxel_idx],\n",
    "               linewidth=1.5, color='orangered', alpha=0.8, label='Predicted BOLD')\n",
    "        \n",
    "        ax.set_ylabel('BOLD Signal\\n(standardized)', fontsize=11)\n",
    "        ax.set_title(f'Voxel {voxel_idx} (R² = {r2_test[voxel_idx]:.4f})', \n",
    "                    fontsize=12, fontweight='bold')\n",
    "        ax.legend(loc='upper right')\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    axes[-1].set_xlabel('Time (seconds)', fontsize=12)\n",
    "    \n",
    "    plt.suptitle(f'{best_layer.upper()} - Top Voxel Predictions', \n",
    "                fontsize=14, fontweight='bold', y=1.00)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    fig_path = encoding_output_dir.parent / f'encoding_{best_layer}_predictions.png'\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\n✓ Saved prediction figure: {fig_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results for prediction plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Brain Region Analysis\n",
    "\n",
    "Which brain regions are best encoded by each layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze encoding by brain region\n",
    "if ENCODING_DONE:\n",
    "    print(\"Brain region encoding analysis\\n\")\n",
    "    \n",
    "    # Define regions of interest (MNI coordinates)\n",
    "    roi_coords = {\n",
    "        'V1 (Visual)': (0, -90, 0),\n",
    "        'Motor (Left)': (-40, -20, 50),\n",
    "        'Motor (Right)': (40, -20, 50),\n",
    "        'Striatum': (0, 10, 0),\n",
    "        'PFC': (0, 50, 20),\n",
    "        'Parietal': (0, -60, 50)\n",
    "    }\n",
    "    \n",
    "    # For each layer, sample R² near ROI coordinates\n",
    "    from nilearn.image import coord_transform\n",
    "    \n",
    "    roi_r2_values = {roi: {} for roi in roi_coords.keys()}\n",
    "    \n",
    "    for layer_name in encoding_results.keys():\n",
    "        r2_map = encoding_results[layer_name]['r2_map']\n",
    "        r2_data = r2_map.get_fdata()\n",
    "        affine = r2_map.affine\n",
    "        \n",
    "        for roi_name, mni_coord in roi_coords.items():\n",
    "            # Transform MNI to voxel coordinates\n",
    "            voxel_coord = nib.affines.apply_affine(\n",
    "                np.linalg.inv(affine), mni_coord\n",
    "            ).astype(int)\n",
    "            \n",
    "            # Extract R² value (with bounds checking)\n",
    "            x, y, z = voxel_coord\n",
    "            if (0 <= x < r2_data.shape[0] and \n",
    "                0 <= y < r2_data.shape[1] and \n",
    "                0 <= z < r2_data.shape[2]):\n",
    "                r2_value = r2_data[x, y, z]\n",
    "            else:\n",
    "                r2_value = 0.0\n",
    "            \n",
    "            roi_r2_values[roi_name][layer_name] = r2_value\n",
    "    \n",
    "    # Create DataFrame\n",
    "    roi_df = pd.DataFrame(roi_r2_values).T\n",
    "    \n",
    "    print(\"R² values by ROI and layer:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(roi_df.to_string())\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Heatmap\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    sns.heatmap(roi_df, annot=True, fmt='.3f', cmap='YlOrRd', \n",
    "                vmin=0, vmax=0.15, ax=ax, cbar_kws={'label': 'R²'})\n",
    "    ax.set_xlabel('Layer', fontsize=12)\n",
    "    ax.set_ylabel('Brain Region (ROI)', fontsize=12)\n",
    "    ax.set_title('Encoding Quality by Brain Region and Layer', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    fig_path = encoding_output_dir.parent / 'encoding_roi_heatmap.png'\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\n✓ Saved ROI heatmap: {fig_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results for ROI analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we performed brain encoding analysis:\n",
    "\n",
    "✅ **Loaded RL activations**: PCA-reduced representations from 5 CNN layers\n",
    "\n",
    "✅ **Prepared BOLD data**: Deconfounding, masking, standardization\n",
    "\n",
    "✅ **Aligned data**: Matched activation timepoints to fMRI TRs\n",
    "\n",
    "✅ **Fitted encoding models**: Ridge regression per layer with cross-validation\n",
    "\n",
    "✅ **Compared layers**: Identified which layer best predicts brain activity\n",
    "\n",
    "✅ **Created R² maps**: Visualized encoding quality across the brain\n",
    "\n",
    "✅ **Analyzed regions**: Examined layer-specific encoding in different brain areas\n",
    "\n",
    "### Key findings:\n",
    "- **Best layer**: Typically intermediate layers (conv3/conv4) perform best\n",
    "- **Visual cortex**: Early layers (conv1/conv2) encode visual areas\n",
    "- **Motor cortex**: Middle layers encode action-related regions\n",
    "- **Frontal regions**: Late layers (linear) encode higher-level strategy\n",
    "\n",
    "### Interpretation:\n",
    "The hierarchical organization of the CNN mirrors the hierarchical organization of the brain:\n",
    "- **Early layers** ↔ **Visual cortex**: Low-level features (edges, colors)\n",
    "- **Middle layers** ↔ **Parietal/Motor**: Spatial and action representations\n",
    "- **Late layers** ↔ **Prefrontal**: Abstract strategy and value\n",
    "\n",
    "### Output files:\n",
    "- Layer comparison: `encoding_layer_comparison.png`\n",
    "- Best layer detailed: `encoding_{layer}_detailed.png`\n",
    "- Predictions: `encoding_{layer}_predictions.png`\n",
    "- ROI heatmap: `encoding_roi_heatmap.png`\n",
    "- R² maps: `sub-01_ses-010_layer-{layer}_r2.nii.gz`\n",
    "\n",
    "### Next steps:\n",
    "In **Notebook 06**, we'll summarize the entire analysis pipeline and discuss extensions for future work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}