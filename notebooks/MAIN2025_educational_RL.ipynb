{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Agent & Brain Encoding Tutorial\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**This notebook requires variables from the main tutorial:**\n",
    "\n",
    "You must run **Section 3 (GLM Analysis)** from `MAIN2025_educational.ipynb` first, which provides:\n",
    "- `runs` - List of run IDs (e.g., ['run-1', 'run-2', ...])\n",
    "- `all_events` - Event DataFrames for each run\n",
    "- `common_mask` - Brain mask from multi-run GLM\n",
    "- `sourcedata_path` - Path to data\n",
    "\n",
    "**Or run this cell to load them:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5aaccc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup - imports and configuration\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "src_dir = Path('..') / 'src'\n",
    "sys.path.insert(0, str(src_dir))\n",
    "\n",
    "# Import utilities\n",
    "from utils import (\n",
    "    get_sourcedata_path,\n",
    "    load_events,\n",
    "    get_session_runs,\n",
    "    load_bold\n",
    ")\n",
    "\n",
    "# Import RL utilities\n",
    "from rl_utils import (\n",
    "    create_simple_proxy_features,\n",
    "    convolve_with_hrf,\n",
    "    apply_pca\n",
    ")\n",
    "\n",
    "# Import RL visualizations\n",
    "from rl_viz_utils import (\n",
    "    plot_pca_variance_per_layer,\n",
    "    plot_layer_activations_sample\n",
    ")\n",
    "\n",
    "# Import encoding utilities\n",
    "from encoding_utils import (\n",
    "    load_and_prepare_bold,\n",
    "    fit_encoding_model_per_layer,\n",
    "    compare_layer_performance\n",
    ")\n",
    "\n",
    "# Import encoding visualizations\n",
    "from encoding_viz_utils import (\n",
    "    plot_layer_comparison_bars,\n",
    "    plot_r2_brainmap\n",
    ")\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Get sourcedata path\n",
    "sourcedata_path = get_sourcedata_path()\n",
    "\n",
    "print(\"‚úì Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7668c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd536536",
   "metadata": {},
   "source": [
    "# Section 4: RL Agent\n",
    "\n",
    "## Learning Representations from Gameplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07ecb25",
   "metadata": {},
   "source": [
    "## Why RL for fMRI?\n",
    "\n",
    "### Limitations of Traditional GLM\n",
    "\n",
    "**GLM approach:**\n",
    "- Hand-crafted regressors (LEFT, RIGHT, Powerup, etc.)\n",
    "- Hypothesis-driven\n",
    "- Interpretable but limited\n",
    "\n",
    "**Problems:**\n",
    "- Can't capture complex strategies\n",
    "- Misses latent variables (intentions, predictions, value)\n",
    "- Requires knowing what to look for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c00038a",
   "metadata": {},
   "source": [
    "## RL Agent Approach\n",
    "\n",
    "**Key idea:** Train agent to play ‚Üí Extract learned representations ‚Üí Predict brain activity\n",
    "\n",
    "**Advantages:**\n",
    "1. **Data-driven:** No assumptions about relevant features\n",
    "2. **Hierarchical:** Multiple levels of abstraction (pixels ‚Üí strategy)\n",
    "3. **Latent variables:** Captures value, predictions, uncertainty\n",
    "4. **Hypothesis generation:** Discover what brain encodes\n",
    "\n",
    "**Hypothesis:** Brain uses similar representations as RL agent for gameplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c8bba5",
   "metadata": {},
   "source": [
    "## PPO Agent Architecture\n",
    "\n",
    "### Proximal Policy Optimization (PPO)\n",
    "\n",
    "**Input:** 4 stacked frames (84√ó84 grayscale) ‚Üí Temporal context\n",
    "\n",
    "**Convolutional layers (feature hierarchy):**\n",
    "```\n",
    "conv1: 4 ‚Üí 32 channels (42√ó42)   # Edges, colors\n",
    "conv2: 32 ‚Üí 32 channels (21√ó21)  # Textures, patterns  \n",
    "conv3: 32 ‚Üí 32 channels (11√ó11)  # Objects, enemies\n",
    "conv4: 32 ‚Üí 32 channels (6√ó6)    # Spatial relations\n",
    "linear: 1152 ‚Üí 512 features      # Strategy, value\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c0576",
   "metadata": {},
   "source": [
    "## PPO Architecture (continued)\n",
    "\n",
    "**Output heads:**\n",
    "- **Actor:** 512 ‚Üí 12 actions (LEFT, RIGHT, A, B, combinations)\n",
    "- **Critic:** 512 ‚Üí 1 value (expected future reward)\n",
    "\n",
    "**Analogy to visual cortex:**\n",
    "- conv1/conv2 ‚âà V1/V2 (primary visual cortex)\n",
    "- conv3/conv4 ‚âà V4/IT (object recognition)\n",
    "- linear ‚âà PFC (executive function, planning)\n",
    "\n",
    "**Total parameters:** ~150k (compact but powerful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0690132b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Found pretrained weights: ../models/mario_ppo_agent.pth\n",
      "  File size: 7.5 MB\n"
     ]
    }
   ],
   "source": [
    "# Check for pretrained agent weights\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_DIR = Path('../models')\n",
    "MODEL_PATH = MODEL_DIR / 'mario_ppo_agent.pth'\n",
    "\n",
    "if MODEL_PATH.exists():\n",
    "    print(f\"‚úì Found pretrained weights: {MODEL_PATH}\")\n",
    "    print(f\"  File size: {MODEL_PATH.stat().st_size / 1e6:.1f} MB\")\n",
    "    HAS_WEIGHTS = True\n",
    "else:\n",
    "    print(f\"‚úó No pretrained weights found at: {MODEL_PATH}\")\n",
    "    print(f\"\\nTo train agent, run:\")\n",
    "    print(f\"  python ../train_mario_agent.py --steps 5000000\")\n",
    "    print(f\"\\nOr for quick demo (10k steps, ~2 min):\")\n",
    "    print(f\"  python ../train_mario_agent.py --steps 10000\")\n",
    "    print(f\"\\n‚Üí For now, using behavioral proxy features\")\n",
    "    HAS_WEIGHTS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5df038",
   "metadata": {},
   "source": [
    "## Agent Gameplay Demonstration\n",
    "\n",
    "Let's watch the trained agent play a level!\n",
    "\n",
    "**What you'll see:**\n",
    "- Agent playing in real-time\n",
    "- Actions being selected by the CNN\n",
    "- Level progression\n",
    "\n",
    "**Select a level below to watch the agent play.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80d9884f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available levels:\n",
      "  0: Level1-1\n",
      "  1: Level1-2\n",
      "  2: Level4-1\n",
      "  3: Level4-2\n",
      "  4: Level5-1\n",
      "  5: Level5-2\n",
      "\n",
      "Playing: Level1-1\n",
      "\n",
      "‚úì Model loaded\n",
      "\n",
      "‚ñ∂ Playing... (close window to stop)\n",
      "\n",
      "\n",
      "Window closed by user.\n",
      "\n",
      "‚úì Episode complete!\n",
      "  Steps: 1092\n",
      "  Total reward: 370.0\n",
      "  Completed: No\n"
     ]
    }
   ],
   "source": [
    "# Agent gameplay demonstration\n",
    "\n",
    "if HAS_WEIGHTS:\n",
    "    import importlib\n",
    "    import rl_utils\n",
    "    importlib.reload(rl_utils)\n",
    "    from rl_utils import load_pretrained_model, play_agent_episode\n",
    "    \n",
    "    # Select level to play\n",
    "    available_levels = ['Level1-1', 'Level1-2', 'Level4-1', 'Level4-2', 'Level5-1', 'Level5-2']\n",
    "    \n",
    "    print(\"Available levels:\")\n",
    "    for idx, level in enumerate(available_levels):\n",
    "        print(f\"  {idx}: {level}\")\n",
    "    \n",
    "    # Select level (change this number to play different levels)\n",
    "    level_idx = 0  # Play Level1-1 by default\n",
    "    selected_level = available_levels[level_idx]\n",
    "    \n",
    "    print(f\"\\nPlaying: {selected_level}\\n\")\n",
    "    \n",
    "    # Load trained model\n",
    "    model = load_pretrained_model(MODEL_PATH, device='cpu')\n",
    "    print(\"‚úì Model loaded\\n\")\n",
    "    \n",
    "    # Play episode\n",
    "    print(\"‚ñ∂ Playing... (close window to stop)\\n\")\n",
    "    results = play_agent_episode(model, selected_level, sourcedata_path, max_steps=5000)\n",
    "    \n",
    "    # Show results\n",
    "    print(f\"\\n‚úì Episode complete!\")\n",
    "    print(f\"  Steps: {results['steps']}\")\n",
    "    print(f\"  Total reward: {results['reward']:.1f}\")\n",
    "    print(f\"  Completed: {'Yes' if results['completed'] else 'No'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö† No trained weights available.\")\n",
    "    print(\"  Train agent first: python ../train_mario_agent.py --steps 5000000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d25eda4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Layer Configurations:\n",
      "\n",
      "  conv1   : 56,448 features\n",
      "  conv2   : 14,112 features\n",
      "  conv3   : 3,872 features\n",
      "  conv4   : 1,152 features\n",
      "  linear  : 512 features\n"
     ]
    }
   ],
   "source": [
    "# Define CNN layer configurations\n",
    "\n",
    "LAYER_CONFIGS = {\n",
    "    'conv1': 32 * 42 * 42,  # Early visual features\n",
    "    'conv2': 32 * 21 * 21,  # Mid-level features\n",
    "    'conv3': 32 * 11 * 11,  # High-level visual\n",
    "    'conv4': 32 * 6 * 6,    # Abstract features\n",
    "    'linear': 512           # Semantic features\n",
    "}\n",
    "\n",
    "print(\"CNN Layer Configurations:\\n\")\n",
    "for layer, n_features in LAYER_CONFIGS.items():\n",
    "    print(f\"  {layer:8s}: {n_features:,} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d12c4db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating behavioral proxy features...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'runs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreating behavioral proxy features...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m base_features_per_run = []\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m run_idx, run \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mruns\u001b[49m):\n\u001b[32m     11\u001b[39m     events = all_events[run_idx]\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Get number of TRs from BOLD\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'runs' is not defined"
     ]
    }
   ],
   "source": [
    "# Create behavioral proxy features for each run\n",
    "\n",
    "from rl_utils import create_simple_proxy_features\n",
    "from utils import load_bold\n",
    "\n",
    "print(\"Creating behavioral proxy features...\\n\")\n",
    "\n",
    "base_features_per_run = []\n",
    "\n",
    "for run_idx, run in enumerate(runs):\n",
    "    events = all_events[run_idx]\n",
    "    \n",
    "    # Get number of TRs from BOLD\n",
    "    bold_img = load_bold(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    n_trs = bold_img.shape[-1]\n",
    "    \n",
    "    # Create proxy features (buttons + game events)\n",
    "    proxy_feats = create_simple_proxy_features(events, n_trs, TR)\n",
    "    base_features_per_run.append(proxy_feats['combined_features'])\n",
    "    \n",
    "    print(f\"  {run}: {proxy_feats['combined_features'].shape}\")\n",
    "\n",
    "print(f\"\\n‚úì Created features for {len(runs)} runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a0c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate CNN layer activations (random projections + behavioral mixing)\n",
    "\n",
    "from rl_utils import convolve_with_hrf\n",
    "import numpy as np\n",
    "\n",
    "print(\"Simulating CNN activations...\\n\")\n",
    "\n",
    "all_layer_activations = {layer: [] for layer in LAYER_CONFIGS.keys()}\n",
    "\n",
    "for run_idx, base_features in enumerate(base_features_per_run):\n",
    "    n_trs = base_features.shape[0]\n",
    "    \n",
    "    for layer_name, n_features in LAYER_CONFIGS.items():\n",
    "        # Random baseline activations\n",
    "        layer_acts = np.random.randn(n_trs, n_features) * 0.3\n",
    "        \n",
    "        # Mix in behavioral features (first 50 neurons)\n",
    "        n_mix = min(50, n_features)\n",
    "        for i in range(min(base_features.shape[1], 10)):\n",
    "            layer_acts[:, :n_mix] += np.outer(\n",
    "                base_features[:, i], \n",
    "                np.random.randn(n_mix)\n",
    "            ) * 0.5\n",
    "        \n",
    "        # Convolve with HRF (simulate BOLD response)\n",
    "        layer_acts_hrf = convolve_with_hrf(layer_acts, TR, hrf_model='spm')\n",
    "        all_layer_activations[layer_name].append(layer_acts_hrf)\n",
    "    \n",
    "    print(f\"  {runs[run_idx]}: ‚úì\")\n",
    "\n",
    "print(f\"\\n‚úì Simulated activations for {len(LAYER_CONFIGS)} layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate runs\n",
    "\n",
    "print(\"Concatenating runs...\\n\")\n",
    "\n",
    "for layer_name in all_layer_activations.keys():\n",
    "    all_layer_activations[layer_name] = np.concatenate(\n",
    "        all_layer_activations[layer_name], axis=0\n",
    "    )\n",
    "    print(f\"  {layer_name}: {all_layer_activations[layer_name].shape}\")\n",
    "\n",
    "print(f\"\\n‚úì All layers concatenated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA dimensionality reduction\n",
    "\n",
    "from rl_utils import apply_pca\n",
    "\n",
    "N_COMPONENTS = 50\n",
    "\n",
    "print(\"Applying PCA (50 components per layer)...\\n\")\n",
    "\n",
    "pca_results = {}\n",
    "reduced_activations = {}\n",
    "\n",
    "for layer_name, acts in all_layer_activations.items():\n",
    "    reduced, pca_model, variance_explained = apply_pca(\n",
    "        acts, n_components=N_COMPONENTS, variance_threshold=0.9\n",
    "    )\n",
    "    \n",
    "    pca_results[layer_name] = {\n",
    "        'pca': pca_model,\n",
    "        'variance_explained': variance_explained\n",
    "    }\n",
    "    reduced_activations[layer_name] = reduced\n",
    "    \n",
    "    total_var = np.sum(variance_explained)\n",
    "    print(f\"  {layer_name:8s}: {acts.shape[1]:,} ‚Üí {reduced.shape[1]} \"\n",
    "          f\"components ({total_var*100:.1f}% variance)\")\n",
    "\n",
    "print(\"\\n‚úì PCA complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA variance explained\n",
    "\n",
    "from rl_viz_utils import plot_pca_variance_per_layer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plot_pca_variance_per_layer(pca_results, LAYER_CONFIGS)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample activations visualization\n",
    "\n",
    "from rl_viz_utils import plot_layer_activations_sample\n",
    "\n",
    "fig = plot_layer_activations_sample(\n",
    "    reduced_activations, \n",
    "    layer_name='conv3',\n",
    "    n_trs=200,\n",
    "    n_features=10\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì These are HRF-convolved activations ready for encoding!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Brain Encoding\n",
    "\n",
    "## Predicting fMRI from Learned Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Model Framework\n",
    "\n",
    "### The Brain Encoding Problem\n",
    "\n",
    "**Goal:** Use RL features to predict brain activity\n",
    "\n",
    "**Model:** Ridge Regression\n",
    "```\n",
    "BOLD(voxel, time) = Œ£ Œ≤·µ¢ ¬∑ Feature_i(time) + Œµ\n",
    "```\n",
    "\n",
    "**Ridge regression:** Linear regression with L2 regularization\n",
    "- Handles high-dimensional features (50 components)\n",
    "- Prevents overfitting\n",
    "- Cross-validation to select regularization strength (Œ±)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Strategy\n",
    "\n",
    "**Strategy:**\n",
    "1. **Separate model per layer:** Which layer best predicts brain?\n",
    "2. **Voxel-wise fitting:** Each voxel gets its own weights\n",
    "3. **Train/test split:** 80% train, 20% test\n",
    "4. **Evaluation:** R¬≤ score per voxel\n",
    "\n",
    "**Key questions:**\n",
    "- Which CNN layer best predicts BOLD?\n",
    "- Which brain regions are encoded by each layer?\n",
    "- How much variance can we explain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9bf216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prerequisites\n",
    "\n",
    "from nilearn.masking import compute_multi_epi_mask\n",
    "\n",
    "# Define constants (assumed from the first tutorial)\n",
    "SUBJECT = 'sub-01'\n",
    "SESSION = 'ses-010'\n",
    "TR = 1.49\n",
    "\n",
    "\n",
    "# Get runs\n",
    "runs = get_session_runs(SUBJECT, SESSION, sourcedata_path)\n",
    "print(f\"Found {len(runs)} runs: {runs}\")\n",
    "\n",
    "# Load events\n",
    "all_events = []\n",
    "for run in runs:\n",
    "    events = load_events(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    all_events.append(events)\n",
    "    print(f\"  {run}: {len(events)} events\")\n",
    "\n",
    "# Create common mask (or load from main tutorial)\n",
    "print(\"\\nCreating common brain mask...\")\n",
    "bold_imgs= []\n",
    "for run in runs:\n",
    "    bold_img = load_bold(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    bold_imgs.append(bold_img)\n",
    "\n",
    "common_mask = compute_multi_epi_mask(bold_imgs, n_jobs=1)\n",
    "n_voxels = int((common_mask.get_fdata() > 0).sum())\n",
    "print(f\"‚úì Common mask: {n_voxels:,} voxels\")\n",
    "\n",
    "print(\"\\n‚úì All prerequisites loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and prepare BOLD data\n",
    "\n",
    "from encoding_utils import load_and_prepare_bold\n",
    "\n",
    "print(\"Cleaning BOLD (detrending, standardizing)...\\n\")\n",
    "\n",
    "bold_data = load_and_prepare_bold(\n",
    "    bold_imgs,\n",
    "    mask_img=common_mask,\n",
    "    confounds_list=None,  # Already cleaned in GLM\n",
    "    detrend=True,\n",
    "    standardize=True,\n",
    "    high_pass=1/128,\n",
    "    t_r=TR\n",
    ")\n",
    "\n",
    "print(f\"‚úì BOLD prepared:\")\n",
    "print(f\"  Shape: {bold_data.shape}\")\n",
    "print(f\"  Timepoints: {bold_data.shape[0]}\")\n",
    "print(f\"  Voxels: {bold_data.shape[1]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align timepoints between BOLD and activations\n",
    "\n",
    "n_bold = bold_data.shape[0]\n",
    "n_acts = list(reduced_activations.values())[0].shape[0]\n",
    "\n",
    "print(f\"BOLD timepoints: {n_bold}\")\n",
    "print(f\"Activations timepoints: {n_acts}\")\n",
    "\n",
    "# Take minimum (align)\n",
    "n_time = min(n_bold, n_acts)\n",
    "\n",
    "bold_data = bold_data[:n_time]\n",
    "for layer in reduced_activations.keys():\n",
    "    reduced_activations[layer] = reduced_activations[layer][:n_time]\n",
    "\n",
    "print(f\"\\n‚úì Aligned to {n_time} timepoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test split (80/20)\n",
    "\n",
    "n_train = int(n_time * 0.8)\n",
    "train_idx = np.arange(n_train)\n",
    "test_idx = np.arange(n_train, n_time)\n",
    "\n",
    "print(f\"Train/test split:\")\n",
    "print(f\"  Train: {len(train_idx)} timepoints\")\n",
    "print(f\"  Test: {len(test_idx)} timepoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit ridge regression encoding models\n",
    "\n",
    "from encoding_utils import fit_encoding_model_per_layer\n",
    "\n",
    "alphas = [0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "\n",
    "print(\"Fitting ridge regression (5 layers √ó voxels)...\")\n",
    "print(\"This takes ~3-5 minutes\\n\")\n",
    "\n",
    "encoding_results = fit_encoding_model_per_layer(\n",
    "    reduced_activations, \n",
    "    bold_data, \n",
    "    common_mask,\n",
    "    train_idx, \n",
    "    test_idx, \n",
    "    alphas=alphas\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Encoding complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare layer performance\n",
    "\n",
    "from encoding_utils import compare_layer_performance\n",
    "from encoding_viz_utils import plot_layer_comparison_bars\n",
    "\n",
    "comparison_df = compare_layer_performance(encoding_results)\n",
    "\n",
    "print(\"Layer Performance:\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_layer = comparison_df.iloc[0]['layer']\n",
    "best_r2 = comparison_df.iloc[0]['mean_r2']\n",
    "\n",
    "print(f\"\\n‚≠ê Best: {best_layer.upper()} (R¬≤ = {best_r2:.4f})\")\n",
    "\n",
    "# Visualize\n",
    "layer_order = ['conv1', 'conv2', 'conv3', 'conv4', 'linear']\n",
    "fig = plot_layer_comparison_bars(encoding_results, layer_order)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize R¬≤ brain maps (best layer)\n",
    "\n",
    "from encoding_viz_utils import plot_r2_brainmap\n",
    "\n",
    "best_layer = comparison_df.iloc[0]['layer']\n",
    "best_r2_map = encoding_results[best_layer]['r2_map']\n",
    "\n",
    "print(f\"Best layer: {best_layer.upper()}\\n\")\n",
    "\n",
    "fig = plot_r2_brainmap(\n",
    "    best_r2_map, \n",
    "    best_layer,\n",
    "    threshold=0.01,\n",
    "    vmax=0.2\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìç Interpretation:\")\n",
    "print(\"  Hot regions = Well predicted by this layer\")\n",
    "print(\"  - Early layers ‚Üí Visual cortex\")\n",
    "print(\"  - Middle layers ‚Üí Motor/parietal\")\n",
    "print(\"  - Late layers ‚Üí Frontal/executive\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
