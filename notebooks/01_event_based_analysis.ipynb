{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Setup - hidden from presentation\nimport sys\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nibabel as nib\nfrom nilearn import plotting\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Add src to path\nsrc_dir = Path('..') / 'src'\nsys.path.insert(0, str(src_dir))\n\nfrom utils import (\n    get_sourcedata_path,\n    get_derivatives_path,\n    load_events,\n    get_session_runs,\n    load_lowlevel_confounds\n)\n\n# Plotting style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (14, 6)\nplt.rcParams['font.size'] = 11\n\n# Define constants\nSUBJECT = 'sub-01'\nSESSION = 'ses-010'\nTR = 1.49\n\nprint(\"Setup complete!\")\n\n# Get sourcedata path\nsourcedata_path = get_sourcedata_path()\n\n# Load runs and events\nruns = get_session_runs(SUBJECT, SESSION, get_sourcedata_path())\nprint(f\"Found {len(runs)} runs: {runs}\\n\")\n\n# Load all events\nall_events = []\nfor run in runs:\n    events = load_events(SUBJECT, SESSION, run, get_sourcedata_path())\n    all_events.append(events)\n    print(f\"  {run}: {len(events)} events\")\n\nEVENTS_LOADED = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# GLM Analysis\n\n## Finding Brain Regions for Actions and Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLM Fundamentals\n\n### The General Linear Model for fMRI\n\n**Basic idea:** Model brain activity as a weighted sum of explanatory variables\n\n```\nBOLD(t) = \u03b2\u2081\u00b7Regressor\u2081(t) + \u03b2\u2082\u00b7Regressor\u2082(t) + ... + \u03b5(t)\n```\n\n**Steps:**\n1. **Event timing** \u2192 Neural activity (stick functions)\n2. **Add confounds** \u2192 Motion, physiology, low-level features\n3. **HRF convolution** \u2192 Expected BOLD response\n4. **Fit model** \u2192 Estimate \u03b2 weights (using nilearn's FirstLevelModel)\n5. **Compute contrasts** \u2192 Test hypotheses\n\n**Our confound strategy (following shinobi_fmri):**\n- **Motion:** 6 motion parameters from fMRIPrep\n- **Physiology:** WM, CSF signals (loaded via load_confounds)\n- **Task:** Button press counts (manual psychophysics confound)\n- **Standardization:** Done by nilearn (`clean_img` with `standardize=True`)\n\n**Models we'll fit:**\n1. **Hand lateralization:** LEFT_THUMB (D-pad) vs RIGHT_THUMB (A+B buttons)\n\n**Multi-run fitting:**\n- nilearn's FirstLevelModel can fit all runs simultaneously\n- Provides list of BOLD images + list of design matrices\n- More efficient than fitting runs separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load BOLD image\n",
    "from utils import load_bold\n",
    "from nilearn.image import clean_img\n",
    "\n",
    "bold_img = load_bold(SUBJECT, SESSION, runs[0], sourcedata_path)\n",
    "bold_clean = clean_img(\n",
    "    bold_img,\n",
    "    standardize=True,\n",
    "    detrend=True,\n",
    "    high_pass=None,\n",
    "    t_r=TR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load fMRIPrep confounds\n",
    "\n",
    "from nilearn.interfaces.fmriprep import load_confounds\n",
    "\n",
    "def load_fmriprep_confounds(subject, session, run):\n",
    "    \"\"\"\n",
    "    Load confounds from fMRIPrep for a given run.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    subject : str\n",
    "        Subject ID (e.g., 'sub-01')\n",
    "    session : str\n",
    "        Session ID (e.g., 'ses-010')\n",
    "    run : str\n",
    "        Run ID (e.g., 'run-1')\n",
    "    derivatives_path : Path\n",
    "        Path to derivatives directory\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    confounds_df : pd.DataFrame\n",
    "        Confounds dataframe with selected regressors\n",
    "    sample_mask : np.ndarray or None\n",
    "        Sample mask (if applicable)\n",
    "    \"\"\"\n",
    "    sourcedata_path = get_sourcedata_path()\n",
    "    # Path to preprocessed BOLD image\n",
    "    bold_path = (\n",
    "        sourcedata_path / 'mario.fmriprep' / subject / session / 'func' /\n",
    "        f'{subject}_{session}_task-mario_{run}_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz'\n",
    "    )\n",
    "    \n",
    "    # Load confounds using nilearn's fMRIPrep interface\n",
    "    # This automatically finds the corresponding *_desc-confounds_timeseries.tsv file\n",
    "    confounds_df, sample_mask = load_confounds(\n",
    "        str(bold_path),\n",
    "        strategy=(\"motion\", \"high_pass\", \"wm_csf\"),\n",
    "        motion=\"full\",       # Motion parameters (6 params + derivatives + powers)\n",
    "        wm_csf=\"basic\",      # White matter + CSF signals\n",
    "        global_signal=\"full\", # Global signal\n",
    "        demean=False, # Because we will standardize later\n",
    "    )\n",
    "\n",
    "    return confounds_df, sample_mask\n",
    "\n",
    "# Test on first run\n",
    "confounds_df, sample_mask = load_fmriprep_confounds(\n",
    "    SUBJECT, SESSION, runs[0]\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Loaded {confounds_df.shape[1]} confounds for {runs[0]}\")\n",
    "print(f\"  Timepoints: {confounds_df.shape[0]}\")\n",
    "print(f\"\\nConfound types:\")\n",
    "motion_cols = [c for c in confounds_df.columns if 'trans' in c or 'rot' in c]\n",
    "physio_cols = [c for c in confounds_df.columns if 'csf' in c or 'white_matter' in c or 'global_signal' in c]\n",
    "drift_cols = [c for c in confounds_df.columns if 'cosine' in c]\n",
    "print(f\"  Motion: {len(motion_cols)} (6 params + derivatives + quadratic)\")\n",
    "print(f\"  Physiology: {len(physio_cols)} (WM, CSF, global signal)\")\n",
    "print(f\"  Drift: {len(drift_cols)} (cosine basis for high-pass filtering)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add low-level game features as confounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Add psychophysics confounds (following shinobi_fmri)\n",
    "\n",
    "def add_psychophysics_confounds(confounds_df, lowlevel_features, tr):\n",
    "    \"\"\"\n",
    "    Add psychophysics (low-level game features) as confounds.\n",
    "    \n",
    "    Following shinobi_fmri: downsample 60Hz features to TR.\n",
    "    These capture low-level visual/game state that we want to regress out.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    confounds_df : pd.DataFrame\n",
    "        Existing confounds\n",
    "    lowlevel_features : dict\n",
    "        Dictionary of feature name -> 60Hz signal arrays\n",
    "    tr : float\n",
    "        Repetition time in seconds\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Confounds with added psychophysics columns\n",
    "    \"\"\"\n",
    "    if lowlevel_features is None or len(lowlevel_features) == 0:\n",
    "        return confounds_df\n",
    "    \n",
    "    confounds_df = confounds_df.copy()\n",
    "    n_scans = len(confounds_df)\n",
    "    \n",
    "    # Downsample each feature from 60Hz to TR\n",
    "    for feature_name, signal_60hz in lowlevel_features.items():\n",
    "        # Convert 60Hz signal to TR-sampled\n",
    "        n_frames = len(signal_60hz)\n",
    "        frame_times = np.arange(n_frames) / 60.0  # 60 fps\n",
    "        \n",
    "        # Bin frames by TR\n",
    "        tr_bins = (frame_times // tr).astype(int)\n",
    "        \n",
    "        # Average within each TR\n",
    "        signal_tr = np.zeros(n_scans)\n",
    "        for tr_idx in range(n_scans):\n",
    "            frames_in_tr = signal_60hz[tr_bins == tr_idx]\n",
    "            if len(frames_in_tr) > 0:\n",
    "                signal_tr[tr_idx] = np.mean(frames_in_tr)\n",
    "        \n",
    "        # Add to confounds\n",
    "        confounds_df[f'{feature_name}'] = signal_tr\n",
    "    \n",
    "    return confounds_df\n",
    "\n",
    "lowlevel_features = load_lowlevel_confounds(SUBJECT, SESSION, runs[0], sourcedata_path)\n",
    "\n",
    "confounds_df = add_psychophysics_confounds(confounds_df, lowlevel_features, TR)\n",
    "print(f\"\u2713 Added {len(lowlevel_features.keys())} psychophysics confounds:\")\n",
    "for key in lowlevel_features.keys():\n",
    "    print(f\"  - {key}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Total confounds: {confounds_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load behavioral event annotations for the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Add button press confounds (following shinobi_fmri)\n",
    "\n",
    "def add_button_press_confounds(confounds_df, events_df, tr):\n",
    "    \"\"\"\n",
    "    Add button press counts as confounds.\n",
    "    \n",
    "    Following shinobi_fmri: count button presses in each TR.\n",
    "    This captures motor-related noise from button pressing.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    confounds_df : pd.DataFrame\n",
    "        Existing confounds\n",
    "    events_df : pd.DataFrame\n",
    "        Events dataframe with trial_type, onset columns\n",
    "    tr : float\n",
    "        Repetition time in seconds\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Confounds with added button_count_press column\n",
    "    \"\"\"\n",
    "    confounds_df = confounds_df.copy()\n",
    "    n_scans = len(confounds_df)\n",
    "    \n",
    "    # Initialize button press count\n",
    "    button_count = np.zeros(n_scans)\n",
    "    \n",
    "    # Button types\n",
    "    button_types = ['A', 'B', 'LEFT', 'RIGHT', 'UP', 'DOWN']\n",
    "    \n",
    "    # Count presses in each TR\n",
    "    for _, event in events_df.iterrows():\n",
    "        if event['trial_type'] in button_types:\n",
    "            tr_idx = int(np.floor(event['onset'] / tr))\n",
    "            if 0 <= tr_idx < n_scans:\n",
    "                button_count[tr_idx] += 1\n",
    "    \n",
    "    confounds_df['button_count_press'] = button_count\n",
    "    \n",
    "    return confounds_df\n",
    "\n",
    "# Test on first run\n",
    "events = load_events(SUBJECT, SESSION, runs[0], sourcedata_path)\n",
    "confounds_df = add_button_press_confounds(confounds_df, events, TR)\n",
    "\n",
    "button_count = confounds_df['button_count_press'].values\n",
    "print(f\"\u2713 Added button_count_press confound\")\n",
    "print(f\"  Mean: {button_count.mean():.2f} presses/TR\")\n",
    "print(f\"  Max: {int(button_count.max())} presses in a single TR\")\n",
    "print(f\"  Non-zero TRs: {(button_count > 0).sum()}/{len(button_count)}\")\n",
    "print(f\"\\n\ud83d\udcca Total confounds: {confounds_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Standardize confounds\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "confounds_df.loc[:, confounds_df.columns] = scaler.fit_transform(confounds_df)\n",
    "print(f\"\u2713 Standardized confounds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confound structure\n",
    "from glm_utils import plot_confounds_structure\n",
    "\n",
    "fig = plot_confounds_structure(confounds_df, runs[0], button_count=button_count)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create hand lateralization event regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create hand lateralization events\n",
    "\n",
    "def create_hand_lateralization_events(events_df):\n",
    "    \"\"\"\n",
    "    Create hand lateralization events from button presses.\n",
    "    \n",
    "    LEFT_THUMB: D-pad buttons (LEFT, RIGHT, UP, DOWN) - controlled by left thumb\n",
    "    RIGHT_THUMB: Action buttons (A, B) - controlled by right thumb\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    events_df : pd.DataFrame\n",
    "        Events with trial_type, onset, duration\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Events with LEFT_THUMB and RIGHT_THUMB conditions\n",
    "    \"\"\"\n",
    "    hand_events = []\n",
    "    \n",
    "    for _, event in events_df.iterrows():\n",
    "        trial_type = event['trial_type']\n",
    "        \n",
    "        if trial_type in ['LEFT', 'RIGHT', 'UP', 'DOWN']:\n",
    "            hand_events.append({\n",
    "                'onset': event['onset'],\n",
    "                'duration': event['duration'],\n",
    "                'trial_type': 'LEFT_THUMB'\n",
    "            })\n",
    "        elif trial_type in ['A', 'B']:\n",
    "            hand_events.append({\n",
    "                'onset': event['onset'],\n",
    "                'duration': event['duration'],\n",
    "                'trial_type': 'RIGHT_THUMB'\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(hand_events)\n",
    "\n",
    "# Create for first run\n",
    "hand_events = create_hand_lateralization_events(events)\n",
    "\n",
    "# Show summary\n",
    "n_left = (hand_events['trial_type'] == 'LEFT_THUMB').sum()\n",
    "n_right = (hand_events['trial_type'] == 'RIGHT_THUMB').sum()\n",
    "\n",
    "print(f\"Hand lateralization events for {runs[0]}:\")\n",
    "print(f\"  LEFT_THUMB: {n_left} events\")\n",
    "print(f\"  RIGHT_THUMB: {n_right} events\")\n",
    "print(f\"\\nFirst few events:\")\n",
    "print(hand_events.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Create design matrix for one run\n",
    "\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix\n",
    "from nilearn.plotting import plot_design_matrix\n",
    "\n",
    "def create_design_matrix(bold_img, hand_events, confounds_df, tr):\n",
    "    \"\"\"\n",
    "    Create design matrix for GLM analysis.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bold_img : Nifti1Image\n",
    "        Preprocessed BOLD image\n",
    "    hand_events : pd.DataFrame\n",
    "        Hand lateralization events (LEFT_THUMB, RIGHT_THUMB)\n",
    "    confounds_df : pd.DataFrame\n",
    "        Confounds (motion, physiology, task)\n",
    "    tr : float\n",
    "        Repetition time in seconds\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    design_matrix : pd.DataFrame\n",
    "        Design matrix with task regressors + confounds + drift\n",
    "    \"\"\"\n",
    "    # Get frame times\n",
    "    n_scans = bold_img.shape[-1]\n",
    "    frame_times = np.arange(n_scans) * tr\n",
    "    \n",
    "    # Create design matrix\n",
    "    design_matrix = make_first_level_design_matrix(\n",
    "        frame_times,\n",
    "        events=hand_events,\n",
    "        hrf_model='spm',           # SPM canonical HRF\n",
    "        drift_model=None,          # Handled by confounds\n",
    "        high_pass=None,            # Handled by confounds\n",
    "        add_regs=confounds_df,\n",
    "        add_reg_names=list(confounds_df.columns)\n",
    "    )\n",
    "    \n",
    "    return design_matrix\n",
    "\n",
    "# Create design matrix for first run\n",
    "design_matrix = create_design_matrix(\n",
    "    bold_img, hand_events, confounds_df, TR\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Created design matrix for {runs[0]}\\n\")\n",
    "print(f\"Design matrix structure:\")\n",
    "print(f\"  Shape: {design_matrix.shape}\")\n",
    "print(f\"  Timepoints: {design_matrix.shape[0]}\")\n",
    "print(f\"  Regressors: {design_matrix.shape[1]}\")\n",
    "print(f\"\\nRegressor breakdown:\")\n",
    "print(f\"  Task: 2 (LEFT_THUMB, RIGHT_THUMB)\")\n",
    "print(f\"  Confounds: {len(confounds_df.columns)}\")\n",
    "print(f\"  Constant: 1\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "plot_design_matrix(design_matrix, ax=ax, rescale=True);\n",
    "ax.set_title(f'Design Matrix - {runs[0]}', fontsize=14, fontweight='bold')\n",
    "ax.grid(False)  # Deactivate grid\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the GLM design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Scrub volumes between repetitions.\n",
    "def add_scrub_regressors(run_events, design_matrix):\n",
    "    \"\"\"Creates a scrub regressor that indicates when a repetition (with available bk2)\n",
    "    is ongoing or not. If no bk2 is available, the repetition will be ignored and\n",
    "    scrubbed as 0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    run_events : DataFrame\n",
    "        The original dataframe created with create_runevents, containing all the events of one run\n",
    "    design_matrix : DataFrame\n",
    "        Design matrix as created by Nilearn's make_first_level_design_matrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    design_matrix : DataFrame\n",
    "        Same design_matrix as above, with the addition of a scrub confounds.\n",
    "    \"\"\"\n",
    "    reps = []\n",
    "    # Get repetition segments\n",
    "    for i in range(len(run_events)):\n",
    "        if run_events['trial_type'][i] == \"gym-retro_game\":\n",
    "            reps.append(run_events.iloc[i,:])\n",
    "\n",
    "    # Get time vector\n",
    "    time = np.array(design_matrix.index)\n",
    "\n",
    "    to_keep = np.zeros(len(time))\n",
    "    # Generate binary regressor\n",
    "    for i in range(len(time)):\n",
    "        for rep in reps:\n",
    "            if type(rep[\"stim_file\"]) == str and rep[\"stim_file\"] != \"Missing file\" and type(rep[\"stim_file\"]) != float:\n",
    "                if time[i] >= rep['onset'] and time[i] <= rep['onset'] + rep['duration']:\n",
    "                    to_keep[i] = 1.0\n",
    "\n",
    "    scrub_idx = 1\n",
    "    for idx, timepoint in enumerate(to_keep):\n",
    "        if timepoint == 0.0: # If to_keep is zero create a scrub regressor to remove this frame\n",
    "            scrub_regressor = np.zeros(len(time))\n",
    "            scrub_regressor[idx] = 1.0\n",
    "            design_matrix[f'scrub{scrub_idx}'] = scrub_regressor\n",
    "            scrub_idx += 1\n",
    "    return design_matrix\n",
    "design_matrix = add_scrub_regressors(events, design_matrix)\n",
    "print(f\"\u2713 Added scrub regressors\")\n",
    "print(f\"\\n\ud83d\udcca Total regressors: {design_matrix.shape[1]}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "plot_design_matrix(design_matrix, ax=ax, rescale=True)\n",
    "ax.set_title(f'Design Matrix - {runs[0]}', fontsize=14, fontweight='bold')\n",
    "ax.grid(False)  # Deactivate grid\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Prepare inputs of each run for FirstLevel GLM\n",
    "\n",
    "from nilearn.image import clean_img\n",
    "from nilearn.masking import compute_multi_epi_mask\n",
    "from utils import load_bold, load_brain_mask\n",
    "\n",
    "print(\"Preparing data for all runs...\\n\")\n",
    "\n",
    "bold_imgs = []\n",
    "design_matrices = []\n",
    "confounds_list = []\n",
    "events_list = []\n",
    "hand_events_list = []\n",
    "DESIGN_READY = False\n",
    "bold_imgs_for_mask = []\n",
    "\n",
    "for run_idx, run in enumerate(runs):\n",
    "    print(f\"{run}...\")\n",
    "    bold_img = load_bold(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    bold_imgs_for_mask.append(bold_img) # Keep original for mask computation\n",
    "    bold_clean = clean_img(\n",
    "        bold_img,\n",
    "        standardize=True,\n",
    "        detrend=True,\n",
    "        high_pass=None,\n",
    "        t_r=TR\n",
    "    )\n",
    "    bold_imgs.append(bold_clean)\n",
    "\n",
    "    confounds_df, _ = load_fmriprep_confounds(SUBJECT, SESSION, run)\n",
    "    events = all_events[run_idx] if EVENTS_LOADED else load_events(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    events_list.append(events)\n",
    "\n",
    "    confounds_df = add_button_press_confounds(confounds_df, events, TR)\n",
    "    lowlevel = load_lowlevel_confounds(SUBJECT, SESSION, run, sourcedata_path)\n",
    "    confounds_df = add_psychophysics_confounds(confounds_df, lowlevel, TR)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    confounds_df.loc[:, confounds_df.columns] = scaler.fit_transform(confounds_df)\n",
    "    confounds_list.append(confounds_df)\n",
    "\n",
    "    hand_events = create_hand_lateralization_events(events)\n",
    "    hand_events_list.append(hand_events)\n",
    "\n",
    "    design_matrix = create_design_matrix(bold_clean, hand_events, confounds_df, TR)\n",
    "    design_matrix = add_scrub_regressors(events, design_matrix)\n",
    "    design_matrices.append(design_matrix)\n",
    "\n",
    "    print(f\"  \u2713 {bold_clean.shape[-1]} TRs, {len(confounds_df.columns)} confounds, {design_matrix.shape[1]} regressors\")\n",
    "\n",
    "print(\"\\nCreating common mask...\")\n",
    "# Create mask from original images\n",
    "common_mask = compute_multi_epi_mask(bold_imgs_for_mask, n_jobs=1)\n",
    "n_voxels_mask = int((common_mask.get_fdata() > 0).sum())\n",
    "print(f\"\u2713 Common mask: {n_voxels_mask:,} voxels\")\n",
    "\n",
    "\n",
    "DESIGN_READY = len(design_matrices) == len(bold_imgs) and len(bold_imgs) > 0\n",
    "\n",
    "print(f\"\\n\u2713 {len(bold_imgs)} runs prepared\")\n",
    "print(f\"\u2713 Design matrices ready: {DESIGN_READY}\")\n",
    "print(f\"\u2713 Hand event tables: {len(hand_events_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit GLM on all runs\n",
    "\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "\n",
    "# Define model\n",
    "glm = FirstLevelModel(\n",
    "    t_r=TR,\n",
    "    mask_img=common_mask,\n",
    "    noise_model='ar1',        # AR(1) autocorrelation\n",
    "    standardize=False,        # Already standardized\n",
    "    smoothing_fwhm=5,         # 5mm FWHM spatial smoothing\n",
    "    minimize_memory=True\n",
    ")\n",
    "\n",
    "# Fit on all runs simultaneously\n",
    "print(\"Fitting GLM...\")\n",
    "glm.fit(bold_imgs, design_matrices=design_matrices)\n",
    "\n",
    "print(f\"\\n\u2713 GLM fitted on {len(bold_imgs)} runs\")\n",
    "print(f\"  Model: AR(1) noise, SPM HRF\")\n",
    "print(f\"  Voxels: {n_voxels_mask:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute contrast and visualize\n",
    "\n",
    "from glm_utils import plot_contrast_surfaces\n",
    "\n",
    "# Compute LEFT_THUMB - RIGHT_THUMB contrast\n",
    "z_map = glm.compute_contrast('LEFT_THUMB - RIGHT_THUMB', output_type='z_score')\n",
    "\n",
    "# Plot on surface (using helper function)\n",
    "fig = plot_contrast_surfaces(\n",
    "    z_map, \n",
    "    contrast_name='Hand Lateralization: LEFT_THUMB - RIGHT_THUMB',\n",
    "    stat_threshold=4.0,\n",
    "    cluster_threshold=10\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udcca Interpretation:\")\n",
    "print(\"  Red: LEFT_THUMB > RIGHT_THUMB \u2192 Right motor cortex (contralateral)\")\n",
    "print(\"  Blue: RIGHT_THUMB > LEFT_THUMB \u2192 Left motor cortex (contralateral)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Cluster-level FWE correction for multiple comparisons\n",
    "\n",
    "from nilearn.glm import cluster_level_inference\n",
    "\n",
    "print(\"Applying cluster-level FWE correction (alpha=0.05)...\\n\")\n",
    "\n",
    "# Perform cluster-level inference with FWE correction\n",
    "# Returns a Nifti1Image with proportion of true discoveries per voxel\n",
    "proportion_true_discoveries = cluster_level_inference(\n",
    "    z_map,\n",
    "    mask_img=common_mask,\n",
    "    threshold=3,  # Cluster-forming threshold\n",
    "    alpha=0.05      # Family-wise error rate\n",
    ")\n",
    "\n",
    "print(f\"\u2713 FWE correction complete\")\n",
    "print(f\"  Cluster-forming threshold: Z > 3.1\")\n",
    "print(f\"  FWE-corrected alpha: 0.05\")\n",
    "\n",
    "# Create FWE-corrected z-map by masking original z-map\n",
    "# Only keep voxels with proportion > 0 (significant clusters)\n",
    "from nilearn.image import math_img\n",
    "\n",
    "z_map_fwe = math_img(\n",
    "    \"img1 * (img2 > 0)\",\n",
    "    img1=z_map,\n",
    "    img2=proportion_true_discoveries\n",
    ")\n",
    "\n",
    "# Count significant voxels\n",
    "fwe_data = z_map_fwe.get_fdata()\n",
    "n_sig_voxels = np.sum(np.abs(fwe_data) > 0)\n",
    "\n",
    "print(f\"  Significant voxels: {n_sig_voxels:,}\")\n",
    "\n",
    "# Plot FWE-corrected results\n",
    "print(\"\\nPlotting FWE-corrected contrast...\")\n",
    "fig = plot_contrast_surfaces(\n",
    "    z_map_fwe,\n",
    "    contrast_name='Hand Lateralization (FWE-corrected, p<0.05)',\n",
    "    stat_threshold=0.1,  # Lower threshold since already FWE-corrected\n",
    "    cluster_threshold=0\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2705 Multiple comparison correction applied!\")\n",
    "print(\"   Compare this with the uncorrected map above.\")\n",
    "print(\"   FWE correction is more conservative but controls false positives.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLM Analysis for Naturalistic Paradigms\n\n### The Complete Pipeline\n\nWe just performed a **session-level GLM analysis** on naturalistic fMRI data (video game playing). Let's understand what happened at each step:\n\n---\n\n### 1. Data Preparation\n\n**BOLD Preprocessing:**\n```python\nclean_img(bold_img, standardize=True, detrend=True, high_pass=None, t_r=TR)\n```\n- **Standardization (z-score):** Removes mean, scales to unit variance\n  - Makes BOLD comparable across voxels and runs\n  - Necessary because raw BOLD has arbitrary units (scanner-dependent)\n- **Linear detrending:** Removes linear drift within each run\n  - Scanner signal drifts over time (heating, subject movement)\n  - Detrending isolates neural signal from scanner drift\n- **No high-pass filtering here:** Handled by cosine basis functions from fMRIPrep confounds\n\n**Common Masking:**\n```python\ncompute_multi_epi_mask(bold_imgs_for_mask, n_jobs=1)\n```\n- Creates **intersection** of all run masks\n- Only analyzes voxels that are valid in **ALL** runs\n- **Why necessary?**\n  - Subject moves between runs \u2192 brain edges shift\n  - Without common mask: some voxels missing data in some runs\n  - Result: invalid statistics at brain boundaries\n- **Trade-off:** Smaller mask, but more reliable\n\n---\n\n### 2. Confound Strategy\n\n**What are confounds?**\nSignals that affect BOLD but aren't related to task/cognition.\n\n**Our confound sources (via nilearn's `load_confounds`):**\n```python\nload_confounds(bold_path, strategy=(\"motion\", \"high_pass\", \"wm_csf\"), ...)\n```\n- **Motion (24 parameters):** 6 motion params + derivatives + quadratic terms\n- **Physiology:** White matter + CSF signals (basic)\n- **Global signal:** Full global signal regressors\n- **High-pass filtering:** Cosine basis functions from fMRIPrep\n\n**Additional task confounds:**\n- **Button press counts:** Total presses per TR (motor noise)\n- **Low-level game features:** Player position (x, y), score, time, lives, powerup state\n  - Downsampled from 60Hz to TR resolution\n  - Captures visual/game state variance we want to control for\n\n**Confound standardization:**\n```python\nscaler = StandardScaler()\nconfounds_df.loc[:, confounds_df.columns] = scaler.fit_transform(confounds_df)\n```\n- All confounds z-scored before adding to design matrix\n- Ensures comparable scaling across different confound types\n\n**How confounds are handled:**\n```python\nmake_first_level_design_matrix(..., add_regs=confounds_df, add_reg_names=list(confounds_df.columns))\n```\n- Confounds become **nuisance regressors** in design matrix\n- GLM estimates their contribution and removes it\n\n---\n\n### 3. Scrubbing Strategy\n\n**Problem:** Not all timepoints have valid replay data (.bk2 files)\n\n**Solution:** Scrub regressors for invalid timepoints\n```python\nadd_scrub_regressors(run_events, design_matrix)\n```\n- Creates one-hot regressors for each invalid TR\n- Timepoints outside valid game repetitions are regressed out\n- Ensures we only analyze TRs with corresponding behavioral data\n\n---\n\n### 4. Multi-Run GLM Fitting\n\n**Traditional approach:**\n```\nFor each run:\n  Fit GLM \u2192 Get run-level betas\nAggregate run-level betas \u2192 Session-level betas\n```\n\n**Nilearn's multi-run approach:**\n```python\nglm.fit(bold_imgs, design_matrices=design_matrices)\n```\n- Fits all runs **simultaneously**\n- Models run-to-run variance properly\n- More efficient (one optimization, not 5 separate ones)\n- Produces session-level effects directly\n\n**GLM settings:**\n```python\nFirstLevelModel(\n    t_r=TR,\n    mask_img=common_mask,\n    noise_model='ar1',        # AR(1) autocorrelation\n    standardize=False,        # Already done in clean_img\n    smoothing_fwhm=5,         # 5mm FWHM spatial smoothing\n    minimize_memory=True\n)\n```\n\n---\n\n### 5. The Hand Lateralization Contrast\n\n**Hypothesis:** Contralateral motor control\n- **LEFT_THUMB** (D-pad: LEFT, RIGHT, UP, DOWN) \u2192 Right motor cortex\n- **RIGHT_THUMB** (A, B buttons) \u2192 Left motor cortex\n\n**What the contrast does:**\n```python\nz_map = glm.compute_contrast('LEFT_THUMB - RIGHT_THUMB', output_type='z_score')\n```\n- For each voxel: (\u03b2_LEFT - \u03b2_RIGHT) / SE\n- Positive z-scores: more active for left thumb \u2192 right motor cortex\n- Negative z-scores: more active for right thumb \u2192 left motor cortex\n\n---\n\n### 6. Statistical Thresholding\n\n**Uncorrected visualization:**\n```python\nplot_contrast_surfaces(z_map, stat_threshold=4.0, cluster_threshold=10)\n```\n- **Z > 4.0:** Stringent uncorrected threshold\n- **Cluster size > 10 voxels:** Reduces isolated false positives\n\n**FWE-corrected inference:**\n```python\ncluster_level_inference(z_map, mask_img=common_mask, threshold=3, alpha=0.05)\n```\n- **Cluster-forming threshold:** Z > 3\n- **Family-wise error rate:** \u03b1 = 0.05\n- Controls false positive rate across the whole brain\n\n---\n\n### 7. Visualization \n\n**What we're seeing:**\n- Red regions: LEFT_THUMB > RIGHT_THUMB\n- Blue regions: RIGHT_THUMB > LEFT_THUMB\n- Background sulci: anatomical reference\n\n---\n\n### Strengths of This Approach\n\n\u2705 **Event-related design:** Models individual button presses\n\u2705 **Multi-run aggregation:** Robust session-level statistics\n\u2705 **Comprehensive confound control:** Motion, physiology, task, low-level features\n\u2705 **Scrubbing:** Excludes invalid timepoints\n\u2705 **FWE correction:** Proper multiple comparison correction\n\u2705 **Interpretable:** Clear link between behavior and brain\n\n---\n\n### Limitations and Considerations\n\n**1. Sparse Event Coverage**\n- Only models button presses (a few events per second)\n- Ignores continuous gameplay (visual flow, cognitive planning, value)\n- **Alternative:** Encoding models with continuous features (see Section 5)\n\n**2. Hand-Crafted Regressors**\n- LEFT_THUMB vs RIGHT_THUMB is our hypothesis\n- What if brain codes actions differently? (e.g., spatial vs non-spatial)\n- **Alternative:** Data-driven approaches (MVPA, RSA, encoding models)\n\n**3. Session-Level Analysis Only**\n- 5 runs, ~25 minutes of data\n- Subject-specific activations, not generalizable yet\n- **Better:** Multi-session or multi-subject analysis\n\n**4. Hemodynamic Response Assumptions**\n- Used canonical SPM HRF (standard shape)\n- Real HRF varies across regions and subjects\n- **Alternative:** FIR models (finite impulse response) - no HRF assumption\n\n---\n\n### When to Use GLM vs Other Approaches\n\n**GLM is best for:**\n- Testing specific hypotheses (e.g., \"Does motor cortex code hand laterality?\")\n- Event-related designs with known trial timing\n- Interpreting results in terms of experimental conditions\n- Publication in cognitive neuroscience journals\n\n**GLM is limited for:**\n- Discovering unexpected brain representations\n- Continuous naturalistic stimuli (movies, games)\n- Modeling latent variables (value, prediction error, uncertainty)\n- Predicting brain activity from complex features\n\n**Complementary approaches:**\n- **Encoding models:** Predict BOLD from stimulus/behavior features\n- **MVPA (decoding):** Predict behavior from BOLD patterns\n- **RSA:** Compare representational geometry\n- **ICA/PCA:** Discover data-driven networks\n\n---\n\n### Next Steps\n\nIn **Section 5 (Brain Encoding)**, we'll address GLM's limitations by:\n1. Using **continuous RL features** (not just button presses)\n2. **Predicting BOLD** from learned representations\n3. Discovering which brain regions encode **hierarchical game features**\n4. Using **data-driven** (not hypothesis-driven) approach\n\n**Key difference:**\n- GLM: \"Does LEFT_THUMB activate motor cortex?\" (test hypothesis)\n- Encoding: \"What features predict motor cortex activity?\" (explore)\n\nBoth are valuable! GLM for **interpretation**, encoding for **prediction**."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "rise": {
   "autolaunch": false,
   "enable_chalkboard": true,
   "footer": "<h3>CNeuromod 2025</h3>",
   "header": "<h2>Mario fMRI Tutorial</h2>",
   "scroll": true,
   "slideNumber": true,
   "theme": "simple",
   "transition": "fade"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}